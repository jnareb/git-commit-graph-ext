[
  {
    "objectID": "reach.html",
    "href": "reach.html",
    "title": "Reachability queries",
    "section": "",
    "text": "Imports for the git_commit_graph_ext.reachability module generated from this notebook"
  },
  {
    "objectID": "reach.html#reachability-queries-with-topological-levels-and-min-post-intervals",
    "href": "reach.html#reachability-queries-with-topological-levels-and-min-post-intervals",
    "title": "Reachability queries",
    "section": "Reachability queries with topological levels and min-post intervals",
    "text": "Reachability queries with topological levels and min-post intervals\nStart with what was defined in the original Google Colab notebook, called there generic_is_reachable().\nIt will not be using FELINE index though, as it seems that this type of index is not a good fit for large commit graphs.\n\nRecursive chatty version (levels + min-post) - generic_is_reachable\nThis recursive version is based on the Algorithm 3 from the FELINE paper, translated from pseudocode to Python (and NetworkX).\n\nsource\n\n\ngeneric_is_reachable\n\n generic_is_reachable (DG, u, v, II=None, l=None, stats=None,\n                       verbose=None)\n\nWhether in graph DG \\(v\\) is reachable from \\(u\\), utilizing given indices\nGiven (u, v) ∈ V², two vertices in the DAG given by the DG parameter, calculate r(u,v), that is whether vertex v is reachable from vertex u.\nThis is based on the Algorithm 3 from FELINE paper, translated from pseudocode to Python (and NetworkX).\nNOTE: this is a straightforward recursive version of the function.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nDG\nNetworkX digraph\n\nDirected acyclic graph.\n\n\nu\nnode\n\nSource node.\n\n\nv\nnode\n\nTarget node.\n\n\nII\nNoneType\nNone\nA dictionary with nodes as keys and min-post interval index as values(2-element lists, representing intervals).\n\n\nl\nNoneType\nNone\nA dictionary with nodes as keys and backward topological vertex level as values(vertex level is also known as generation number).\n\n\nstats\nNoneType\nNone\nA dictionary gathering statistics about calls. Currently supportedare: * ‘access’ key, counting the number of intermediate vertices it checks / accesses * ‘level’ key, storing list of vertices where level index cut off further walk (further search) * ‘walk’ key, with list of all vertices walked, in order * ‘min-post’ key, storing the vertex at which positive-cut min-post interval cut-off the need for further search\n\n\nverbose\nNoneType\nNone\nWhether to print debugging information. If set to None (the default),it prints debugging information if stats parameter is set.\n\n\nReturns\nbool\n\nWhether v is reachable from u\n\n\n\nTest that everything works. For this we need example graphs, and a way to compute reachability levels\n\n# example graphs\nimport git_commit_graph_ext.example_graphs as graphs\n# reachability indexes\nfrom git_commit_graph_ext.labelling.levels import *\nfrom git_commit_graph_ext.labelling.dfs_intervals import *\n# plotting, for visualizations\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nUse the example graph from the PReaCH paper for testing\n\n# lazy generation of the graph\ntry:\n    ch\nexcept NameError:\n    ch=graphs.RCH_graph()\n\nrecalculate = True\nif recalculate or not hasattr(ch, 'lvl'):\n    ch.lvl = find_levels(ch) # TODO: pre-computed levels are forward levels\nif recalculate or not hasattr(ch, 'mpi'):\n    ch.mpi = find_dfs_intervals(ch)\n\n#print('%r' % ch)\nprint('graph has {} nodes and {} edges'.format(ch.number_of_nodes(), ch.number_of_edges()))\nprint('nodes: {!r}'.format(list(ch.nodes())))\nprint('it has the following attributes:')\nfor (attr,val) in ch.__dict__.items():\n  if isinstance(val, dict) and not attr.startswith('_'):\n    #print('- %s: %r' % (attr, ch.__dict__[attr]))\n    print('- %s' % attr)\n\ngraph has 17 nodes and 19 edges\nnodes: ['a', 'e', 'b', 'f', 'g', 'c', 'h', 'i', 'd', 'j', 'o', 'k', 'l', 'm', 'n', 'p', 'q']\nit has the following attributes:\n- graph\n- lvl\n- pos\n- mpi\nWall time: 6.99 ms\n\n\n\nplt.axis(\"off\")\nplt.title('Example graph from PReaCH paper (Figures 1 and 2)\\ndraw reversed - with arrows pointing down')\nnx.draw_networkx(ch,pos={key:(value[0],-value[1]) for (key,value) in ch.pos.items()},\n                 node_size=500,width=2.0,node_color='#dd44ff')\nnx.draw_networkx_edges(ch,pos={key:(value[0],-value[1]) for (key,value) in ch.pos.items()},\n                       edgelist=find_dfs_spanning(ch),\n                       node_size=500,\n                       width=8.0,edge_color='#4444ff',alpha=0.4,arrowstyle='-&gt;')\nplt.show()\n\n\n\n\n\n# test unreachable\nstats = {}\nresult = generic_is_reachable(ch, 'a', 'g', stats=stats)\nprint('r(%s,%s)=%r in %d steps (naive)\\n  %r\\n' %\n      ('a', 'g', result, stats['access'], stats))\nstats = {}\nresult = generic_is_reachable(ch, 'a', 'g', II=ch.mpi, stats=stats)\nprint('r(%s,%s)=%r in %d steps (min-post)\\n  %r\\n' %\n      ('a', 'g', result, stats['access'], stats))\nstats = {}\nresult = generic_is_reachable(ch, 'a', 'g', l=ch.lvl, stats=stats)\nprint('r(%s,%s)=%r in %d steps (level)\\n  %r' %\n      ('a', 'g', result, stats['access'], stats))\n\na-&gt;['e']\ne-&gt;['j']\nj-&gt;['o', 'p']\no-&gt;[]\np-&gt;[]\nr(a,g)=False in 4 steps (naive)\n  {'access': 4, 'level': [], 'walk': ['a', 'e', 'j', 'o', 'p']}\n\na-&gt;['e']\ne-&gt;['j']\nj-&gt;['o', 'p']\no-&gt;[]\np-&gt;[]\nr(a,g)=False in 4 steps (min-post)\n  {'access': 4, 'level': [], 'walk': ['a', 'e', 'j', 'o', 'p']}\n\na-&gt;['e']\ne-&gt;g level cut ¬2 &lt; 2\nr(a,g)=False in 1 steps (level)\n  {'access': 1, 'level': ['e'], 'walk': ['a', 'e']}\n\n\n\n# test reachable\nstats = {}\nu='b'\nv='o'\n#u='c'\n#v='q'\nresult = generic_is_reachable(ch, u, v, stats=stats)\nprint('r(%s,%s)=%r in %d steps (naive)\\n  %r\\n' %\n      (u, v, result, stats['access'], stats))\nstats = {}\nresult = generic_is_reachable(ch, u, v, II=ch.mpi, stats=stats)\nprint('r(%s,%s)=%r in %d steps (min-post)\\n  %r\\n' %\n      (u, v, result, stats['access'], stats))\nstats = {}\nresult = generic_is_reachable(ch, u, v, l=ch.lvl, stats=stats)\nprint('r(%s,%s)=%r in %d steps (level)\\n  %r\\n' %\n      (u, 'g', result, stats['access'], stats))\nstats = {}\nresult = generic_is_reachable(ch, u, v, II=ch.mpi, l=ch.lvl, stats=stats)\nprint('r(%s,%s)=%r in %d steps (min-post + level)\\n  %r' %\n      (u, v, result, stats['access'], stats))\n\nb-&gt;['e', 'f', 'g']\ne-&gt;['j']\nj-&gt;['o', 'p']\np-&gt;[]\nf-&gt;['o', 'k']\nk-&gt;['p']\np-&gt;[]\ng-&gt;['l']\nl-&gt;['q']\nq-&gt;[]\nr(b,o)=True in 11 steps (naive)\n  {'access': 11, 'level': [], 'walk': ['b', 'e', 'j', 'o', 'p', 'f', 'o', 'k', 'p', 'g', 'l', 'q']}\n\nb-&gt;['e', 'f', 'g']\ne-&gt;o min-post resolved 1 ∈ [1, 1] ⊆ [1, 4]\nf-&gt;['o', 'k']\nk-&gt;['p']\np-&gt;[]\ng-&gt;['l']\nl-&gt;['q']\nq-&gt;[]\nr(b,o)=True in 8 steps (min-post)\n  {'access': 8, 'level': [], 'walk': ['b', 'e', 'f', 'o', 'k', 'p', 'g', 'l', 'q'], 'min-post': 'e'}\n\nb-&gt;['e', 'f', 'g']\ne-&gt;['j']\nj-&gt;['o', 'p']\np-&gt;o level cut ¬0 &lt; 0\nf-&gt;['o', 'k']\nk-&gt;['p']\np-&gt;o level cut ¬0 &lt; 0\ng-&gt;['l']\nl-&gt;['q']\nq-&gt;o level cut ¬0 &lt; 0\nr(b,g)=True in 11 steps (level)\n  {'access': 11, 'level': ['p', 'p', 'q'], 'walk': ['b', 'e', 'j', 'o', 'p', 'f', 'o', 'k', 'p', 'g', 'l', 'q']}\n\nb-&gt;['e', 'f', 'g']\ne-&gt;o min-post resolved 1 ∈ [1, 1] ⊆ [1, 4]\nf-&gt;['o', 'k']\nk-&gt;['p']\np-&gt;o level cut ¬0 &lt; 0\ng-&gt;['l']\nl-&gt;['q']\nq-&gt;o level cut ¬0 &lt; 0\nr(b,o)=True in 8 steps (min-post + level)\n  {'access': 8, 'level': ['p', 'q'], 'walk': ['b', 'e', 'f', 'o', 'k', 'p', 'g', 'l', 'q'], 'min-post': 'e'}\n\n\n\n\nDFS walk version (levels + min-post) - generic_is_reachable_dfs\nThe code below was called generic_is_reachable_large_v1() and aliased to generic_is_reachable_large() in the original Google Colab notebook (see the index for reference); though the one below has had its support for FELINE labels removed.\n\nsource\n\n\ngeneric_is_reachable_dfs\n\n generic_is_reachable_dfs (DG, u, v, II=None, l=None, stats=None)\n\nWhether in large graph DG \\(v\\) is reachable from \\(u\\), utilizing given indices\nGiven (u, v) ∈ V², two vertices in the DAG given by the DG parameter, calculate r(u,v), whether vertex v is reachable from vertex u.\nThis is non-recursive version of the function, without verbose mode, intended for larger graphs. It travels the graph using depth-first search (DFS).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nDG\nNetworkX digraph\n\nDirected acyclic graph.\n\n\nu\nnode\n\nSource node.\n\n\nv\nnode\n\nTarget node.\n\n\nII\nNoneType\nNone\nA dictionary with nodes as keys and min-post interval index as values(2-element lists, representing intervals), e.g. result offind_dfs_intervals().\n\n\nl\nNoneType\nNone\nA dictionary with nodes as keys and vertex level as values(vertex level is also known as generation number), e.g. result offind_levels().\n\n\nstats\nNoneType\nNone\nA dictionary gathering statistics about calls. Currently supportedare: * ‘access’ key, counting the number of intermediate vertices it checks / accesses. * ‘level-filter’ key, storing nodes that level index stopped searching at * ‘walk’ key, storing all walked nodes * ‘min-post’ key, storing node where min-post filter found reachable * ‘max-depth’ key, with maximum stack depth\n\n\nReturns\nbool\n\nWhether v is reachable from u\n\n\n\nBasic test that generic_is_reachable_dfs() works (among others that there are no syntax errors).\n\nstats = {}\n#u='b'\nu='a'\nv='o'\n\nprint('graph has {} nodes and {} edges'.format(ch.number_of_nodes(), ch.number_of_edges()))\nprint('nodes: {!r}'.format(list(ch.nodes())))\nprint('it has the following attributes:')\nfor (attr,val) in ch.__dict__.items():\n  if isinstance(val, dict) and not attr.startswith('_'):\n    #print('- %s: %r' % (attr, ch.__dict__[attr]))\n    print('- %s' % attr)\n\nresult = generic_is_reachable_dfs(ch, u, v, II=ch.mpi, l=ch.lvl, stats=stats)\nprint('r(%s,%s)=%r in %d steps (min-post + level)\\n  %r' %\n      (u, v, result, stats['access'], stats))\n\ngraph has 17 nodes and 19 edges\nnodes: ['a', 'e', 'b', 'f', 'g', 'c', 'h', 'i', 'd', 'j', 'o', 'k', 'l', 'm', 'n', 'p', 'q']\nit has the following attributes:\n- graph\n- lvl\n- pos\n- mpi\nr(a,o)=True in 0 steps (min-post + level)\n  {'access': 0, 'level-filter': [], 'walk': ['a'], 'max-depth': 0, 'visited-filter': 0, 'min-post': 'a'}\n\n\n\n\nBFS walk version, actively developed - generic_is_reachable_bfs\nThis is called generic_is_reachable_large_bfs() (where BFS = breadth-first search) in the Google Colab notebook, but with the support for FELINE index stripped, and the support for PReaCH extensions to the min-post intervals index commented out; the support for min-post graph intervals as negative-cut filters was kept (was part of PReaCH extension).\nThis code tries to be more generic by separating reachable_positive_cut and reachable_negative_cut into their own functions, but they are still very much tied to given set of reachability indices.\nCurrently there is no way to add another index at later time (the design is not open to extensions), but making something generic may be not necessary as it is mainly exploraatory notebook. The code in Git would be written in C language, not in Python.\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Todo:\n  else: warn(msg)\n\nsource\n\n\ngeneric_is_reachable_bfs\n\n generic_is_reachable_bfs (DG, u, v, II=None, l=None, stats=None)\n\nWhether in large graph DG \\(v\\) is reachable from \\(u\\), utilizing given indices\nGiven (u, v) ∈ V², two vertices in the DAG given by the DG parameter, calculate r(u,v), whether vertex v is reachable from vertex u.\nThis is non-recursive version of the function, without verbose mode, intended for larger graphs.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nDG\nNetworkX digraph\n\nDirected acyclic graph.\n\n\nu\nnode\n\nSource node.\n\n\nv\nnode\n\nTarget node.\n\n\nII\nNoneType\nNone\nA dictionary with nodes as keys and min-post interval index as values(2-element lists, representing intervals), e.g. result offind_dfs_intervals().\n\n\nl\nNoneType\nNone\n\n\n\nstats\nNoneType\nNone\n\n\n\nReturns\nbool\n\nWhether v is reachable from u\n\n\n\n\nsource\n\n\nwalk_spanning\n\n walk_spanning (DG, u, v, II)\n\nWalk spanning tree from \\(u\\) to \\(v\\), return path or []\n\nsource\n\n\nreachable_negative_cut\n\n reachable_negative_cut (u, v, II=None, l=None, stats=None)\n\nWhether given indices say that \\(v\\) is not reachable from \\(u\\)\nGiven (u, v) ∈ V², a negative cut happens if the index implies that vertex v is not reachable from vertex u.\nr(u,v) =&gt; test(u,v)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nu\nnode\n\nSource node.\n\n\nv\nnode\n\nTarget node.\n\n\nII\nNoneType\nNone\nA dictionary with nodes as keys and min-post interval index as values(2-element lists, representing intervals), e.g. result offind_dfs_intervals().\n\n\nl\nNoneType\nNone\n\n\n\nstats\nNoneType\nNone\n\n\n\nReturns\nbool\n\nWhether index indicates that v is not reachable from u\n\n\n\n\nsource\n\n\nreachable_positive_cut\n\n reachable_positive_cut (u, v, II=None, stats=None)\n\nWhether given indices say that \\(v\\) is reachable from \\(u\\)\nGiven (u, v) ∈ V², a positive cut happens if the reachability index implies that vertex v is reachable from vertex u.\ntest(u,v) =&gt; r(u,v)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nu\nnode\n\nSource node.\n\n\nv\nnode\n\nTarget node.\n\n\nII\nNoneType\nNone\nA dictionary with nodes as keys and min-post interval index as values(2-element lists, representing intervals), e.g. result offind_dfs_intervals().\n\n\nstats\nNoneType\nNone\n\n\n\nReturns\nbool\n\nWhether index indicates that v is reachable from u\n\n\n\n\n# lazy generation of the graph\ntry:\n    ch\nexcept NameError:\n    ch=graphs.RCH_graph()\n\nrecalculate = False\nif recalculate or not hasattr(ch, 'lvl'):\n    ch.lvl = find_levels(ch) # TODO: pre-computed levels are forward levels\nif recalculate or not hasattr(ch, 'mpi'):\n    ch.mpi = find_dfs_intervals(ch)\nif recalculate or not hasattr(ch, 'mpi_ext'):\n    ch.mpi_ext = find_dfs_intervals_extra(ch)\n\n#print('%r' % ch)\nprint('graph has {} nodes and {} edges'.format(ch.number_of_nodes(), ch.number_of_edges()))\nprint('nodes: {!r}'.format(list(ch.nodes())))\nprint('it has the following attributes:')\nfor (attr,val) in ch.__dict__.items():\n  if isinstance(val, dict) and not attr.startswith('_'):\n    #print('- %s: %r' % (attr, ch.__dict__[attr]))\n    print('- %s' % attr)\n\ngraph has 17 nodes and 19 edges\nnodes: ['a', 'e', 'b', 'f', 'g', 'c', 'h', 'i', 'd', 'j', 'o', 'k', 'l', 'm', 'n', 'p', 'q']\nit has the following attributes:\n- graph\n- lvl\n- pos\n- mpi\n- mpi_ext\n\n\n\nplt.axis(\"off\")\nplt.title('Example graph from PReaCH paper (Figures 1 and 2)\\ndraw reversed - with arrows pointing down')\nnx.draw_networkx(ch,pos={key:(value[0],-value[1]) for (key,value) in ch.pos.items()},\n                 node_size=500,width=2.0,node_color='#dd44ff')\nnx.draw_networkx_edges(ch,pos={key:(value[0],-value[1]) for (key,value) in ch.pos.items()},\n                       edgelist=find_dfs_spanning(ch),\n                       node_size=500,\n                       width=8.0,edge_color='#4444ff',alpha=0.4,arrowstyle='-&gt;')\nplt.show()\n\n\n\n\n\n# test unreachable\nstats = {}\nresult = generic_is_reachable_bfs(ch, 'a', 'g', stats=stats)\nprint('r(%s,%s)=%r in %d steps (naive)\\n' %\n      ('a', 'g', result, stats['access']))\nstats\n\nr(a,g)=False in 4 steps (naive)\n\n\n\n{'negative-cut': {'visited': []},\n 'depth': {'a': 0, 'e': 1, 'j': 2, 'o': 3, 'p': 3},\n 'prev': {'a': None, 'e': 'a', 'j': 'e', 'o': 'j', 'p': 'j'},\n 'access': 4,\n 'walk': ['a', 'e', 'j', 'o', 'p'],\n 'max_queue_size': 2,\n 'visited': {'a', 'e', 'j', 'o', 'p'}}\n\n\n\n# test unreachable\nstats = {}\nresult = generic_is_reachable_bfs(ch, 'a', 'g', l=ch.lvl, stats=stats)\nprint('r(%s,%s)=%r in %d steps (level)\\n' %\n      ('a', 'g', result, stats['access']))\nstats\n\nr(a,g)=False in 1 steps (level)\n\n\n\n{'negative-cut': {'level_lite': [], 'level_full': ['e'], 'visited': []},\n 'depth': {'a': 0, 'e': 1},\n 'prev': {'a': None, 'e': 'a'},\n 'access': 1,\n 'walk': ['a', 'e'],\n 'max_queue_size': 1,\n 'visited': {'a', 'e'}}\n\n\nTesting find_dfs_intervals_extra() computing min-post graph intervals (negative-cut filter) and generic_is_reachable_bfs() ability to handle this information and use it.\n\n# test unreachable\nstats = {}\nresult = generic_is_reachable_bfs(ch, 'a', 'g', II=ch.mpi_ext, l=ch.lvl, stats=stats)\nprint('r(%s,%s)=%r in %d steps (level + min-post graph + min-post tree)\\n' %\n      ('a', 'g', result, stats['access']))\nprint('%s: level=%d, interval_graph = [%2d, %2d]'   % ('a',ch.lvl['a'],ch.mpi_ext['a']['f_min'],ch.mpi_ext['a']['post']))\nprint('%s: level=%d, interval_graph = [%2d, %2d]\\n' % ('g',ch.lvl['g'],ch.mpi_ext['g']['f_min'],ch.mpi_ext['g']['post']))\nstats\n\nr(a,g)=False in 0 steps (level + min-post graph + min-post tree)\n\na: level=3, interval_graph = [ 1,  5]\ng: level=2, interval_graph = [ 8, 10]\n\n\n\n{'negative-cut': {'level_lite': [],\n  'level_full': [],\n  'f_max': ['a'],\n  'f_min': [],\n  'visited': []},\n 'depth': {'a': 0},\n 'prev': {'a': None},\n 'access': 0,\n 'walk': ['a'],\n 'max_queue_size': 1,\n 'visited': {'a'}}\n\n\n\n# test reachable\nstats = {}\nu='b'\nv='o'\n#u='c'\n#v='q'\nresult = generic_is_reachable_bfs(ch, u, v, stats=stats)\nprint('r(%s,%s)=%r in %d steps (naive)\\n  %r\\n' %\n      (u, v, result, stats['access'], stats))\nstats = {}\nresult = generic_is_reachable_bfs(ch, u, v, II=ch.mpi, stats=stats)\nprint('r(%s,%s)=%r in %d steps (min-post)\\n  %r\\n' %\n      (u, v, result, stats['access'], stats))\nstats = {}\nresult = generic_is_reachable_bfs(ch, u, v, l=ch.lvl, stats=stats)\nprint('r(%s,%s)=%r in %d steps (level)\\n  %r\\n' %\n      (u, 'g', result, stats['access'], stats))\nstats = {}\nresult = generic_is_reachable_bfs(ch, u, v, II=ch.mpi, l=ch.lvl, stats=stats)\nprint('r(%s,%s)=%r in %d steps (min-post + level)\\n  %r' %\n      (u, v, result, stats['access'], stats))\n\nr(b,o)=True in 9 steps (naive)\n  {'negative-cut': {'visited': []}, 'depth': {'b': 0, 'e': 1, 'f': 1, 'g': 1, 'j': 2, 'o': 3, 'k': 2, 'l': 2, 'p': 3}, 'prev': {'b': None, 'e': 'b', 'f': 'b', 'g': 'b', 'j': 'e', 'o': 'j', 'k': 'f', 'l': 'g', 'p': 'j'}, 'access': 9, 'walk': ['b', 'e', 'f', 'g', 'j', 'o'], 'max_queue_size': 5, 'visited': {'b', 'e', 'g', 'j', 'f', 'o'}, 'path': ['b', 'e', 'j', 'o'], 'len': 3}\n\nr(b,o)=True in 3 steps (min-post)\n  {'negative-cut': {'visited': []}, 'depth': 1, 'prev': {'b': None, 'e': 'b', 'f': 'b', 'g': 'b'}, 'access': 3, 'walk': ['b', 'e'], 'max_queue_size': 3, 'visited': {'b', 'e'}, 'positive-cut': {'type': 'min-post', 'node': 'e'}, 'path-pre': ['b', 'e'], 'path-post': ['e', 'j', 'o'], 'len-pre': 1, 'len-post': 2, 'path': ['b', 'e', 'j', 'o'], 'len': 3}\n\nr(b,g)=True in 9 steps (level)\n  {'negative-cut': {'level_lite': [], 'level_full': [], 'visited': []}, 'depth': {'b': 0, 'e': 1, 'f': 1, 'g': 1, 'j': 2, 'o': 3, 'k': 2, 'l': 2, 'p': 3}, 'prev': {'b': None, 'e': 'b', 'f': 'b', 'g': 'b', 'j': 'e', 'o': 'j', 'k': 'f', 'l': 'g', 'p': 'j'}, 'access': 9, 'walk': ['b', 'e', 'f', 'g', 'j', 'o'], 'max_queue_size': 5, 'visited': {'b', 'e', 'g', 'j', 'f', 'o'}, 'path': ['b', 'e', 'j', 'o'], 'len': 3}\n\nr(b,o)=True in 3 steps (min-post + level)\n  {'negative-cut': {'level_lite': [], 'level_full': [], 'visited': []}, 'depth': 1, 'prev': {'b': None, 'e': 'b', 'f': 'b', 'g': 'b'}, 'access': 3, 'walk': ['b', 'e'], 'max_queue_size': 3, 'visited': {'b', 'e'}, 'positive-cut': {'type': 'min-post', 'node': 'e'}, 'path-pre': ['b', 'e'], 'path-post': ['e', 'j', 'o'], 'len-pre': 1, 'len-post': 2, 'path': ['b', 'e', 'j', 'o'], 'len': 3}\n\n\n\n#@title test generic_is_reachable_large_bfs { form-width: \"30%\" }\n\n# lazy generation of the graph\ntry:\n    DG\nexcept NameError:\n    DG=graphs.crown_DAG()\n\nrecalculate = False\nif recalculate or not hasattr(DG, 'lvl'):\n    DG.lvl = find_levels(DG)\nif recalculate or not hasattr(DG, 'mpi'):\n    DG.mpi = find_dfs_intervals(DG)\n\nif recalculate or not hasattr(DG, 'pos'):\n    DG.pos={1:(1,4),2:(3,6),'u':(2,2),3:(4,1),4:(6,3),'v':(5,5)}\nif recalculate or not hasattr(DG, 'span'):\n    DG.span=find_dfs_spanning(DG)\n    \n# configure plot\nplt.axis('off')\nplt.title('Crown DAG $S_0^3$ with the spanning tree overlay '+\n          '(find_dfs_spanning)')\n\n# draw graph and its spanning tree\nnx.draw_networkx(DG,\n                 pos=DG.pos,\n                 node_size=500,width=8.0,\n                 edge_color='#aaaaaa')\nnx.draw_networkx_edges(DG,\n                       pos=DG.pos,\n                       edgelist=DG.span,\n                       node_size=500,width=2.0,\n                       arrowstyle='-&gt;')\nplt.draw()\n    \nprint('crown DAG: %r' % DG)\nprint('- r(u,v): %r' % \n      generic_is_reachable_bfs(DG,'u','v',\n                               l=DG.lvl,II=DG.mpi))\nprint('- r(1,2): %r' %\n      generic_is_reachable_bfs(DG, 1,  2,\n                               l=DG.lvl,II=DG.mpi))\n\ncrown DAG: &lt;networkx.classes.digraph.DiGraph object at 0x000001963BB1F9D0&gt;\n- r(u,v): False\n- r(1,2): True\n\n\n\n\n\n\nTODO: - examine other example graphs, or - create interactive visualization - examine paths, cutoffs, etc"
  },
  {
    "objectID": "repos.html",
    "href": "repos.html",
    "title": "Large Git repositories",
    "section": "",
    "text": "Imports for handling graphs in Python\nimport networkx as nx\nImports for cloning Git repositories and extracting the commit graph\nfrom git_commit_graph_ext.commit_graph import commit_graph, _repo_graph_name\nGathering cloned repositories and their graphs\nknown_repos = []\nThis should perhaps be a list of objects of specific defined type, not list of dicts\ndef get_known_repos():\n    return known_repos\nget_known_repos()\n\n[]"
  },
  {
    "objectID": "repos.html#from-gen-test-repository",
    "href": "repos.html#from-gen-test-repository",
    "title": "Large Git repositories",
    "section": "From gen-test repository",
    "text": "From gen-test repository\ngen-test – Test scripts for testing new versions of generation numbers.\nFrom the README:\n\nThis report investigates four replacements for generation numbers, and compares the number of walked commits to the existing algorithms (both using generation numbers and not using them at all). We can use this data to make decisions for the future of the feature.\n\nhttps://github.com/derrickstolee/gen-test/blob/master/clone-repos.sh\ngit clone git@github.com:derrickstolee/git.git --branch reach-perf\n\ngit clone git@github.com:curl/curl\ngit clone git@github.com:electron/electron\ngit clone git@github.com:FFmpeg/FFmpeg\ngit clone git@github.com:eclipse/jgit\ngit clone git@github.com:JuliaLang/julia\ngit clone git@github.com:jetbrains/kotlin\ngit clone git@github.com:torvalds/linux\ngit clone git@github.com:odoo/odoo\ngit clone git@github.com:openssl/openssl\ngit clone git@github.com:apple/swift\ngit clone git@github.com:tensorflow/tensorflow\ngit clone git@github.com:microsoft/TypeScript\n\ngit clone https://gerrit.googlesource.com/gerrit\ngit clone https://android.googlesource.com/platform/frameworks/base/ android-base\ngit clone https://chromium.googlesource.com/chromium/src chromium\n\n\n\n\n\n\nNote\n\n\n\nthe repositoriess from googlesource.com have trouble with cloning; it might be better to clone them from command line, not from this Jupyter notebook.\n\n\n\ncURL repository\nCurl is a command-line tool for transferring data specified with URL syntax.\n\ncurl_graph = commit_graph('https://github.com/curl/curl.git', 'curl')\n\nWall time: 289 ms\n\n\nIt takes around 9 s to reclone the curl repository, 4 s to rescan it, and 300 ms to refresh it.\n\nknown_repos = []\nknown_repos.append(\n    {\n        'name': 'curl',\n        'url': 'https://github.com/curl/curl',\n        'graph': curl_graph,\n    }\n)\nknown_repos[0]\n\n{'name': 'curl',\n 'url': 'https://github.com/curl/curl',\n 'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3ca0&gt;}\n\n\n\nprint('commit graph of \"{}\" project has'.format(known_repos[0]['name']))\nprint('- {} nodes / vertices / commits'.format(known_repos[0]['graph'].number_of_nodes()))\nprint('- {} edges'.format(known_repos[0]['graph'].number_of_edges()))\nprint('- {:.5f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repos[0]['graph'].number_of_edges() / known_repos[0]['graph'].number_of_nodes()))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repos[0]['graph'] if known_repos[0]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repos[0]['graph'] if known_repos[0]['graph'].in_degree(n) == 0])))\n\ncommit graph of \"curl\" project has\n- 26609 nodes / vertices / commits\n- 26631 edges\n- 1.00083 edge density, number of edges divided by number of nodes (vertices)\n- 1 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 14 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\n\n\nElectron repository\nElectron – Build cross-platform desktop apps with JavaScript, HTML, and CSS\n\nelectron_graph = commit_graph('https://github.com/electron/electron.git', 'electron')\n\nWall time: 553 ms\n\n\nIt takes around 15 seconds to clone the Electron repository, and construct it’s commit graph as NetworkX.DiGraph\n\nknown_repos.append({\n    'name': 'electron',\n    'url': 'https://github.com/electron/electron.git',\n    'graph': electron_graph,\n})\nknown_repos\n\n[{'name': 'curl',\n  'url': 'https://github.com/curl/curl',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3ca0&gt;},\n {'name': 'electron',\n  'url': 'https://github.com/electron/electron.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3970&gt;}]\n\n\n\nprint('commit graph of \"{}\" project has'.format(known_repos[1]['name']))\nprint('- {} nodes / vertices / commits'.format(known_repos[1]['graph'].number_of_nodes()))\nprint('- {} edges'.format(known_repos[1]['graph'].number_of_edges()))\nprint('- {:.5f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repos[1]['graph'].number_of_edges() / known_repos[1]['graph'].number_of_nodes()))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repos[1]['graph'] if known_repos[1]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repos[1]['graph'] if known_repos[1]['graph'].in_degree(n) == 0])))\n\ncommit graph of \"electron\" project has\n- 29956 nodes / vertices / commits\n- 34167 edges\n- 1.14057 edge density, number of edges divided by number of nodes (vertices)\n- 4 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 142 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\n\n\nFFmpeg repository\nhttps://github.com/FFmpeg/FFmpeg is a mirror of https://git.ffmpeg.org/ffmpeg.git\nFFmpeg is a collection of libraries and tools to process multimedia content such as audio, video, subtitles and related metadata.\n\nffmpeg_graph = commit_graph('https://github.com/FFmpeg/FFmpeg.git', 'ffmpeg')\n\nWall time: 2.14 s\n\n\nIt takes around 14 seconds to clone the FFmpeg repository, and construct it’s commit graph as NetworkX.DiGraph,\nand around 1.4 seconds to refresh it.\n\nknown_repos.append({\n    'name': 'ffmpeg',\n    'url': 'https://github.com/FFmpeg/FFmpeg.git',\n    'graph': ffmpeg_graph\n})\nknown_repos\n\n[{'name': 'curl',\n  'url': 'https://github.com/curl/curl',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3ca0&gt;},\n {'name': 'electron',\n  'url': 'https://github.com/electron/electron.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3970&gt;},\n {'name': 'ffmpeg',\n  'url': 'https://github.com/FFmpeg/FFmpeg.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3ee0&gt;}]\n\n\n\ngraph_idx = 2\nprint('commit graph [{}] of \"{}\" project has'.format(graph_idx, known_repos[graph_idx]['name']))\nprint('- {} nodes / vertices / commits'.format(known_repos[graph_idx]['graph'].number_of_nodes()))\nprint('- {} edges'.format(known_repos[graph_idx]['graph'].number_of_edges()))\nprint('- {:.5f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repos[graph_idx]['graph'].number_of_edges() / known_repos[graph_idx]['graph'].number_of_nodes()))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repos[graph_idx]['graph'] if known_repos[graph_idx]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repos[graph_idx]['graph'] if known_repos[graph_idx]['graph'].in_degree(n) == 0])))\n\ncommit graph [2] of \"ffmpeg\" project has\n- 120583 nodes / vertices / commits\n- 130606 edges\n- 1.08312 edge density, number of edges divided by number of nodes (vertices)\n- 1 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 30 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\n\n\nJGit repository\nJGit - An implementation of the Git version control system in pure Java\n\njgit_graph = commit_graph('https://github.com/eclipse/jgit.git', 'jgit')\n\nWall time: 195 ms\n\n\nIt takes around 34 seconds to clone the JGit repository, and construct it’s commit graph as NetworkX.DiGraph,\nand 70 ms to refresh it\n\n#known_repos[3] = {\nknown_repos.append({\n    'name': 'jgit',\n    'url': 'https://github.com/eclipse/jgit.git',\n    'graph': jgit_graph\n})\nknown_repos\n\n[{'name': 'curl',\n  'url': 'https://github.com/curl/curl',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3ca0&gt;},\n {'name': 'electron',\n  'url': 'https://github.com/electron/electron.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3970&gt;},\n {'name': 'ffmpeg',\n  'url': 'https://github.com/FFmpeg/FFmpeg.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3ee0&gt;},\n {'name': 'jgit',\n  'url': 'https://github.com/eclipse/jgit.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e79ea4f0&gt;}]\n\n\nOn 01.01.2021 the JGit project had 8,162 commits, according to GitHub web UI\n\ngraph_idx = 3\nprint('commit graph [{}] of \"{}\" project has'.format(graph_idx, known_repos[graph_idx]['name']))\nprint('- {} nodes / vertices / commits'.format(known_repos[graph_idx]['graph'].number_of_nodes()))\nprint('- {} edges'.format(known_repos[graph_idx]['graph'].number_of_edges()))\nprint('- {:.5f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repos[graph_idx]['graph'].number_of_edges() / known_repos[graph_idx]['graph'].number_of_nodes()))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repos[graph_idx]['graph'] if known_repos[graph_idx]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repos[graph_idx]['graph'] if known_repos[graph_idx]['graph'].in_degree(n) == 0])))\n\ncommit graph [3] of \"jgit\" project has\n- 8173 nodes / vertices / commits\n- 9853 edges\n- 1.20555 edge density, number of edges divided by number of nodes (vertices)\n- 1 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 2 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\n\n\nThe Julia language repository\nThe Julia Programming Language\nhttps://julialang.org/\nJulia is a high-level, high-performance dynamic language for technical computing.\n\njulia_graph = commit_graph('https://github.com/JuliaLang/julia.git', 'julia')\n\nWall time: 1.38 s\n\n\nIt takes around 32.7 seconds to run commit_graph() from scratch\nThe Julia language repository had 48,658 commits on 01.01.2021\n\nknown_repos.append({\n    'name': 'julia',\n    'url': 'https://github.com/JuliaLang/julia.git',\n    'homepage': 'https://julialang.org',\n    'graph': julia_graph\n})\nknown_repos\n\n[{'name': 'curl',\n  'url': 'https://github.com/curl/curl',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3ca0&gt;},\n {'name': 'electron',\n  'url': 'https://github.com/electron/electron.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3970&gt;},\n {'name': 'ffmpeg',\n  'url': 'https://github.com/FFmpeg/FFmpeg.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3ee0&gt;},\n {'name': 'jgit',\n  'url': 'https://github.com/eclipse/jgit.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e79ea4f0&gt;},\n {'name': 'julia',\n  'url': 'https://github.com/JuliaLang/julia.git',\n  'homepage': 'https://julialang.org',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195f34bdc40&gt;}]\n\n\n\ngraph_idx = 4\nprint('commit graph [{}] of \"{}\" project has'.format(graph_idx, known_repos[graph_idx]['name']))\nprint('- {} nodes / vertices / commits'.format(known_repos[graph_idx]['graph'].number_of_nodes()))\nprint('- {} edges'.format(known_repos[graph_idx]['graph'].number_of_edges()))\nprint('- {:.5f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repos[graph_idx]['graph'].number_of_edges() / known_repos[graph_idx]['graph'].number_of_nodes()))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repos[graph_idx]['graph'] if known_repos[graph_idx]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repos[graph_idx]['graph'] if known_repos[graph_idx]['graph'].in_degree(n) == 0])))\n\ncommit graph [4] of \"julia\" project has\n- 56048 nodes / vertices / commits\n- 67547 edges\n- 1.20516 edge density, number of edges divided by number of nodes (vertices)\n- 5 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 869 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\n\n\nThe Kotlin language repository\nThe Kotlin Programming Language\nhttps://kotlinlang.org/\nKotlin is an open-source, statically typed programming language supported and developed by JetBrains and open-source contributors.\n\nkotlin_graph = commit_graph('https://github.com/JetBrains/kotlin.git', 'kotlin')\n\nWall time: 2.74 s\n\n\n\nknown_repos.append({\n    'url': 'https://github.com/JetBrains/kotlin.git',\n    'name': 'kotlin',\n    'homepage': 'https://kotlinlang.org/',\n    'graph': kotlin_graph,\n})\nknown_repos\n\n[{'name': 'curl',\n  'url': 'https://github.com/curl/curl',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3ca0&gt;},\n {'name': 'electron',\n  'url': 'https://github.com/electron/electron.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3970&gt;},\n {'name': 'ffmpeg',\n  'url': 'https://github.com/FFmpeg/FFmpeg.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3ee0&gt;},\n {'name': 'jgit',\n  'url': 'https://github.com/eclipse/jgit.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e79ea4f0&gt;},\n {'name': 'julia',\n  'url': 'https://github.com/JuliaLang/julia.git',\n  'homepage': 'https://julialang.org',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195f34bdc40&gt;},\n {'url': 'https://github.com/JetBrains/kotlin.git',\n  'name': 'kotlin',\n  'homepage': 'https://kotlinlang.org/',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195f7f1a670&gt;}]\n\n\nGitHub repo @ 01.01.2021: 71,704 commits, 3,007 branches, 20,721 tags\n\ngraph_idx = 5\nprint('commit graph [{}] of \"{}\" project has'.format(graph_idx, known_repos[graph_idx]['name']))\nprint('- {} nodes / vertices / commits'.format(known_repos[graph_idx]['graph'].number_of_nodes()))\nprint('- {} edges'.format(known_repos[graph_idx]['graph'].number_of_edges()))\nprint('- {:.5f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repos[graph_idx]['graph'].number_of_edges() / known_repos[graph_idx]['graph'].number_of_nodes()))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repos[graph_idx]['graph'] if known_repos[graph_idx]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repos[graph_idx]['graph'] if known_repos[graph_idx]['graph'].in_degree(n) == 0])))\n\ncommit graph [5] of \"kotlin\" project has\n- 117495 nodes / vertices / commits\n- 119013 edges\n- 1.01292 edge density, number of edges divided by number of nodes (vertices)\n- 9 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 2592 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\n\n\nLinux repository\nLinux kernel source tree (on GitHub)\n\nlinux_graph = commit_graph('https://github.com/torvalds/linux.git', 'linux')\n\nWall time: 16.9 s\n\n\nWall time: 1min 44s (full clone and extracting the commit graph)\n\nknown_repos.append({\n    'url': 'https://github.com/torvalds/linux.git',\n    'name': 'linux',\n    'graph': linux_graph,\n})\nknown_repos\n\n[{'name': 'curl',\n  'url': 'https://github.com/curl/curl',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3ca0&gt;},\n {'name': 'electron',\n  'url': 'https://github.com/electron/electron.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3970&gt;},\n {'name': 'ffmpeg',\n  'url': 'https://github.com/FFmpeg/FFmpeg.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3ee0&gt;},\n {'name': 'jgit',\n  'url': 'https://github.com/eclipse/jgit.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e79ea4f0&gt;},\n {'name': 'julia',\n  'url': 'https://github.com/JuliaLang/julia.git',\n  'homepage': 'https://julialang.org',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195f34bdc40&gt;},\n {'url': 'https://github.com/JetBrains/kotlin.git',\n  'name': 'kotlin',\n  'homepage': 'https://kotlinlang.org/',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195f7f1a670&gt;},\n {'url': 'https://github.com/torvalds/linux.git',\n  'name': 'linux',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3550&gt;}]\n\n\n982,030 commits, 1 branch, 679 tags\n\ngraph_idx = 6\nprint('commit graph [{}] of \"{}\" project has'.format(graph_idx, known_repos[graph_idx]['name']))\nprint('- {} nodes / vertices / commits'.format(known_repos[graph_idx]['graph'].number_of_nodes()))\nprint('- {} edges'.format(known_repos[graph_idx]['graph'].number_of_edges()))\nprint('- {:.5f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repos[graph_idx]['graph'].number_of_edges() / known_repos[graph_idx]['graph'].number_of_nodes()))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repos[graph_idx]['graph'] if known_repos[graph_idx]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repos[graph_idx]['graph'] if known_repos[graph_idx]['graph'].in_degree(n) == 0])))\n\ncommit graph [6] of \"linux\" project has\n- 982030 nodes / vertices / commits\n- 1061095 edges\n- 1.08051 edge density, number of edges divided by number of nodes (vertices)\n- 4 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 1 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\n\n\nOdoo repository\nOdoo. Open Source Apps To Grow Your Business.\nhttps://www.odoo.com/\nOdoo is a suite of web based open source business apps.\n\nodoo_graph = commit_graph('https://github.com/odoo/odoo.git', 'odoo')\n\nWall time: 2.51 s\n\n\nWall time 1min 4s (first time commit_graph())\n\nknown_repo = {repo['name']: repo for repo in known_repos}\nknown_repo\n\n{'curl': {'name': 'curl',\n  'url': 'https://github.com/curl/curl',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3ca0&gt;},\n 'electron': {'name': 'electron',\n  'url': 'https://github.com/electron/electron.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3970&gt;},\n 'ffmpeg': {'name': 'ffmpeg',\n  'url': 'https://github.com/FFmpeg/FFmpeg.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3ee0&gt;},\n 'jgit': {'name': 'jgit',\n  'url': 'https://github.com/eclipse/jgit.git',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e79ea4f0&gt;},\n 'julia': {'name': 'julia',\n  'url': 'https://github.com/JuliaLang/julia.git',\n  'homepage': 'https://julialang.org',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195f34bdc40&gt;},\n 'kotlin': {'url': 'https://github.com/JetBrains/kotlin.git',\n  'name': 'kotlin',\n  'homepage': 'https://kotlinlang.org/',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195f7f1a670&gt;},\n 'linux': {'url': 'https://github.com/torvalds/linux.git',\n  'name': 'linux',\n  'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e59d3550&gt;}}\n\n\n\nknown_repo['odoo'] = {\n    'name': 'odoo',\n    'url': 'https://github.com/odoo/odoo.git',\n    'homepage': 'https://www.odoo.com/',\n    'graph': odoo_graph,\n}\nknown_repo['odoo']\n\n{'name': 'odoo',\n 'url': 'https://github.com/odoo/odoo.git',\n 'homepage': 'https://www.odoo.com/',\n 'graph': &lt;networkx.classes.digraph.DiGraph at 0x195f34bd4c0&gt;}\n\n\n\nrepo_name = 'odoo'\nprint('commit graph of \"{}\" project has:'.format(repo_name))\nprint('- {} nodes / vertices / commits'.format(known_repo[repo_name]['graph'].number_of_nodes()))\nprint('- {} edges'.format(known_repo[repo_name]['graph'].number_of_edges()))\nprint('- {:.6f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repo[repo_name]['graph'].number_of_edges() / known_repo[repo_name]['graph'].number_of_nodes()))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].in_degree(n) == 0])))\n\ncommit graph of \"odoo\" project has:\n- 158324 nodes / vertices / commits\n- 184403 edges\n- 1.164719 edge density, number of edges divided by number of nodes (vertices)\n- 7 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 33 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\nOn GitHub @ 01.01.2021: 139,989 commits, 82 branches, 0 tags\n\n\nOpenSSL repository\nOpenSSL is a robust, commercial-grade, full-featured Open Source Toolkit for the Transport Layer Security (TLS) protocol formerly known as the Secure Sockets Layer (SSL) protocol. The protocol implementation is based on a full-strength general purpose cryptographic library, which can also be used stand-alone.\nOpenSSL is descended from the SSLeay library developed by Eric A. Young and Tim J. Hudson.\nThe official Home Page of the OpenSSL Project is &lt;www.openssl.org&gt;.\n\nopenssl_graph = commit_graph('https://github.com/openssl/openssl.git', 'openssl')\n\nWall time: 763 ms\n\n\nWall time 18 seconds (first time)\n\nknown_repo['openssl'] = {\n    'name': 'openssl',\n    'url': 'https://github.com/openssl/openssl.git',\n    'homepage': 'https://www.openssl.org',\n    'graph': openssl_graph,\n}\nknown_repo['openssl']\n\n{'name': 'openssl',\n 'url': 'https://github.com/openssl/openssl.git',\n 'homepage': 'https://www.openssl.org',\n 'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e2d57430&gt;}\n\n\n\nrepo_name = 'openssl'\nprint('commit graph of \"{}\" project has:'.format(repo_name))\nprint('- {} nodes / vertices / commits'.format(known_repo[repo_name]['graph'].number_of_nodes()))\nprint('- {} edges'.format(known_repo[repo_name]['graph'].number_of_edges()))\nprint('- {:.6f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repo[repo_name]['graph'].number_of_edges() / known_repo[repo_name]['graph'].number_of_nodes()))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].in_degree(n) == 0])))\n\ncommit graph of \"openssl\" project has:\n- 42818 nodes / vertices / commits\n- 42997 edges\n- 1.004180 edge density, number of edges divided by number of nodes (vertices)\n- 6 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 19 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\nOpenSSL repo @ 01.01.2021: 27,812 commits, 20 branches, 320 tags\n\n\nThe Swift language repository\nThe Swift Programming Language\nhttps://swift.org\nSwift is a high-performance system programming language. It has a clean and modern syntax, offers seamless access to existing C and Objective-C code and frameworks, and is memory safe by default.\nAlthough inspired by Objective-C and many other languages, Swift is not itself a C-derived language. As a complete and independent language, Swift packages core features like flow control, data structures, and functions, with high-level constructs like objects, protocols, closures, and generics. Swift embraces modules, eliminating the need for headers and the code duplication they entail.\nTo learn more about the programming language, visit swift.org.\n\nswift_graph = commit_graph('https://github.com/apple/swift.git', 'swift')\n\nWall time: 4.25 s\n\n\nWall time: 41 s (first time)\n\nknown_repo['swift'] = {\n    'name': 'swift',\n    'url': 'https://github.com/apple/swift.git',\n    'homepage': 'https://swift.org',\n    'graph': swift_graph,\n    'n_nodes': swift_graph.number_of_nodes(),\n    'n_edges': swift_graph.number_of_edges(),\n}\nknown_repo['swift']\n\n{'name': 'swift',\n 'url': 'https://github.com/apple/swift.git',\n 'homepage': 'https://swift.org',\n 'graph': &lt;networkx.classes.digraph.DiGraph at 0x195f7f1a130&gt;,\n 'n_nodes': 132444,\n 'n_edges': 176028}\n\n\n\nrepo_name = 'swift'\nprint('commit graph of \"{}\" project has:'.format(repo_name))\nprint('- {} nodes / vertices / commits'.format(known_repo[repo_name]['graph'].number_of_nodes()))\nprint('- {} edges'.format(known_repo[repo_name]['graph'].number_of_edges()))\nprint('- {:.6f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repo[repo_name]['graph'].number_of_edges() / known_repo[repo_name]['graph'].number_of_nodes()))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].in_degree(n) == 0])))\n\ncommit graph of \"swift\" project has:\n- 132444 nodes / vertices / commits\n- 176028 edges\n- 1.329075 edge density, number of edges divided by number of nodes (vertices)\n- 1 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 209 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\nSwift repository on GitHub @ 01.01.2021: 114,559 commits\n\n\nTensorFlow repository\nAn Open Source Machine Learning Framework for Everyone\nhttps://tensorflow.org\nTensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries, and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications.\n\ntensorflow_graph = commit_graph('https://github.com/tensorflow/tensorflow.git', 'tensorflow')\n\nWall time: 2.12 s\n\n\nWall time: 34.3 s (first time)\n\nknown_repo['tensorflow'] = {\n    'name': 'tensorflow',\n    'url': 'https://github.com/tensorflow/tensorflow.git',\n    'homepage': 'https://tensorflow.org',\n    'graph': tensorflow_graph,\n    'n_nodes': tensorflow_graph.number_of_nodes(),\n    'n_edges': tensorflow_graph.number_of_edges(),\n}\nknown_repo['tensorflow']\n\n{'name': 'tensorflow',\n 'url': 'https://github.com/tensorflow/tensorflow.git',\n 'homepage': 'https://tensorflow.org',\n 'graph': &lt;networkx.classes.digraph.DiGraph at 0x195e79ea340&gt;,\n 'n_nodes': 105236,\n 'n_edges': 115687}\n\n\n\nrepo_name = 'tensorflow'\nprint('commit graph of \"{}\" project has:'.format(repo_name))\nprint('- {} nodes / vertices / commits'.format(known_repo[repo_name]['n_nodes']))\nprint('- {} edges'.format(known_repo[repo_name]['n_edges']))\nprint('- {:.6f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repo[repo_name]['n_edges'] / known_repo[repo_name]['n_nodes']))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].in_degree(n) == 0])))\n\ncommit graph of \"tensorflow\" project has:\n- 105236 nodes / vertices / commits\n- 115687 edges\n- 1.099310 edge density, number of edges divided by number of nodes (vertices)\n- 3 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 32 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\nTensorFlow repository on GitHub @ 01.01.2021: 102,131 commits, 36 branches\n\n\nThe TypeScript language repository\nTypeScript is a superset of JavaScript that compiles to clean JavaScript output.\nwww.typescriptlang.org\nTypeScript is a language for application-scale JavaScript. TypeScript adds optional types to JavaScript that support tools for large-scale JavaScript applications for any browser, for any host, on any OS. TypeScript compiles to readable, standards-based JavaScript.\n\ntypescript_graph = commit_graph('https://github.com/microsoft/TypeScript.git', 'typescript')\n\nWall time: 1.04 s\n\n\nWall time: 23.4 s (first time)\n\nknown_repo['typescript'] = {\n    'name': 'typescript',\n    'url': 'https://github.com/microsoft/TypeScript.git',\n    'homepage': 'https://www.typescriptlang.org/',\n    'graph': typescript_graph,\n    'n_nodes': typescript_graph.number_of_nodes(),\n    'n_edges': typescript_graph.number_of_edges(),\n}\nknown_repo['typescript']\n\n{'name': 'typescript',\n 'url': 'https://github.com/microsoft/TypeScript.git',\n 'homepage': 'https://www.typescriptlang.org/',\n 'graph': &lt;networkx.classes.digraph.DiGraph at 0x195f7f1ad90&gt;,\n 'n_nodes': 35486,\n 'n_edges': 43895}\n\n\n\nrepo_name = 'typescript'\nprint('commit graph of \"{}\" project has:'.format(repo_name))\nprint('- {} nodes / vertices / commits'.format(known_repo[repo_name]['n_nodes']))\nprint('- {} edges'.format(known_repo[repo_name]['n_edges']))\nprint('- {:.6f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repo[repo_name]['n_edges'] / known_repo[repo_name]['n_nodes']))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].in_degree(n) == 0])))\n\ncommit graph of \"typescript\" project has:\n- 35486 nodes / vertices / commits\n- 43895 edges\n- 1.236967 edge density, number of edges divided by number of nodes (vertices)\n- 1 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 419 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\nTypeScript repository on GitHub @ 01.01.2021: 31,596 commits, 455 branches\n\n\nGit project repository\nhttps://github.com/git/git is Git Source Code Mirror - This is a publish-only repository and all pull requests are ignored.\nhttps://git-scm.com/\nGit is a fast, scalable, distributed revision control system with an unusually rich command set that provides both high-level operations and full access to internals.\n\ngit_graph = commit_graph('https://github.com/git/git.git', 'git.git')\n\nWall time: 754 ms\n\n\n\nknown_repo['git.git'] = {\n    'name': 'git.git',\n    'graph_name': _repo_graph_name('repos/git.git'),\n    'url': 'https://github.com/git/git.git',\n    'homepage': 'https://git-scm.com/',\n    'graph': git_graph,\n    'n_nodes': git_graph.number_of_nodes(),\n    'n_edges': git_graph.number_of_edges(),\n}\nknown_repo['git.git']\n\n{'name': 'git.git',\n 'graph_name': 'git-commit_graph',\n 'url': 'https://github.com/git/git.git',\n 'homepage': 'https://git-scm.com/',\n 'graph': &lt;networkx.classes.digraph.DiGraph at 0x195f34bd640&gt;,\n 'n_nodes': 63829,\n 'n_edges': 79664}\n\n\n\nrepo_name = 'git.git'\nprint('commit graph of \"{}\" project has:'.format(repo_name))\nprint('- {} nodes / vertices / commits'.format(known_repo[repo_name]['n_nodes']))\nprint('- {} edges'.format(known_repo[repo_name]['n_edges']))\nprint('- {:.6f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repo[repo_name]['n_edges'] / known_repo[repo_name]['n_nodes']))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].in_degree(n) == 0])))\n\ncommit graph of \"git.git\" project has:\n- 63829 nodes / vertices / commits\n- 79664 edges\n- 1.248085 edge density, number of edges divided by number of nodes (vertices)\n- 9 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 3 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\nGit.git repository on GitHub @ 01.01.2021: 61,475 commits, 5 branches\n\n\nGerrit repository\nGerrit Code Review is a free, web-based team code collaboration tool. Software developers in a team can review each other’s modifications on their source code using a Web browser and approve or reject those changes. It integrates closely with Git, a distributed version control system.\nhttps://www.gerritcodereview.com/\n\ngerrit_graph = commit_graph('https://gerrit.googlesource.com/gerrit', 'gerrit')\n\nWall time: 402 ms\n\n\nWall time: 1min 2s (first time), 0.4s (subsequent times)\n\nknown_repo['gerrit'] = {\n    'name': 'gerrit',\n    'graph_name': _repo_graph_name('repos/gerrit'),\n    'url': 'https://gerrit.googlesource.com/gerrit',\n    'homepage': 'https://www.gerritcodereview.com/',\n    'graph': gerrit_graph,\n    'n_nodes': gerrit_graph.number_of_nodes(),\n    'n_edges': gerrit_graph.number_of_edges(),\n}\nknown_repo['gerrit']\n\nWall time: 51 ms\n\n\n{'name': 'gerrit',\n 'graph_name': 'gerrit-commit_graph',\n 'url': 'https://gerrit.googlesource.com/gerrit',\n 'homepage': 'https://www.gerritcodereview.com/',\n 'graph': &lt;networkx.classes.digraph.DiGraph at 0x196101db850&gt;,\n 'n_nodes': 47687,\n 'n_edges': 63944}\n\n\n\nrepo_name = 'gerrit'\nprint('commit graph of \"{}\" project has:'.format(repo_name))\nprint('- {} nodes / vertices / commits'.format(known_repo[repo_name]['n_nodes']))\nprint('- {} edges'.format(known_repo[repo_name]['n_edges']))\nprint('- {:.6f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repo[repo_name]['n_edges'] / known_repo[repo_name]['n_nodes']))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].in_degree(n) == 0])))\n\ncommit graph of \"gerrit\" project has:\n- 47687 nodes / vertices / commits\n- 63944 edges\n- 1.340911 edge density, number of edges divided by number of nodes (vertices)\n- 4 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 6 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\n\n\nAndroid-base repository\n\nandroid_graph = commit_graph('https://android.googlesource.com/platform/frameworks/base/', 'android-base')\n\nWall time: 9.33 s\n\n\nWall time: 2min 9s (first time), 20.7 s (rescan), 9.33 s (subsequent runs)\n\n#%%time\n#!git clone --filter=tree:0 --no-checkout https://android.googlesource.com/platform/frameworks/base/ repos/android-base\n\n\nknown_repo['android-base'] = {\n    'name': 'android-base',\n    'url': 'https://android.googlesource.com/platform/frameworks/base/',\n    'graph': android_graph,\n    'n_nodes': android_graph.number_of_nodes(),\n    'n_edges': android_graph.number_of_edges(),\n}\nknown_repo['android-base']\n\n{'name': 'android-base',\n 'url': 'https://android.googlesource.com/platform/frameworks/base/',\n 'graph': &lt;networkx.classes.digraph.DiGraph at 0x1961516dc10&gt;,\n 'n_nodes': 559423,\n 'n_edges': 984070}\n\n\n\nrepo_name = 'android-base'\nprint('commit graph of \"{}\" project has:'.format(repo_name))\nprint('- {} nodes / vertices / commits'.format(known_repo[repo_name]['n_nodes']))\nprint('- {} edges'.format(known_repo[repo_name]['n_edges']))\nprint('- {:.6f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repo[repo_name]['n_edges'] / known_repo[repo_name]['n_nodes']))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].in_degree(n) == 0])))\n\ncommit graph of \"android-base\" project has:\n- 559423 nodes / vertices / commits\n- 984070 edges\n- 1.759080 edge density, number of edges divided by number of nodes (vertices)\n- 3 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 1 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\n\n\nChromium repository\nChromium is an open-source browser project that aims to build a safer, faster, and more stable way for all users to experience the web.\nThe project’s web site is https://www.chromium.org.\n\nchromium_graph = commit_graph('https://chromium.googlesource.com/chromium/src', 'chromium')\n\nWall time: 14.1 s\n\n\nWall time: 39min 52s (first time, possibly failed), rescan: 39.3 s, subsequent runs: 14.1 s\n\nknown_repo['chromium'] = {\n    'name': 'chromium',\n    'url': 'https://chromium.googlesource.com/chromium/src',\n    'homepage': 'https://www.chromium.org',\n    'graph': chromium_graph,\n    'n_nodes': chromium_graph.number_of_nodes(),\n    'n_edges': chromium_graph.number_of_edges(),\n}\nknown_repo['chromium']\n\n{'name': 'chromium',\n 'url': 'https://chromium.googlesource.com/chromium/src',\n 'homepage': 'https://www.chromium.org',\n 'graph': &lt;networkx.classes.digraph.DiGraph at 0x196101db7c0&gt;,\n 'n_nodes': 961204,\n 'n_edges': 961263}\n\n\n\nrepo_name = 'chromium'\nprint('commit graph of \"{}\" project has:'.format(repo_name))\nprint('- {} nodes / vertices / commits'.format(known_repo[repo_name]['n_nodes']))\nprint('- {} edges'.format(known_repo[repo_name]['n_edges']))\nprint('- {:.6f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repo[repo_name]['n_edges'] / known_repo[repo_name]['n_nodes']))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].in_degree(n) == 0])))\n\ncommit graph of \"chromium\" project has:\n- 961204 nodes / vertices / commits\n- 961263 edges\n- 1.000061 edge density, number of edges divided by number of nodes (vertices)\n- 3 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 1 sources / roots / heads in git parlance, nodes with no incoming edge"
  },
  {
    "objectID": "repos.html#slides-unused-repositories",
    "href": "repos.html#slides-unused-repositories",
    "title": "Large Git repositories",
    "section": "Slides (unused repositories)",
    "text": "Slides (unused repositories)\nThese are the repositories (not present among the repositories above) that were considered for inclusion in “Graph operations in Git version control system”, and are presented as comments in the *.tex sources.\n\nLLVM repository\nThe LLVM Compiler Infrastructure\nhttps://llvm.org/\nThe LLVM Project is a collection of modular and reusable compiler and toolchain technologies. Despite its name, LLVM has little to do with traditional virtual machines. The name “LLVM” itself is not an acronym; it is the full name of the project.\n\nllvm_graph = commit_graph('https://github.com/llvm/llvm-project.git', 'llvm')\n\nWall time: 3.53 s\n\n\nWall time: 33.5 s (first time), 3.53 s (subsequent run)\n\nknown_repo['llvm'] = {\n    'name': 'llvm',\n    'url': 'https://github.com/llvm/llvm-project.git',\n    'homepage': 'https://llvm.org/',\n    'graph': llvm_graph,\n    'n_nodes': llvm_graph.number_of_nodes(),\n    'n_edges': llvm_graph.number_of_edges(),\n}\nknown_repo['llvm']\n\n{'name': 'llvm',\n 'url': 'https://github.com/llvm/llvm-project.git',\n 'homepage': 'https://llvm.org/',\n 'graph': &lt;networkx.classes.digraph.DiGraph at 0x1969020a430&gt;,\n 'n_nodes': 381471,\n 'n_edges': 381486}\n\n\n\nrepo_name = 'llvm'\nprint('commit graph of \"{}\" project has:'.format(repo_name))\nprint('- {} nodes / vertices / commits'.format(known_repo[repo_name]['n_nodes']))\nprint('- {} edges'.format(known_repo[repo_name]['n_edges']))\nprint('- {:.6f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repo[repo_name]['n_edges'] / known_repo[repo_name]['n_nodes']))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].in_degree(n) == 0])))\n\ncommit graph of \"llvm\" project has:\n- 381471 nodes / vertices / commits\n- 381486 edges\n- 1.000039 edge density, number of edges divided by number of nodes (vertices)\n- 3 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 40 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\nThe LLVM repository on GitHub @ 01.01.2021: 375,962 commits, 42 branches\n\n\nGCC repository\nGCC, the GNU Compiler Collection\nhttps://gcc.gnu.org/\nThe GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, Go, and D, as well as libraries for these languages (libstdc++,…). GCC was originally written as the compiler for the GNU operating system.\n\ngcc_graph = commit_graph('https://gcc.gnu.org/git/gcc.git', 'gcc')\n\nWall time: 2.15 s\n\n\nWall time: 13min 48s (first time), 2.15s (subsequent runs)\n\nknown_repo['gcc'] = {\n    'name': 'gcc',\n    'url': 'https://gcc.gnu.org/git/gcc.git',\n    'homepage': 'https://gcc.gnu.org/',\n    'graph': gcc_graph,\n    'n_nodes': gcc_graph.number_of_nodes(),\n    'n_edges': gcc_graph.number_of_edges(),\n}\nknown_repo['gcc']\n\n{'name': 'gcc',\n 'url': 'https://gcc.gnu.org/git/gcc.git',\n 'homepage': 'https://gcc.gnu.org/',\n 'graph': &lt;networkx.classes.digraph.DiGraph at 0x1969020a250&gt;,\n 'n_nodes': 241243,\n 'n_edges': 242024}\n\n\n\nrepo_name = 'gcc'\nprint('commit graph of \"{}\" project has:'.format(repo_name))\nprint('- {} nodes / vertices / commits'.format(known_repo[repo_name]['n_nodes']))\nprint('- {} edges'.format(known_repo[repo_name]['n_edges']))\nprint('- {:.6f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repo[repo_name]['n_edges'] / known_repo[repo_name]['n_nodes']))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].in_degree(n) == 0])))\n\ncommit graph of \"gcc\" project has:\n- 241243 nodes / vertices / commits\n- 242024 edges\n- 1.003237 edge density, number of edges divided by number of nodes (vertices)\n- 1 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 45 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\n\n\nHomebrew repository\nHomebrew – The Missing Package Manager for macOS (or Linux)\nhttps://brew.sh/\nHomebrew is a free and open-source software package management system that simplifies the installation of software on Apple’s macOS operating system and Linux. The name is intended to suggest the idea of building software on the Mac depending on the user’s taste.\n\nbrew_graph = commit_graph('https://github.com/Homebrew/brew.git', 'homebrew')\n\nWall time: 452 ms\n\n\nWall time: 28 s (first time), 452 ms (subsequent times)\n\nknown_repo['homebrew'] = {\n    'name': 'homebrew',\n    'url': 'https://github.com/Homebrew/brew.git',\n    'homepage': 'https://brew.sh/',\n    'graph': brew_graph,\n    'n_nodes': brew_graph.number_of_nodes(),\n    'n_edges': brew_graph.number_of_edges(),\n}\nknown_repo['homebrew']\n\n{'name': 'homebrew',\n 'url': 'https://github.com/Homebrew/brew.git',\n 'homepage': 'https://brew.sh/',\n 'graph': &lt;networkx.classes.digraph.DiGraph at 0x1961516dbb0&gt;,\n 'n_nodes': 25286,\n 'n_edges': 30956}\n\n\n\nrepo_name = 'homebrew'\nprint('commit graph of \"{}\" project has:'.format(repo_name))\nprint('- {} nodes / vertices / commits'.format(known_repo[repo_name]['n_nodes']))\nprint('- {} edges'.format(known_repo[repo_name]['n_edges']))\nprint('- {:.6f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repo[repo_name]['n_edges'] / known_repo[repo_name]['n_nodes']))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].in_degree(n) == 0])))\n\ncommit graph of \"homebrew\" project has:\n- 25286 nodes / vertices / commits\n- 30956 edges\n- 1.224235 edge density, number of edges divided by number of nodes (vertices)\n- 2 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 4 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\nHomebrew (brew) repository on GitHub @ 05.01.2021: 25,281 commits, 4 branches\n\n\nRuby on Rails repository\nRuby on Rails | A web-application framework that includes everything needed to create database-backed web applications according to the Model-View-Controller (MVC) pattern.\nhttps://rubyonrails.org/\n\nrails_graph = commit_graph('https://github.com/rails/rails.git', 'rails')\n\nWall time: 776 ms\n\n\nWall time: 57.1 s (first time), 776 ms (subsequent times)\n\nknown_repo['rails'] = {\n    'name': 'rails',\n    'url': 'https://github.com/rails/rails.git',\n    'homepage': 'https://rubyonrails.org/',\n    'graph': rails_graph,\n    'n_nodes': rails_graph.number_of_nodes(),\n    'n_edges': rails_graph.number_of_edges(),\n}\nknown_repo['rails']\n\n{'name': 'rails',\n 'url': 'https://github.com/rails/rails.git',\n 'homepage': 'https://rubyonrails.org/',\n 'graph': &lt;networkx.classes.digraph.DiGraph at 0x195f34bd460&gt;,\n 'n_nodes': 92577,\n 'n_edges': 111930}\n\n\n\nrepo_name = 'rails'\nprint('commit graph of \"{}\" project has:'.format(repo_name))\nprint('- {} nodes / vertices / commits'.format(known_repo[repo_name]['n_nodes']))\nprint('- {} edges'.format(known_repo[repo_name]['n_edges']))\nprint('- {:.6f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repo[repo_name]['n_edges'] / known_repo[repo_name]['n_nodes']))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].in_degree(n) == 0])))\n\ncommit graph of \"rails\" project has:\n- 92577 nodes / vertices / commits\n- 111930 edges\n- 1.209048 edge density, number of edges divided by number of nodes (vertices)\n- 7 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 48 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\nRuby on Rails repository on GitHub @ 05.01.2021: 78,831 commits, 52 branches"
  },
  {
    "objectID": "repos.html#github-blog-post-on-cloning-behavior",
    "href": "repos.html#github-blog-post-on-cloning-behavior",
    "title": "Large Git repositories",
    "section": "GitHub Blog post on cloning behavior",
    "text": "GitHub Blog post on cloning behavior\n“Git clone: a data-driven study on cloning behaviors”\nSolmaz Abbaspoursani, December 22, 2020\nhttps://github.blog/2020-12-22-git-clone-a-data-driven-study-on-cloning-behaviors/\n\njQuery repository\njQuery is a fast, small, and feature-rich JavaScript library. It makes things like HTML document traversal and manipulation, event handling, animation, and Ajax much simpler with an easy-to-use API that works across a multitude of browsers.\n\njquery_graph = commit_graph('https://github.com/jquery/jquery.git', 'jquery')\n\nWall time: 69 ms\n\n\nWall time: 12.7 s (first time), 0.07 s (subsequent times)\n\nknown_repo['jquery'] = {\n    'name': 'jquery',\n    'url': 'https://github.com/jquery/jquery.git',\n    'homepage': 'https://jquery.com/',\n    'graph': jquery_graph,\n    'n_nodes': jquery_graph.number_of_nodes(),\n    'n_edges': jquery_graph.number_of_edges(),\n}\nknown_repo['jquery']\n\n{'name': 'jquery',\n 'url': 'https://github.com/jquery/jquery.git',\n 'homepage': 'https://jquery.com/',\n 'graph': &lt;networkx.classes.digraph.DiGraph at 0x196ca968820&gt;,\n 'n_nodes': 7741,\n 'n_edges': 7990}\n\n\n\nrepo_name = 'jquery'\nprint('commit graph of \"{}\" project has:'.format(repo_name))\nprint('- {} nodes / vertices / commits'.format(known_repo[repo_name]['n_nodes']))\nprint('- {} edges'.format(known_repo[repo_name]['n_edges']))\nprint('- {:.6f} edge density, number of edges divided by number of nodes (vertices)'.\n      format(known_repo[repo_name]['n_edges'] / known_repo[repo_name]['n_nodes']))\nprint('- {} sinks / leafs / roots in git parlance, nodes with no outgoing edges'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].out_degree(n) == 0])))\nprint('- {} sources / roots / heads in git parlance, nodes with no incoming edge'.\n     format(len([n for n in known_repo[repo_name]['graph'] if known_repo[repo_name]['graph'].in_degree(n) == 0])))\n\ncommit graph of \"jquery\" project has:\n- 7741 nodes / vertices / commits\n- 7990 edges\n- 1.032166 edge density, number of edges divided by number of nodes (vertices)\n- 1 sinks / leafs / roots in git parlance, nodes with no outgoing edges\n- 4 sources / roots / heads in git parlance, nodes with no incoming edge\n\n\njQuery repository on GitHub on 05.01.2021: 6,519 commits, 4 branches"
  },
  {
    "objectID": "repos.html#other-possibilities",
    "href": "repos.html#other-possibilities",
    "title": "Large Git repositories",
    "section": "Other possibilities",
    "text": "Other possibilities\nBrowsing through OpenHub and GitHub (neither of which provides a search mechanism or a sort order for the number of commits), the following repositories may be considered for inclusion:\n\nOpenBSD, with around 650,000 commits\nFreeBSD, with around 250,000 commits\nLibreOffice, with around 500,000 commits\nMozilla Firefox, with around 740,000 commits\nGNU Emacs, with around 150,000 commits\n\nAnother good source for large repositories might be the Git Merge 2017 talk “Top Ten Worst Repositories to host on GitHub - Git Merge 2017” (on YouTube)\n\nKubernetes (large amoun of pull requests, around 100,000 commits)\nCocoaPods/Specs (with around 550,000 commits, automatic)\noctocats/Spoon-Knife (3 commits, 5k+ pull requests, 115k forks)\nIntelliJ IDEA Community Edition (around 300,000 commits, around 700 tags - was 45k tags)\nEnkiDevs/commit (around 150,000 commits, 26k individual authors, leaderboard, archived)"
  },
  {
    "objectID": "repos.html#summary",
    "href": "repos.html#summary",
    "title": "Large Git repositories",
    "section": "Summary",
    "text": "Summary\nImports for data analysis\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nfor name, repo in known_repo.items():\n    print('... processing {}'.format(name))\n    if 'n_nodes' not in repo:\n        repo['n_nodes'] = repo['graph'].number_of_nodes()\n    if 'n_edges' not in repo:\n        repo['n_edges'] = repo['graph'].number_of_edges()\n\n... processing curl\n... processing electron\n... processing ffmpeg\n... processing jgit\n... processing julia\n... processing kotlin\n... processing linux\n... processing odoo\n... processing openssl\n... processing swift\n... processing tensorflow\n... processing typescript\n... processing git.git\n... processing gerrit\n... processing android-base\n... processing chromium\n... processing llvm\n... processing gcc\n... processing homebrew\n... processing rails\n\n\n\nknown_repo_df = pd.DataFrame.from_dict(known_repo, orient='index',\n                                       columns=['url', 'n_nodes', 'n_edges']).astype({'url': 'str'})\nknown_repo_df['n_edges / n_nodes'] = known_repo_df['n_edges'] / known_repo_df['n_nodes']\nknown_repo_df.sort_values('n_nodes')\n\n\n\n\n\n\n\n\nurl\nn_nodes\nn_edges\nn_edges / n_nodes\n\n\n\n\njquery\nhttps://github.com/jquery/jquery.git\n7741\n7990\n1.032166\n\n\njgit\nhttps://github.com/eclipse/jgit.git\n8173\n9853\n1.205555\n\n\nhomebrew\nhttps://github.com/Homebrew/brew.git\n25286\n30956\n1.224235\n\n\ncurl\nhttps://github.com/curl/curl\n26609\n26631\n1.000827\n\n\nelectron\nhttps://github.com/electron/electron.git\n29956\n34167\n1.140573\n\n\ntypescript\nhttps://github.com/microsoft/TypeScript.git\n35486\n43895\n1.236967\n\n\nopenssl\nhttps://github.com/openssl/openssl.git\n42818\n42997\n1.004180\n\n\ngerrit\nhttps://gerrit.googlesource.com/gerrit\n47687\n63944\n1.340911\n\n\njulia\nhttps://github.com/JuliaLang/julia.git\n56048\n67547\n1.205163\n\n\ngit.git\nhttps://github.com/git/git.git\n63829\n79664\n1.248085\n\n\nrails\nhttps://github.com/rails/rails.git\n92577\n111930\n1.209048\n\n\ntensorflow\nhttps://github.com/tensorflow/tensorflow.git\n105236\n115687\n1.099310\n\n\nkotlin\nhttps://github.com/JetBrains/kotlin.git\n117495\n119013\n1.012920\n\n\nffmpeg\nhttps://github.com/FFmpeg/FFmpeg.git\n120583\n130606\n1.083121\n\n\nswift\nhttps://github.com/apple/swift.git\n132444\n176028\n1.329075\n\n\nodoo\nhttps://github.com/odoo/odoo.git\n158324\n184403\n1.164719\n\n\ngcc\nhttps://gcc.gnu.org/git/gcc.git\n241243\n242024\n1.003237\n\n\nllvm\nhttps://github.com/llvm/llvm-project.git\n381471\n381486\n1.000039\n\n\nandroid-base\nhttps://android.googlesource.com/platform/fram...\n559423\n984070\n1.759080\n\n\nchromium\nhttps://chromium.googlesource.com/chromium/src\n961204\n961263\n1.000061\n\n\nlinux\nhttps://github.com/torvalds/linux.git\n982030\n1061095\n1.080512\n\n\n\n\n\n\n\n\nknown_repo_df.dtypes\n\nurl                   object\nn_nodes                int64\nn_edges                int64\nn_edges / n_nodes    float64\ndtype: object\n\n\nMakit it so that the labels do not overlap.\nhttps://support.sisense.com/hc/en-us/community/posts/360037908374-Getting-Around-Overlapping-Data-Labels-With-Python\nhttps://github.com/Phlya/adjustText/\n\n# source: https://stackoverflow.com/questions/8850142/matplotlib-overlapping-annotations\ndef get_text_positions(x_data, y_data, txt_width, txt_height):\n    a = zip(y_data, x_data)\n    text_positions = y_data.copy()\n    for index, (y, x) in enumerate(a):\n        local_text_positions = [i for i in a if i[0] &gt; (y - txt_height)\n                                and (abs(i[1] - x) &lt; txt_width * 2) and i != (y,x)]\n        if local_text_positions:\n            sorted_ltp = sorted(local_text_positions)\n            if abs(sorted_ltp[0][0] - y) &lt; txt_height: #True == collision\n                differ = np.diff(sorted_ltp, axis=0)\n                a[index] = (sorted_ltp[-1][0] + txt_height, a[index][1])\n                text_positions[index] = sorted_ltp[-1][0] + txt_height\n                for k, (j, m) in enumerate(differ):\n                    #j is the vertical distance between words\n                    if j &gt; txt_height * 1.5: #if True then room to fit a word in\n                        a[index] = (sorted_ltp[k][0] + txt_height, a[index][1])\n                        text_positions[index] = sorted_ltp[k][0] + txt_height\n                        break\n    return text_positions\n\n\ndef text_plotter(x_data, y_data, text_positions, axis,txt_width,txt_height):\n    for x,y,t in zip(x_data, y_data, text_positions):\n        axis.text(x - .03, 1.02*t, '%d'%int(y),rotation=0, color='blue', fontsize=13)\n        if y != t:\n            axis.arrow(x, t+20,0,y-t, color='blue',alpha=0.2, width=txt_width*0.0,\n                       head_width=.02, head_length=txt_height*0.5,\n                       zorder=0,length_includes_head=True)\n\n\n# plot\nfig, ax = plt.subplots(figsize=(18, 10))\nax = known_repo_df.plot.scatter(x='n_nodes', y='n_edges',\n                                logx=True, logy=True,\n                                grid=True, ax=ax,\n                                title='Commit graphs of Git repositories')\n# line\nee = np.geomspace(5000,1100000,num=10)\nax.plot(ee, ee, '-.', label='nodes == edges', c='gray')\n# parameters\nax.set_xlim(left  =5e3)\nax.set_ylim(bottom=5e3)\n#plt.legend(loc='upper left')\n# labels\nfor row in known_repo_df.itertuples():\n    ax.annotate(row[0], xy=(row.n_nodes, row.n_edges),\n                xytext=(-3,2), textcoords='offset points', horizontalalignment='right',\n                family='sans-serif', fontsize=9, color='darkslategrey')\n# show\nplt.show()"
  },
  {
    "objectID": "evaluation.html",
    "href": "evaluation.html",
    "title": "Reachability evaluation",
    "section": "",
    "text": "Imports\nimport numpy as np\nimport networkx as nx\nimport pandas as pd\n\nfrom math import sqrt\nfrom pathlib import Path"
  },
  {
    "objectID": "evaluation.html#load-example-commit-graph-compute-reachability-queries-save-results",
    "href": "evaluation.html#load-example-commit-graph-compute-reachability-queries-save-results",
    "title": "Reachability evaluation",
    "section": "Load example commit graph, compute reachability queries, save results",
    "text": "Load example commit graph, compute reachability queries, save results\n\nfrom git_commit_graph_ext.commit_graph import _commit_graph_name, commit_graph\nfrom git_commit_graph_ext.checkpoint import compute_cached_graph, compute_cached_reachability_labels, save_df_to_file\n\n\n# parameters\nrepo_url  = 'https://github.com/git/git.git' \nrepo_name = 'git'\n\n\ngraph_name = _commit_graph_name(repo_name)\n\n\nprint('loading the commit graph of a git repository')\nprint('- url:   {}'.format(repo_url))\nprint('- name:  {}'.format(repo_name))\nprint('- graph: {}'.format(graph_name))\n\nloading the commit graph of a git repository\n- url:   https://github.com/git/git.git\n- name:  git\n- graph: git-commit_graph\n\n\n\n# checkpoint 1\ngraph = compute_cached_graph(lambda: commit_graph(repo_url, repo_name), graph_name)\n\n\nprint('commit graph of {} repository has:'.format(repo_name))\nprint('- nodes: {}'.format(graph.number_of_nodes()))\nprint('- edges: {}'.format(graph.number_of_edges()))\nprint('\\npublic attributes of retrieved/computed graph')\nfor (attr, val) in graph.__dict__.items():\n    if not isinstance(val, type) and not attr.startswith('_'):\n        print('- {:s} ({})'.format(attr, type(val)))\n\ncommit graph of git repository has:\n- nodes: 63829\n- edges: 79664\n\npublic attributes of retrieved/computed graph\n- graph (&lt;class 'dict'&gt;)\n- df_edgelist (&lt;class 'pandas.core.frame.DataFrame'&gt;)\n\n\n\ngraph = compute_cached_reachability_labels(graph, graph_name)\n\n\ndf = graph.df_nodedata\nprint('public attributes of retrieved/computed graph (with reachability labels):')\nfor (attr, val) in graph.__dict__.items():\n    if not isinstance(val, type) and not attr.startswith('_'):\n        print('- {:s} ({})'.format(attr, type(val)))\n\nprint('')\nprint('node data dataframe properties:')\nprint('- columns: {}'.format(df.columns.tolist()))\nprint('- rows:    {}...'.format(list(graph.nodes)[0:5]))\nprint('- lvls:    {}...'.format({k: graph.lvl[k] for k in list(graph.lvl)[:5]}))\nprint('- mpi_ext: {}...'.format({k: graph.mpi_ext[k] for k in list(graph.mpi_ext)[:2]}))\n\ngraph.df_nodedata.head()\n\npublic attributes of retrieved/computed graph (with reachability labels):\n- graph (&lt;class 'dict'&gt;)\n- df_edgelist (&lt;class 'pandas.core.frame.DataFrame'&gt;)\n- df_nodedata (&lt;class 'pandas.core.frame.DataFrame'&gt;)\n- lvl (&lt;class 'dict'&gt;)\n- mpi_ext (&lt;class 'dict'&gt;)\n- nodes (&lt;class 'networkx.classes.reportviews.NodeView'&gt;)\n\nnode data dataframe properties:\n- columns: ['f_min', 'min', 'post', 'level', 'in degree', 'out degree', 'degree']\n- rows:    ['836aadd78', 'a93475d10', '55fce44a3', 'df525e622', '6d9d59c31']...\n- lvls:    {'e83c51633': 0, '8bc9a0c76': 1, 'e497ea2a9': 2, 'bf0c6e839': 3, '19b2860cb': 4}...\n- mpi_ext: {'e83c51633': {'f_min': 1, 'min': 1, 'post': 1}, '8bc9a0c76': {'f_min': 1, 'min': 1, 'post': 2}}...\n\n\n\n\n\n\n\n\n\nf_min\nmin\npost\nlevel\nin degree\nout degree\ndegree\n\n\nnode\n\n\n\n\n\n\n\n\n\n\n\ne83c51633\n1\n1\n1\n0\n1\n0\n1\n\n\n8bc9a0c76\n1\n1\n2\n1\n1\n1\n2\n\n\ne497ea2a9\n1\n1\n3\n2\n1\n1\n2\n\n\nbf0c6e839\n1\n1\n4\n3\n1\n1\n2\n\n\n19b2860cb\n1\n1\n5\n4\n1\n1\n2"
  },
  {
    "objectID": "evaluation.html#n2-connectivity-on-random-sample-of-commits-nodes",
    "href": "evaluation.html#n2-connectivity-on-random-sample-of-commits-nodes",
    "title": "Reachability evaluation",
    "section": "N^2 connectivity on random sample of commits / nodes",
    "text": "N^2 connectivity on random sample of commits / nodes\n\nn_nodes = graph.number_of_nodes()\nprint('graph \"{}\" has {:d} nodes'.format(graph_name, n_nodes))\n\nn_pairs = 10000\nprint('- selecting {:d} pairs of nodes out of {}'.format(n_pairs, n_nodes*n_nodes))\nchoice_u = np.random.choice(n_nodes, n_pairs)\nchoice_v = np.random.choice(n_nodes, n_pairs)\nconn_choice = list(zip(choice_u, choice_v))\nprint('- choice: {}...'.format(conn_choice[:5]))\n\nprint('- creating a mapping from numbers to node names')\nnodes_dict = {num: v for (num,v) in enumerate(list(graph))}\nconn_nodes = [(nodes_dict[u], nodes_dict[v]) for (u,v) in conn_choice]\n\nprint('- %4d sample size' % len(conn_nodes))\nprint('- node pairs: {}...'.format(conn_nodes[:5]))\n\ngraph \"git-commit_graph\" has 63829 nodes\n- selecting 10000 pairs of nodes out of 4074141241\n- choice: [(54306, 8405), (63370, 27936), (40309, 45188), (33813, 2815), (32667, 23583)]...\n- creating a mapping from numbers to node names\n- 10000 sample size\n- node pairs: [('c7f34c180', '23c204455'), ('9dc527adb', '53ec551c8'), ('5e3ce663b', '9affecbc8'), ('6440fdbab', 'c8c35f6a0'), ('f1a7082f2', 'caac7a3ab')]...\n\n\n\nconn_sample = []\nprint('sample of {:d} node pairs in {} graph'.format(len(conn_nodes), graph_name))\nfor (u,v) in conn_nodes:\n  #print('%r -&gt; %r: %r' % (u, v, nx.has_path(linux_graph_full, u, v)))\n  conn_sample.append({'u': u, 'v': v,\n                      'l_u': graph.lvl[u],\n                      'l_v': graph.lvl[v],\n                      'u-&gt;v': nx.has_path(graph, u, v)})\n\nconn_sample[:5]\n\nsample of 10000 node pairs in git-commit_graph graph\nWall time: 3min 26s\n\n\n[{'u': 'c7f34c180',\n  'v': '23c204455',\n  'l_u': 5579,\n  'l_v': 19392,\n  'u-&gt;v': False},\n {'u': '9dc527adb', 'v': '53ec551c8', 'l_u': 433, 'l_v': 12473, 'u-&gt;v': False},\n {'u': '5e3ce663b', 'v': '9affecbc8', 'l_u': 11072, 'l_v': 9538, 'u-&gt;v': True},\n {'u': '6440fdbab',\n  'v': 'c8c35f6a0',\n  'l_u': 12391,\n  'l_v': 20919,\n  'u-&gt;v': False},\n {'u': 'f1a7082f2',\n  'v': 'caac7a3ab',\n  'l_u': 12813,\n  'l_v': 15628,\n  'u-&gt;v': False}]\n\n\nWall time: 3min 5s\n\nconn_sample_df = pd.DataFrame.from_records(conn_sample)\nconn_sample_df\n\n\n\n\n\n\n\n\nu\nv\nl_u\nl_v\nu-&gt;v\n\n\n\n\n0\nc7f34c180\n23c204455\n5579\n19392\nFalse\n\n\n1\n9dc527adb\n53ec551c8\n433\n12473\nFalse\n\n\n2\n5e3ce663b\n9affecbc8\n11072\n9538\nTrue\n\n\n3\n6440fdbab\nc8c35f6a0\n12391\n20919\nFalse\n\n\n4\nf1a7082f2\ncaac7a3ab\n12813\n15628\nFalse\n\n\n...\n...\n...\n...\n...\n...\n\n\n9995\n9be24a30d\n26b59b481\n1769\n11394\nFalse\n\n\n9996\n2a7453241\n80a14665b\n11757\n13769\nFalse\n\n\n9997\n577ed5c20\na75ef3ff9\n4435\n17803\nFalse\n\n\n9998\n6f5c77a11\n81811a74b\n17947\n288\nTrue\n\n\n9999\nc3a3db7d4\n557c5895c\n20960\n20011\nTrue\n\n\n\n\n10000 rows × 5 columns\n\n\n\n\nconn_sample_df['v-&gt;u'] = False\nmask = ~conn_sample_df['u-&gt;v']\nconn_sample_df.loc[mask, 'v-&gt;u'] = \\\n    conn_sample_df[mask].apply(lambda row: nx.has_path(graph, row['v'], row['u']), axis='columns')\nconn_sample_df\n\nWall time: 40.7 s\n\n\n\n\n\n\n\n\n\nu\nv\nl_u\nl_v\nu-&gt;v\nv-&gt;u\n\n\n\n\n0\nc7f34c180\n23c204455\n5579\n19392\nFalse\nTrue\n\n\n1\n9dc527adb\n53ec551c8\n433\n12473\nFalse\nTrue\n\n\n2\n5e3ce663b\n9affecbc8\n11072\n9538\nTrue\nFalse\n\n\n3\n6440fdbab\nc8c35f6a0\n12391\n20919\nFalse\nTrue\n\n\n4\nf1a7082f2\ncaac7a3ab\n12813\n15628\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\n9be24a30d\n26b59b481\n1769\n11394\nFalse\nFalse\n\n\n9996\n2a7453241\n80a14665b\n11757\n13769\nFalse\nTrue\n\n\n9997\n577ed5c20\na75ef3ff9\n4435\n17803\nFalse\nTrue\n\n\n9998\n6f5c77a11\n81811a74b\n17947\n288\nTrue\nFalse\n\n\n9999\nc3a3db7d4\n557c5895c\n20960\n20011\nTrue\nFalse\n\n\n\n\n10000 rows × 6 columns\n\n\n\nWall time 41.7 s (for computing v-&gt;u)\n\nsample_size = conn_sample_df['u-&gt;v'].count()\nppos_size = conn_sample_df[conn_sample_df['u-&gt;v'] | conn_sample_df['v-&gt;u']]['u-&gt;v'].count()\nnneg_size = conn_sample_df[~conn_sample_df['u-&gt;v'] & ~conn_sample_df['v-&gt;u']]['u-&gt;v'].count()\nprint('there were {:4d} out of {:d} ({:5.2f} %) nodes for which have neither u-&gt;v nor v-&gt;u'.\n      format(nneg_size, sample_size, 100.0*nneg_size/sample_size))\nprint('there were {:4d} out of {:d} ({:5.2f} %) nodes for which have  either u-&gt;v  or v-&gt;u'.\n      format(ppos_size, sample_size, 100.0*ppos_size/sample_size))\nprint('together {:4d} + {:4d} = {:d} vs {:d}'.\n      format(nneg_size, ppos_size, nneg_size + ppos_size, sample_size))\n\nthere were  918 out of 10000 ( 9.18 %) nodes for which have neither u-&gt;v nor v-&gt;u\nthere were 9082 out of 10000 (90.82 %) nodes for which have  either u-&gt;v  or v-&gt;u\ntogether  918 + 9082 = 10000 vs 10000\n\n\n\nFalse positives for backward topological levels (negative cut)\n\nsample_size = conn_sample_df['u-&gt;v'].count()\nprint('connected:        %d of %d (%g +/- %g)' %\n      (conn_sample_df['u-&gt;v'].sum(),\n       conn_sample_df['u-&gt;v'].count(),\n       conn_sample_df['u-&gt;v'].mean(),\n       conn_sample_df['u-&gt;v'].std()/sqrt(sample_size)))\nconn_sample_df['l_v&lt;l_u']=conn_sample_df['l_v']&lt;conn_sample_df['l_u']\nconn_sample_df['l_v&gt;l_u']=conn_sample_df['l_v']&gt;conn_sample_df['l_u']\nprint('levels l_u &lt; l_v: %d (%g +/- %g)' %\n      (conn_sample_df['l_v&lt;l_u'].sum(),\n       conn_sample_df['l_v&lt;l_u'].mean(),\n       conn_sample_df['l_v&lt;l_u'].std()/sqrt(sample_size)))\nprint('levels l_u &gt; l_v: %d (%g +/- %g)' %\n      (conn_sample_df['l_v&gt;l_u'].sum(),\n       conn_sample_df['l_v&gt;l_u'].mean(),\n       conn_sample_df['l_v&gt;l_u'].std()/sqrt(sample_size)))\n\nconnected:        4515 of 10000 (0.4515 +/- 0.00497667)\nlevels l_u &lt; l_v: 4960 (0.496 +/- 0.00500009)\nlevels l_u &gt; l_v: 5038 (0.5038 +/- 0.00500011)\n\n\n\nconn_sample_df['!u-&gt;v'] = ~conn_sample_df['u-&gt;v']\nconn_sample_df['fp_levels'] = conn_sample_df['l_v&lt;l_u'] & conn_sample_df['!u-&gt;v']\nprint('level: false positives %d out of %d negative queries (%g %%), out of %d total' %\n      (conn_sample_df['fp_levels'].sum(),\n       conn_sample_df['!u-&gt;v'].sum(),\n       100.0*conn_sample_df['fp_levels'].sum()/conn_sample_df['!u-&gt;v'].sum(),\n       conn_sample_df['!u-&gt;v'].count()))\nconn_sample_df['fp_levels'].describe()\n\nlevel: false positives 445 out of 5485 negative queries (8.11304 %), out of 10000 total\n\n\ncount     10000\nunique        2\ntop       False\nfreq       9555\nName: fp_levels, dtype: object\n\n\n\n\nFalse negatives for DFS tree interval labels (positive cut)\n\nprint('sample of {:d} node pairs in {} graph'.format(len(conn_nodes), graph_name))\nfor conn in conn_sample:\n    u = conn['u']\n    v = conn['v']\n    conn['min(u)'] = graph.mpi_ext[u]['min']\n    conn['min(v)'] = graph.mpi_ext[v]['min']\n    conn['f_min(u)'] = graph.mpi_ext[u]['f_min']\n    conn['f_min(v)'] = graph.mpi_ext[v]['f_min']\n    conn['post(u)'] = graph.mpi_ext[u]['post']\n    conn['post(v)'] = graph.mpi_ext[v]['post']\n    \nconn_sample[0]\n\nsample of 10000 node pairs in git-commit_graph graph\n\n\n{'u': 'c7f34c180',\n 'v': '23c204455',\n 'l_u': 5579,\n 'l_v': 19392,\n 'u-&gt;v': False,\n 'min(u)': 9555,\n 'min(v)': 55543,\n 'f_min(u)': 1,\n 'f_min(v)': 1,\n 'post(u)': 9555,\n 'post(v)': 55545}\n\n\n\nconn_sample_df_part_2 = pd.DataFrame.from_records(conn_sample, columns=['u','v',\n                                                                        'min(u)','min(v)',\n                                                                        'f_min(u)','f_min(v)',\n                                                                        'post(u)','post(v)'])\nconn_sample_df_part_1 = conn_sample_df\nconn_sample_df = pd.merge(conn_sample_df_part_1, conn_sample_df_part_2)\n\n\nconn_sample_df\n\n\n\n\n\n\n\n\nu\nv\nl_u\nl_v\nu-&gt;v\nv-&gt;u\nl_v&lt;l_u\nl_v&gt;l_u\n!u-&gt;v\nfp_levels\nmin(u)\nmin(v)\nf_min(u)\nf_min(v)\npost(u)\npost(v)\n\n\n\n\n0\nc7f34c180\n23c204455\n5579\n19392\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n9555\n55543\n1\n1\n9555\n55545\n\n\n1\n9dc527adb\n53ec551c8\n433\n12473\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n1\n35785\n1\n1\n459\n35785\n\n\n2\n5e3ce663b\n9affecbc8\n11072\n9538\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\n23750\n1\n1\n1\n23770\n18771\n\n\n3\n6440fdbab\nc8c35f6a0\n12391\n20919\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n30197\n61062\n1\n1\n30202\n61067\n\n\n4\nf1a7082f2\ncaac7a3ab\n12813\n15628\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n31397\n40137\n1\n1\n31397\n40150\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\n9be24a30d\n26b59b481\n1769\n11394\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n61749\n24769\n61749\n1\n63629\n24769\n\n\n9996\n2a7453241\n80a14665b\n11757\n13769\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n26221\n33668\n1\n1\n26221\n33668\n\n\n9997\n577ed5c20\na75ef3ff9\n4435\n17803\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n7255\n1\n1\n1\n7255\n48215\n\n\n9998\n6f5c77a11\n81811a74b\n17947\n288\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\n49000\n14402\n1\n799\n49000\n14407\n\n\n9999\nc3a3db7d4\n557c5895c\n20960\n20011\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\n61372\n57434\n1\n1\n61380\n57441\n\n\n\n\n10000 rows × 16 columns\n\n\n\n\nprint('connected u-&gt;v:   %d of %d (%g)' %\n      (conn_sample_df['u-&gt;v'].sum(),\n       conn_sample_df['u-&gt;v'].count(),\n       conn_sample_df['u-&gt;v'].mean()))\nconn_sample_df['s_u &lt;= v']=conn_sample_df['min(u)'] &lt;= conn_sample_df['post(v)']\nconn_sample_df['e_u &gt;= v']=conn_sample_df['post(v)'] &lt;= conn_sample_df['post(u)']\nconn_sample_df['v in [s_u,e_u]']=conn_sample_df['s_u &lt;= v'] & conn_sample_df['e_u &gt;= v']\nprint('intervals u-~-&gt;v: %d of %d (%g)' %\n      (conn_sample_df['v in [s_u,e_u]'].sum(),\n       conn_sample_df['v in [s_u,e_u]'].count(),\n       conn_sample_df['v in [s_u,e_u]'].mean()))\nconn_sample_df.head()\n\nconnected u-&gt;v:   4515 of 10000 (0.4515)\nintervals u-~-&gt;v: 1217 of 10000 (0.1217)\n\n\n\n\n\n\n\n\n\nu\nv\nl_u\nl_v\nu-&gt;v\nv-&gt;u\nl_v&lt;l_u\nl_v&gt;l_u\n!u-&gt;v\nfp_levels\nmin(u)\nmin(v)\nf_min(u)\nf_min(v)\npost(u)\npost(v)\ns_u &lt;= v\ne_u &gt;= v\nv in [s_u,e_u]\n\n\n\n\n0\nc7f34c180\n23c204455\n5579\n19392\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n9555\n55543\n1\n1\n9555\n55545\nTrue\nFalse\nFalse\n\n\n1\n9dc527adb\n53ec551c8\n433\n12473\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n1\n35785\n1\n1\n459\n35785\nTrue\nFalse\nFalse\n\n\n2\n5e3ce663b\n9affecbc8\n11072\n9538\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\n23750\n1\n1\n1\n23770\n18771\nFalse\nTrue\nFalse\n\n\n3\n6440fdbab\nc8c35f6a0\n12391\n20919\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n30197\n61062\n1\n1\n30202\n61067\nTrue\nFalse\nFalse\n\n\n4\nf1a7082f2\ncaac7a3ab\n12813\n15628\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n31397\n40137\n1\n1\n31397\n40150\nTrue\nFalse\nFalse\n\n\n\n\n\n\n\n\nconn_sample_df['fn_intervals'] = conn_sample_df['u-&gt;v'] & ~conn_sample_df['v in [s_u,e_u]']\nprint('intervals: true positives  %d out of %d positive queries (%g %%)' %\n      (conn_sample_df['v in [s_u,e_u]'].sum(),\n       conn_sample_df['u-&gt;v'].sum(),\n       100.0*conn_sample_df['v in [s_u,e_u]'].sum()/conn_sample_df['u-&gt;v'].sum()))\nprint('intervals: false negatives %d out of %d positive queries (%g %%), out of %d total queries' %\n      (conn_sample_df['fn_intervals'].sum(),\n       conn_sample_df['u-&gt;v'].sum(),\n       100.0*conn_sample_df['fn_intervals'].sum()/conn_sample_df['u-&gt;v'].sum(),\n       conn_sample_df['u-&gt;v'].count()))\nconn_sample_df['fn_intervals'].describe()\n\nintervals: true positives  1217 out of 4515 positive queries (26.9546 %)\nintervals: false negatives 3298 out of 4515 positive queries (73.0454 %), out of 10000 total queries\n\n\ncount     10000\nunique        2\ntop       False\nfreq       6702\nName: fn_intervals, dtype: object\n\n\n\n\nFalse positives for DFS inexact graph interval labels (negative cut)\n\nconn_sample_df['v in [f_min(u),post(u)]'] = \\\n    (conn_sample_df['f_min(u)'] &lt;= conn_sample_df['post(v)']) & \\\n    (conn_sample_df['post(v)']  &lt;= conn_sample_df['post(u)'])\nprint('graph intervals u-~-&gt;v: %d of %d (%g)' %\n      (conn_sample_df['v in [f_min(u),post(u)]'].sum(),\n       conn_sample_df['v in [f_min(u),post(u)]'].count(),\n       conn_sample_df['v in [f_min(u),post(u)]'].mean()))\nconn_sample_df[['u','v',\n                'f_min(u)','f_min(v)',\n                'min(u)','min(v)',\n                'post(u)','post(v)',\n                'u-&gt;v',\n                'v in [f_min(u),post(u)]']].head()\n\ngraph intervals u-~-&gt;v: 4640 of 10000 (0.464)\n\n\n\n\n\n\n\n\n\nu\nv\nf_min(u)\nf_min(v)\nmin(u)\nmin(v)\npost(u)\npost(v)\nu-&gt;v\nv in [f_min(u),post(u)]\n\n\n\n\n0\nc7f34c180\n23c204455\n1\n1\n9555\n55543\n9555\n55545\nFalse\nFalse\n\n\n1\n9dc527adb\n53ec551c8\n1\n1\n1\n35785\n459\n35785\nFalse\nFalse\n\n\n2\n5e3ce663b\n9affecbc8\n1\n1\n23750\n1\n23770\n18771\nTrue\nTrue\n\n\n3\n6440fdbab\nc8c35f6a0\n1\n1\n30197\n61062\n30202\n61067\nFalse\nFalse\n\n\n4\nf1a7082f2\ncaac7a3ab\n1\n1\n31397\n40137\n31397\n40150\nFalse\nFalse\n\n\n\n\n\n\n\n\nconn_sample_df['fp_intervals'] = conn_sample_df['!u-&gt;v'] & conn_sample_df['v in [f_min(u),post(u)]']\n\nprint('%d total queries' % conn_sample_df['u-&gt;v'].count())\nprint('graph intervals: true negatives  %4d out of %4d negative queries (%g %%)' %\n      ((~conn_sample_df['v in [f_min(u),post(u)]']).sum(),\n       conn_sample_df['!u-&gt;v'].sum(),\n       100.0*(~conn_sample_df['v in [f_min(u),post(u)]']).sum()/conn_sample_df['!u-&gt;v'].sum()))\n\nprint('graph intervals: true negatives  %4d not covered by level filter' %\n      (~conn_sample_df['v in [f_min(u),post(u)]'] & conn_sample_df['l_v&lt;l_u'] & conn_sample_df['!u-&gt;v']).sum())\nprint('levels:          true negatives  %4d not covered by graph intervals' %\n      (conn_sample_df['l_v&gt;l_u'] & conn_sample_df['v in [f_min(u),post(u)]'] & conn_sample_df['!u-&gt;v']).sum())\n\nprint('graph intervals: false positives %4d out of %4d negative queries (%g %%)' %\n      (conn_sample_df['fp_intervals'].sum(),\n       conn_sample_df['!u-&gt;v'].sum(),\n       100.0*conn_sample_df['fp_intervals'].sum()/conn_sample_df['!u-&gt;v'].sum()))\n\nconn_sample_df['fp_intervals'].describe()\n\n10000 total queries\ngraph intervals: true negatives  5360 out of 5485 negative queries (97.7211 %)\ngraph intervals: true negatives   421 not covered by level filter\nlevels:          true negatives   101 not covered by graph intervals\ngraph intervals: false positives  125 out of 5485 negative queries (2.27894 %)\n\n\ncount     10000\nunique        2\ntop       False\nfreq       9875\nName: fp_intervals, dtype: object\n\n\n\n\nSummary of findings\n\n10000 total queries\nnumber of connected nodes: 4515 out of 10000 (45.15 % = 0.4515 +/- 0.00497667)\nthere were 918 out of 10000 ( 9.18 %) nodes for which have neither u-&gt;v nor v-&gt;u\nlevel: false positives 445 out of 5485 negative queries (8.11304 %)\ninexact graph intervals: false positives 125 out of 5485 negative queries (2.27894 %), or topological sort\nexact tree intervals: false negatives 3298 out of 4515 positive queries (73.0454 %)"
  },
  {
    "objectID": "evaluation.html#append-results-of-evaluation-to-a-file-todo",
    "href": "evaluation.html#append-results-of-evaluation-to-a-file-todo",
    "title": "Reachability evaluation",
    "section": "Append results of evaluation to a file (TODO)",
    "text": "Append results of evaluation to a file (TODO)\n\ndf_filename = 'datasets/' + graph_name + '-df_reachability_sample.csv.gz'\nif Path(df_filename).exists():\n    print('appending not implemented yet !!!')\nelse:\n    print('saving reachability analysis for {} to \"{}\"'.format(graph_name, df_filename))\n    save_df_to_file(conn_sample_df, df_filename)\n\nsaving reachability analysis for git-commit_graph to \"datasets/git-commit_graph-df_reachability_sample.csv.gz\""
  },
  {
    "objectID": "levels.html",
    "href": "levels.html",
    "title": "Topological levels",
    "section": "",
    "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "levels.html#definition-of-forward-and-backward-topological-levels",
    "href": "levels.html#definition-of-forward-and-backward-topological-levels",
    "title": "Topological levels",
    "section": "Definition of forward and backward topological levels",
    "text": "Definition of forward and backward topological levels\nThe backward topological level of a vertex \\(v\\) (denoted as \\(l_v\\)) can be defined as its depth: - if \\(v\\) has no immediate predecessors (has no outgoing edges, i.e. its out-degree is zero - it is a sink),\nthen \\(l_v = 0\\), - otherwise it is maximum of levels of its immediate predecessors plus one:\n\\(l_v = \\max\\limits_{u\\colon (v,u)\\in E}(l_u)+1\\).\nIn other words it is the maximum length of any path starting from vertex \\(v\\).\nThe backward topological level increases at least by one when going in the opposite direction to edges (going backwards).\n\n\n\n\n\n\nNote\n\n\n\nThis reachability label is immutable with respect to the graph growth by adding nodes.\n\n\n\nThe forward topological level of a vertex \\(v\\) (denoted as \\(L_v\\)) is defined in the following way: - if \\(v\\) has no immediate parents / successors (it has no in-going edges, i.e. its in-degree is zero – it is a source),\nthen \\(L_v = 0\\), - otherwise it is maximum of forward levels of its immediate successors plus one,\nthat is \\(L_v = \\max\\limits_{u\\colon (u,v)\\in E}(L_u + 1)\\)\nIn other words it is maximum length of any path ending at vertex \\(v\\).\nThe forward topological level increases by at least one when going in the direction of edge (going forwards).\nThe image below shows an example directed graph (DAG), marked with forward and backward topological levels (forward marked in green and backward marked in blue). This example is taken from the Figure 2 in the PReaCH paper.\n\n\n\nForward and backward topological levels for an example graph"
  },
  {
    "objectID": "levels.html#properties-of-topological-levels",
    "href": "levels.html#properties-of-topological-levels",
    "title": "Topological levels",
    "section": "Properties of topological levels",
    "text": "Properties of topological levels\nThe forward level \\(L_v\\) of a vertex induces its topological ordering: if \\(u\\) precedes \\(v\\) in the topological ordering of DAG \\(G\\) and \\(u \\neq v\\), then \\(L_u &lt; L_v\\).\n\n\n\n\n\n\nNote\n\n\n\nthe alternative notation for \\(l_u\\) is \\(l(u)\\), and for \\(L_u\\) is \\(L(u)\\).\n\n\nIn other words the following observation holds:\n\nfor forward topological levels (defined as having level equal 0 for source nodes): \\[\\forall u \\neq v \\in V : L(u) \\geq L(v) \\implies u \\nrightarrow v \\]\nfor backward topological levels (defined as having level equal 0 for sink nodes): \\[\\forall u \\neq v \\in V : l(u) \\leq l(v) \\implies u \\nrightarrow v \\]\n\nWe can also use a weaker condition, which has the advantage of avoiding corner cases of checking whether \\(u = v\\) / \\(u \\neq v\\):\n\nfor forward topological levels (defined as having level equal 0 for source nodes): \\[\\forall u \\in V, v \\in V : L(u) &gt; L(v) \\implies u \\nrightarrow v \\]\nfor backward topological levels (defined as having level equal 0 for sink nodes): \\[\\forall u \\in V, v \\in V : l(u) &lt; l(v) \\implies u \\nrightarrow v \\]\n\nIt also helps in the case where newer commits do not have backward topological levels computed, and we want to use topological levels even in the case where at least one vertex in reachability query is new.\nAll this means that both forward and backward topological levels can function as negative-cut filter (reducing the search space)."
  },
  {
    "objectID": "levels.html#computing-backward-topological-levels",
    "href": "levels.html#computing-backward-topological-levels",
    "title": "Topological levels",
    "section": "Computing backward topological levels",
    "text": "Computing backward topological levels\n\nDefinition: The level of a vertex \\(v\\) (denoted as \\(l_v\\)) can be defined as its depth: if \\(v\\) has no immediate predecessors, then \\(l_v = 0\\), otherwise it is maximum of levels of its immediate predecessors plus one: \\[l_v = \\max\\limits_{u\\colon (v,u)\\in E}(l_u)+1\\]\n\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Parameters:\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Returns:\n  else: warn(msg)\n\nsource\n\nfind_levels\n\n find_levels (DG, attr=None)\n\nFind levels (generation number) of all vertices in graph G\nThe backward topological level of a vertex v, which we denote as l_v or l(v), can be defined as its depth, or the length of longestt path to the leaf (sink) node.\n\nif v has no immediate predecessors, then l_v = 0\notherwise l_v = 1 + max_{u: predecessors}(l_u)\n\nThe levels / generation number information can be optionally saved in the graph, under given node attribute (name given by attr parameter).\nNOTE: whether for nodes without outcoming edges (without any neighbours) l_v is defined to be 0, or defined to be 1, is a matter of convention. Here the same convention as in the FELINE paper is used.\nNOTE: for compatibility with forward topological levels sometimes backward topological levels are used with negative sign applied, like in the PReaCH paper.\n\n\nTest find_levels(graph)\nTest that find_levels() works on known example graphs from example_graphs\n\n# this is needed only for demonstrations\nimport matplotlib.pyplot as plt\nimport git_commit_graph_ext.example_graphs as graphs\n\nUse the small DAG from the FELINE paper as a test\n\nsd=graphs.small_DAG_FELINE()\nfind_levels(sd)\n\n{'h': 0, 'c': 1, 'd': 0, 'e': 1, 'a': 2, 'f': 1, 'g': 0, 'b': 2}\n\n\n\nsd=graphs.small_DAG_FELINE()\nsd.lvl=find_levels(sd)\nsd.pos={'a':(2,3),'b':(4.5,3),'c':(1,2),'d':(2,2),'e':(3,2),'f':(4,2),'g':(5,2),'h':(3,1)}\n\nnx.draw_networkx(sd, pos=sd.pos,\n                 with_labels=False,\n                 node_size=500,width=2.0,node_color='y')\nnx.draw_networkx_labels(sd, pos=sd.pos,\n                        labels=sd.lvl)\n\nplt.title('Labels denote backward topological level $l_u$')\nplt.draw()\n\n\n\n\nUse the graph from Figure 9 in the FELINE paper\n\nlf=graphs.levels_DAG_FELINE()\nassert find_levels(lf) == {\n    'c':0,'g':0,'h':0,'i':0,\n    'd':1,'e':1,'f':1,\n    'a':2,'b':2\n}\n\n\nlf=graphs.levels_DAG_FELINE()\n# backward level\nlf.lvl_expected={\n    'c':0,'g':0,'h':0,'i':0,\n    'd':1,'e':1,'f':1,\n    'a':2,'b':2\n}\nlf.lvl=find_levels(lf)\n# positions of nodes\nlf.tree={\n  'a':(2,2),'b':(4,2),\n  'd':(2,1),'e':(3,1),'f':(4,1),\n  'c':(1,0),'g':(2,0),'h':(3,0),'i':(4,0)\n}\n# configure plot\n#plt.axis('off')\nplt.ylabel('backward topological level')\n#plt.subplots_adjust(left=0,right=1.2)\nplt.xlim(left=0.25,right=4.5)\nnx.draw_networkx(lf,\n                 pos=lf.tree,\n                 nodelist=list(lf.lvl.keys()),\n                 node_color=list(lf.lvl.values()),\n                 cmap=plt.cm.Greens,\n                 vmin=-2,vmax=5,\n                 node_size=500,width=2.0)\nnx.draw_networkx_labels(lf,\n                        pos={k: (v[0]-0.3,v[1]-0.1) for (k,v) in lf.tree.items()},\n                        labels={k:'$l_'+str(k)+'='+str(l)+'$'\n                                for (k,l) in lf.lvl.items()})\nplt.draw()\n\n\n\n\nTODO: Interactive visualization"
  },
  {
    "objectID": "interval_labels.html",
    "href": "interval_labels.html",
    "title": "DFS Intervals Labelling",
    "section": "",
    "text": "Imports for the module"
  },
  {
    "objectID": "interval_labels.html#definitions-out-tree-spanning-tree-tree-cover-etc.",
    "href": "interval_labels.html#definitions-out-tree-spanning-tree-tree-cover-etc.",
    "title": "DFS Intervals Labelling",
    "section": "Definitions: out-tree, spanning tree / tree cover, etc.",
    "text": "Definitions: out-tree, spanning tree / tree cover, etc.\n\n\n\nA tree in graph theory (Wikipedia)\n\n\nIn graph theory, a tree is an undirected graph in which any two vertices are connected by exactly one path, or equivalently a connected acyclic undirected graph.\n\nIn graph theory, a directed rooted tree or an arborescence, or out-tree is a directed graph in which, for a vertex \\(u\\) called the root and any other vertex \\(v\\), there is exactly one directed path from \\(u\\) to \\(v\\). Equivalently, an arborescence is a directed, rooted tree in which all edges point away from the root.\nEvery directed rooted tree is a directed acyclic graph (DAG), but not every DAG is a tree.\n\n\n\n\n\n\nNote\n\n\n\nhere root denotes a source node / verted, with in-degree of zero.\n\n\nA directed rooted forest is a disjoint union of directed rooted trees. It is called branching or out-forest if all its edges point away from the root in each rooted tree.\n\n\n\nA 4x4 grid graph and one of its spanning trees (Wikipedia)\n\n\nIn the mathematical field of graph theory, a spanning tree \\(T\\) of an undirected graph \\(G\\) is a subgraph that is a tree which includes all of the vertices of \\(G\\), with a minimum possible number of edges.\nA full spanning forest in an undirected graph (possibly not connected) is a maximal acyclic subgraph of the given graph, or equivalently a graph consisting of a spanning tree in each connected component of the graph.\nGiven a vertex \\(v\\) on a directed multigraph \\(G\\), an oriented spanning out-tree, or tree cover \\(T(G)\\) rooted at \\(v\\) is an acyclic subgraph of \\(G\\) in which every vertex other than \\(v\\) has in-degree 1.\n\n\n\nSmall DAG with the spanning out-forest overlay and min-post intervals\n\n\nGiven directed acyclic graph \\(G\\), a spanning forest, or oriented spanning out-forest is such its subgraph, for which the following is true: - it includes all original (full) graph nodes - there is at most one incoming edge per node (every vertex has in-degree 1 or 0)"
  },
  {
    "objectID": "interval_labels.html#min-post-intervals-in-directed-trees",
    "href": "interval_labels.html#min-post-intervals-in-directed-trees",
    "title": "DFS Intervals Labelling",
    "section": "min-post intervals in directed trees",
    "text": "min-post intervals in directed trees\nThe reachability problem on trees (out-forests) can be solved effectively by interval labeling, which takes linear time and space for constructing the index, and provides constant time querying.\nIt labels each node \\(u\\) with a min-post range \\(I_u = [s_u, e_u]\\), where - \\(s_u\\) denotes the rank of the node \\(u\\) in a post-order traversal of the tree, where the ranks are assumed to begin at 1, and all the children of a node are assumed to be ordered and fixed for that traversal.\nNote that the tree needs to be traversed in depth-first search (DFS) manner.\n\n\\(s_u\\) denotes the lowest rank for any node \\(x\\), or in other words minimum of DFS number of a node in the subtree rooted at \\(u\\) (i.e., including \\(u\\)).\n\nMin-post interval for a vertex (node) \\(u\\) in a directed acyclic graph \\(G\\), denoted as \\(I(u) \\equiv I_u = [s_u, e_u]\\), can be computed in the following way: - \\(e_u\\) is defined as \\(e_u = \\pi(u) = \\text{post}(u)\\), i.e. post-order value (back traversal) in DFS traversal - \\(s_u = \\pi(u) = e_u\\) for leaf nodes, also known as sink nodes (no outcoming edges, out-degree 0),\notherwise \\(s_u = \\min\\{s_x \\colon x \\in \\text{out-neighbours}(s)\\}\\) (out-neighbours are sometimes called children).\nSimilarly, max-pre interval for a vertex (node) \\(u\\) in a directed acyclic graph \\(G\\), denoted as \\(\\text{range}(u) \\equiv \\hat{I}_u = [\\hat{\\phi}(u), \\phi(u)]\\), can be computed in the following way: - \\(\\phi(u)\\) is defined as \\(\\phi(u) = \\text{pre}(u)\\), i.e. pre-order value (forward traversal) in DFS traversal - \\(\\hat{\\phi}(u) = \\phi(u)\\) for leaf nodes, also known as sink nodes (no outcoming edges, out-degree 0),\notherwise \\(\\hat{\\phi}(u) = \\max\\{\\hat{\\phi}(x) \\colon x \\in \\text{out-neighbours}(s)\\}\\).\n\nMin-post interval properties for trees\nFor out-forests (directed forests) the containment between intervals is equivalent to the reachability relationship between the nodes, since the post-order DFS traversal enters a node before all of its descendants, and leaves after having visited all of its descendants.\nIn other words, \\(u \\leadsto v \\iff I_v \\subseteq I_u\\)\n\n\n\n\n\n\nNote\n\n\n\nfor trees it is enough to check that \\(s_v \\in I_u\\), because that is equivalent to checking \\(I_v \\subseteq I_u\\).\n\n\nThis means that the reachability query for trees can be answered in constant time.\nMin-post and max-pre intervals for trees functions both as positive-cut and as negative-cut filter: - if \\(I_v \\subseteq I_u\\), then \\(u\\) can reach \\(v\\) (positive-cut filter) - if \\(u\\) can reach \\(v\\), then \\(I_v \\subseteq I_u\\) (negative-cut filter)"
  },
  {
    "objectID": "interval_labels.html#min-post-intervals-in-directed-acyclic-graphs-dags",
    "href": "interval_labels.html#min-post-intervals-in-directed-acyclic-graphs-dags",
    "title": "DFS Intervals Labelling",
    "section": "min-post intervals in directed acyclic graphs (DAGs)",
    "text": "min-post intervals in directed acyclic graphs (DAGs)\nThe DFS post-order value (back traversal) induces topological sorting for a DAG.\nThis means that if \\(u\\) can reach \\(v\\) (i.e. \\(r(u,v)\\) is true, or \\(u \\leadsto v\\)), and \\(u \\neq v\\), then \\(\\text{post}(u) &gt; \\text{post}(v)\\).\nConversely, if \\(\\text{post}(u) &lt; \\text{post}(v)\\), then \\(u\\) cannot reach \\(v\\) (negative-cut filter).\n\n\n\nCommit graph with the spanning forest\n\n\nFor example, as shown in the above image, \\(n_{18} \\leadsto n_{9}\\), but \\(n_{18} \\not\\leadsto n_{19}\\)\n\nmin-post interval for a spanning tree (positive-cut filter)\nFor a directed acyclic graph \\(G\\), we define a tree cover (or forest cover) of \\(G\\), denoted as \\(T(G) = (V_T, E_T)\\), as a directed spanning tree of \\(G\\).\n\n\n\n\n\n\nTip\n\n\n\nif we want to have a tree cover rather than a forest,\n\n\n\nwe can augment graph \\(G\\) by introducing an artificial root node \\(r\\) that is connected to every node with no incoming edge: \\[ G' := (V \\cup \\{r\\}, E \\cup \\{(r,v) \\colon v \\in V, \\text{indegree}(v) = 0\\}) \\] Note that this modification has no effect on the reachability relation among the existing nodes of \\(G\\).\n\nWe define min-post tree interval as following\n\\[ I_v = \\{\\pi(w) \\colon w \\in V_{T_v}\\}\n   = \\left[ \\min_{w \\in V_{T_v}} \\pi(w), \\max_{w \\in V_{T_v}} \\pi(w) \\right]\n   = \\left[ \\min_{w \\in V_{T_v}} \\pi(w), \\pi(v) \\right]\\]\nHere \\(T_v = (V_{T_v}, E_{T_v})\\) denote the subtree of \\(T(G)\\) rooted at node \\(v\\).\nThe spanning forest, or tree cover, or out-forest \\(T(G)\\) of directed acyclic graph \\(G\\) has the following properties: - if there exists a path from \\(u\\) to \\(v\\) in the spanning tree \\(T(G)\\), then \\(u\\) can reach \\(v\\) in the full graph \\(G\\) - but the path from \\(u\\) to \\(v\\) could require going through edges outside the spanning tree \\(T(G)\\)\nThe min-post tree interval labels have therefore the following properties: - path from \\(u\\) to \\(v\\) in the spanning tree \\(T(G)\\) exists if and only if \\(I_v \\subseteq I_u\\) (or \\(\\pi(v) \\in I_u\\)) - if \\(I_v \\subseteq I_u\\), then \\(u \\leadsto v\\) (\\(u\\) can reach \\(v\\), also in the full graph \\(G\\))\nIt can be therefore used as positive-cut filter.\nExample:\n\n\n\nSmall DAG with the spanning out-forest overlay and min-post intervals\n\n\nIn the graph and its spanning tree shown in the above figure we have: - \\([3,3] \\subseteq [1,5]\\) then \\(a \\leadsto h\\), - \\([3,3] \\not\\subseteq [7,9]\\), but \\(b \\leadsto h\\) (via out-of-tree edge)\nSuch interval is called tree interval or exact interval and denoted \\(I_T(u)\\) in the FERRARI paper,\nand the corresponding max-pre interval as a range is called full interval (because every \\(v\\) such that \\(\\phi(v) \\in I_T(u)\\) is reachable from \\(v\\)) in the PReaCH paper\n\n\nmin-post interval for a DAG (negative-cut filter)\nWhile above technique (min-post intervals) can be used to easily answer reachability queries on trees, the case of general DAGs is much more challenging.\nThe reason is that, in general, the reachable set \\(\\mathcal{R}(v)\\) of a vertex \\(v\\) in the DAG (i.e. the set of vertices reachable from \\(v\\): \\(\\mathcal{R}(v) = \\{u \\in V\\colon v \\leadsto u\\}\\)) is only partly represented by the interval \\(I_T(v)\\), as the tree interval only accounts for reachability relationships that are preserved in \\(T\\). Vertices that can only be reached from a node \\(v\\) by traversing one or more non-tree edges have to be handled seperately: instead of merely storing the tree intervals \\(I_T(v)\\), every node \\(v\\) is now assigned a set of intervals, denoted by \\(\\mathcal{I}(v)\\). The purpose of this so-called reachable interval set is to capture the complete reachability information of a node.\nLet’s define the approximate graph interval as a single interval \\(\\tilde{I}_G(v) = [\\tilde{s}_u, \\tilde{e}_u]\\) approximating the reachable set \\(\\mathcal{R}(u)\\) of a vertex \\(u\\) in the DAG from the above (i.e. \\(\\mathcal{R}(u) \\subseteq \\tilde{I}_G(u)\\)) in the following way:\n\\[ \\tilde{I}_G(v)\n   = \\left[ \\min_{w \\in \\mathcal{R}(v)} \\pi(w), \\max_{w \\in \\mathcal{R}_v} \\pi(w) \\right]\n   = \\left[ \\min_{w \\in V} \\pi(w), \\pi(v) \\right]\\]\nThe second part of the above equation is true for DFS traversal; the fact that the end of interval is simply post-traversal value \\(\\pi(v)\\) was mentioned earlier.\nThis min-post approximate graph interval has the following properties: - if \\(u\\) can reach \\(v\\) (denoted as \\(u \\leadsto v\\)), then \\(\\tilde{I}_G(v) \\subseteq \\tilde{I}_G(u)\\) - this means that if \\(\\tilde{I}_G(v) \\not\\subseteq \\tilde{I}_G(u)\\), then \\(u\\) cannot reach \\(v\\) (negative-cut filter)\n\n\n\nInterval Labeling: Tree (a) and DAG: Single (b)\n\n\nFor example, part (a) of the figure above (taken from GRAIL paper) shows the interval labeling on a tree, assuming that the children are ordered from left to right. It is easy to see that reachability can be answered by interval containment. For example, \\(1 \\leadsto 9\\), since \\(I_9 = [2, 2] \\subset [1, 6] = L_1\\), but \\(2 \\not\\leadsto 7\\), since \\(I_7 = [1, 3] \\not\\subseteq [7, 9] = L_2\\).\nFor example, part (b) of the figure above shows an interval labeling on a DAG, assuming a left to right ordering of the children. As one can see, interval containment of nodes in a DAG is not exactly equivalent to reachability. For example, \\(5 \\not\\leadsto 4\\), but \\(\\tilde{I}_4 = [1, 5] \\subseteq [1, 8] = \\tilde{I}_5\\). In other words, \\(\\tilde{U}_v \\subseteq \\tilde{I}_u\\) does not imply that \\(u \\leadsto v\\). On the other hand, one can show that \\(\\tilde{I}_v \\not\\subseteq \\tilde{I}_u \\implies u \\not\\leadsto v\\)."
  },
  {
    "objectID": "interval_labels.html#extending-min-post-intervals-for-graphs",
    "href": "interval_labels.html#extending-min-post-intervals-for-graphs",
    "title": "DFS Intervals Labelling",
    "section": "Extending min-post intervals for graphs",
    "text": "Extending min-post intervals for graphs\nThis section describes various extensions to the min-post intervals (DFS numbering) going beyond those negative-cut and positive-cut filters described above.\n\nMultiple intervals labelling: GRAIL (TODO)\n\n\nExtending interval labelling: PReaCH\nThe PReaCH paper extends min-post intervals labelling with approach slightly different from GRAIL, where instead of using additional DFS searches, additional extra labels are used.\nNOTE: the PReaCH paper uses max-pre interval labelling instead of min-post one, but one can be translated into the other.\n\nOriginal extension of max-pre intervals (Section 3.3 of PReaCH)\nSection 3.3 “Pruning Based on DFS Numbering” in the PReaCH paper.\n\n\n\nFull and empty intervals derived from a DFS ordering\n\n\nLet \\(\\phi(v)\\) denote the pre-visit DFS number of node \\(v\\) and \\(\\hat{\\phi}(v)\\) the largest DFS number of a node in the subtree of the DFS tree rooted at \\(v\\). The properties of DFS ensure that the nodes in \\(\\text{range}(v) := \\phi(v)..\\hat{\\phi}(v)\\) are all reachable from \\(v\\) (they form the subtree of the DFS tree which is rooted at \\(v\\)) and that no nodes with DFS number exceeding \\(\\hat{\\phi}(v)\\) is reachable from \\(v\\).\nLet \\(u \\to v\\) denote that \\(r(u,v)\\) is true, and \\(u \\nrightarrow v\\) that \\(r(u,v)\\) is false.\n(We would use \\(u \\leadsto v\\)/\\(u \\rightsquigarrow v\\), but \\(u \\not\\leadsto v\\) is broken, and there is no \\(\\nrightsquigarrow\\)).\n\n\\(\\forall v,t \\in V \\colon\\ t \\in \\text{range}(v) \\implies v \\to t\\)\n(positive-cut)\n\\(\\forall v,t \\in V \\colon\\ \\phi(t) &gt; \\hat{\\phi}(v) \\implies v \\nrightarrow t\\)\n(negative-cut)\n\nLet’s define node \\(w = p_{\\text{tree}}(v)\\) as the node that has \\(v \\to w\\) and \\(w \\notin \\text{range}(v)\\) which maximizes \\(\\lvert\\text{range}(w)\\rvert\\). Then, if \\(p_{\\text{tree}}(v)\\) exists\n\n\\(\\forall v,t \\in V \\colon\\ p_{\\text{tree}}(v) \\neq \\bot \\land t \\in \\text{range}(p_{\\text{tree}}(v)) \\implies v \\to t\\)\n(positive-cut)\n\nFor any \\(v \\in V\\), let \\(\\phi_{\\text{min}}(v)\\) denote the smallest DFS number of a node reachable from \\(v\\) (including \\(v\\)). Then\n\n\\(\\forall v,t \\in V \\colon\\ \\phi(t) &lt; \\phi_{\\text{min}}(v) \\implies v \\nrightarrow t\\)\n(negative-cut)\n\nWe can also define \\(\\phi_{\\text{gap}}(v)\\) in such way that\n\n\\(\\forall v,t \\in V \\colon\\ \\phi_{\\text{gap}}(v) &lt; \\phi(t) &lt; \\phi(v) \\implies v \\nrightarrow t\\)\n(negative-cut)\n\nAll those values: \\(\\phi(v), \\hat{\\phi}(v), p_{\\text{tree}}(v), \\phi_{\\text{min}}(v), \\phi_{\\text{gap}}(v)\\) can be done while computing the DFS numbering.\nOn choosing the DFS spanning-tree from PReaCH paper\nThere are many ways to define a DFS ordering: We are free to choose the order in which we scan the nodes for starting recursive exploration and we can choose the order in which we inspect edges leaving a node being explored.\nOne heuristics that seems to be useful: Make sure that most nodes are in a small number of trees because this leads to large positive intervals. It therefore makes sense to only uses sources of the graph as tree roots.\nIn addition, one can order the sources by the number of nodes reached from them during the search for topological levels.\n\n\nTranslating PReaCH extension to min-post intervals\n\n\n\n\nFull and empty intervals derived from a post-visit DFS ordering\n\n\nLet \\(\\pi(v)\\) denote the post-visit DFS number of node \\(v\\) and \\(\\breve{\\pi}(v)\\) the smallest post-visit DFS number of a node in the subtree of the DFS tree rooted at \\(v\\). The properties of DFS ensure that the nodes in \\(I_T(v) := \\breve{\\pi}(v)..\\pi(v) = [\\breve{\\pi}(v),\\pi(v)]\\) are all reachable from \\(v\\) (they form the subtree of the DFS tree which is rooted at \\(v\\)) and that no nodes with DFS number exceeding \\(\\pi(v)\\) is reachable from \\(v\\).\nLet \\(u \\to v\\) denote that \\(r(u,v)\\) is true, and \\(u \\nrightarrow v\\) that \\(r(u,v)\\) is false.\n\n\\(\\forall v,t \\in V \\colon\\ t \\in I_T(v) \\implies v \\to t\\)\n(positive-cut)\n\\(\\forall v,t \\in V \\colon\\ \\pi(t) &gt; \\pi(v) \\implies v \\nrightarrow t\\)\n(negative-cut)\n\nLet’s define node \\(w = p_{\\text{tree}}(v) = p_{\\text{t}}(v)\\) as the node that has \\(v \\to w\\) and \\(w \\notin I_T(v)\\) which maximizes \\(\\lvert I_T(w)\\rvert\\).\nThen, if \\(p_{\\text{tree}}(v)\\) exists (which we denote as \\(p_{\\text{tree}}(v) \\neq \\bot\\))\n\n\\(\\forall v,t \\in V \\colon\\ p_{\\text{tree}}(v) \\neq \\bot \\land t \\in I_T(p_{\\text{tree}}(v)) \\implies v \\to t\\)\n(positive-cut)\n\nFor any \\(v \\in V\\), let \\(\\pi_{\\text{min}}(v)\\) denote the smallest post-visit DFS number of a node reachable from \\(v\\) (including \\(v\\)) in the graph.\n\n\n\n\n\n\nNote\n\n\n\nwith this notation we can say that \\(\\tilde{I}_G(v) = [\\pi_{\\text{min}}(v), \\pi(v)]\\).\n\n\nThen\n\n\\(\\forall v,t \\in V \\colon\\ \\pi(t) &lt; \\pi_{\\text{min}}(v) \\implies v \\nrightarrow t\\)\n(negative-cut)\n\nWe can also define \\(\\pi_{\\text{gap}}(v)\\) in such way that\n\n\\(\\forall v,t \\in V \\colon\\ \\pi_{\\text{gap}}(v) &lt; \\pi(t) &lt; \\pi(v) \\implies v \\nrightarrow t\\)\n(negative-cut)\n\nAll those values: \\(\\pi(v), \\breve{\\pi}(v), p_{\\text{tree}}(v), \\pi_{\\text{min}}(v), \\pi_{\\text{gap}}(v)\\) can be done while computing the DFS numbering.\n\n\n\nExtending interval labelling: FERRARI (TODO)"
  },
  {
    "objectID": "interval_labels.html#visualizing-dfs-intervals-on-example-graphs",
    "href": "interval_labels.html#visualizing-dfs-intervals-on-example-graphs",
    "title": "DFS Intervals Labelling",
    "section": "Visualizing DFS intervals on example graphs",
    "text": "Visualizing DFS intervals on example graphs\nTODO\n\n# this is needed for example (small) graphs\nimport git_commit_graph_ext.example_graphs as graphs\n\n\n# this is needed only for demonstrations\nimport matplotlib.pyplot as plt\n\n\nExamples of spanning trees and min-post tree intervals labelling\nDraw a small DAG (from FELINE paper) and its spanning tree, like on Figure 8 of the FELINE paper, together with its min-post tree intervals.\n\n#@title Draw a small DAG and its spanning-tree, as in Figure 8\nfrom copy import deepcopy\n\nplt.axis('off')\nplt.title('Small DAG with the spanning tree overlay and min-post intervals')\n\nsd=graphs.small_DAG_FELINE()\nsd.tree={\n  'a':(2,3),'b':(4.5,3),\n  'c':(1,2),'d':(2,2),'e':(3,2),'f':(4,2),'g':(5,2),\n  'h':(3,1)\n}\nnx.draw_networkx(sd,\n                 pos=sd.tree,\n                 node_size=500,width=8.0,\n                 edge_color='#aaaaaa',node_color='y')\nnx.draw_networkx_edges(sd,\n                       pos=sd.tree,\n                       edgelist=[\n                            ('a','c'),('a','d'),('a','e'),\n                            ('e','h'),\n                            ('b','f'),('b','g')\n                        ],\n                        node_size=500,width=2.0,\n                        arrowstyle='-&gt;')\n\nsd.lpos=deepcopy(sd.tree)\nfor v in list('abh'):\n  sd.lpos[v] = (sd.lpos[v][0]+0.40,sd.lpos[v][1])\nfor v in list('cdfg'):\n  sd.lpos[v] = (sd.lpos[v][0]+0.05,sd.lpos[v][1]-0.2)\nfor v in list('e'):\n  sd.lpos[v] = (sd.lpos[v][0]+0.35,sd.lpos[v][1]+0.1)\nsd.mpi={\n    'a':(1,5),'b':(7,9),\n    'c':(1,1),'d':(2,2),'e':(3,4),'f':(7,7),'g':(8,8),\n    'h':(3,3),\n}\nsd.mpi_labels={\n  v:('[%d,%d]'%(i[0],i[1]))\n  for (v,i) in sd.mpi.items()\n}\n\nnx.draw_networkx_labels(sd,\n                        pos=sd.lpos,\n                        labels=sd.mpi_labels)\nplt.draw()\n\n\n\n\nThe above figure shows Min-post indexing for a spanning tree extracted from the small DAG of Figure 2 in the FELINE paper. Each vertex is labeled with its min-post interval. Original edges of the DAG are marked in light gray.\nFor instance, consider the vertex \\(h\\) of above figure. We can conclude that the query \\(r(a,h)\\) will return true (\\(a \\leadsto h\\)), without search, because \\([3,3] \\subseteq [1,5]\\). However, we can say nothing about the query \\(r(b,h)\\), because there is no tree edge connecting the two vertices.\n\n\nPost-traversal index vs backward topological level drawing of example graphs\n\nfrom matplotlib.lines import Line2D\n\n#plt.axis('off')\nplt.title('Small DAG with the spanning tree overlay and min-post intervals\\n'+\n          'drawn using spanning tree position and levels')\n\nsd=graphs.small_DAG_FELINE()\nsd.lvl = {\n    'h': 0, 'g': 0, 'd': 0,\n    'c': 1, 'e': 1, 'f': 1,\n    'a': 2, 'b': 2,\n}\nsd.mpi={\n    'a':(1,5),'b':(7,9),\n    'c':(1,1),'d':(2,2),'e':(3,4),'f':(7,7),'g':(8,8),\n    'h':(3,3),\n}\nsd.mpi_labels={\n  v:('[%d,%d]'%(i[0],i[1]))\n  for (v,i) in sd.mpi.items()\n}\nsd.mpilvl={\n  v: (sd.mpi[v][1],sd.lvl[v]) for v in sd\n}\n\nplt.xlabel('spanning-tree post-order $\\pi(v)$')\nplt.ylabel('node level $L(v)$')\n\nax=plt.gca()\nfor v in sd:\n  if sd.mpi[v][0] == sd.mpi[v][1]:\n    continue\n  ax.add_line(Line2D([sd.mpi[v][0],sd.mpi[v][1]],\n                     [sd.lvl[v],sd.lvl[v]],\n                     lw=5., alpha=0.3))\n  \nnx.draw_networkx(sd,\n                 pos=sd.mpilvl,\n                 node_size=500,width=8.0,\n                 edge_color='#aaaaaa',node_color='y')\nedges=nx.draw_networkx_edges(sd,\n                             pos=sd.mpilvl,\n                             edgelist=[\n                               ('a','c'),('a','d'),('a','e'),\n                               ('e','h'),\n                               ('b','f'),('b','g')\n                             ],\n                             node_size=500,width=2.0,\n                             arrowstyle='-&gt;')\n\nlabels=nx.draw_networkx_labels(sd,\n                               pos={u:(x+0.55,y-0.08) for (u,(x,y)) in sd.mpilvl.items()},\n                               labels=sd.mpi_labels)\n\n\n\n\nCommit graph with the spanning forest and min-post intervals: - commit graph from the Stolee blog post\nIn the drawing of this graph below arrows (directed edges) point from right to left.\n\n\nthe same commit graph, but with the spanning forest (with the tree cover)\nthe vertices / nodes in this image are post-visit order, denoted \\(\\text{post}(v) = \\pi(v)\\)\nnumbered dashed lines denote the backward topological level (sink-based) of the vertex / node\n\nThe same graph, just drawn differently (post-visit order vs topological level)\nwith the spanning forrest, post-visit order as label, and min-post tree interval marked"
  },
  {
    "objectID": "interval_labels.html#computing-dfs-intervals-labelling",
    "href": "interval_labels.html#computing-dfs-intervals-labelling",
    "title": "DFS Intervals Labelling",
    "section": "Computing DFS intervals labelling",
    "text": "Computing DFS intervals labelling\nHere there would be all code that automatically computes the spanning tree (or rather the spanning forest), also known as tree cover of a graph, and all the DFS numbering based labels.\n\nComputing the spanning tree\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Examples\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Notes\n  else: warn(msg)\n\nsource\n\n\nfind_dfs_spanning\n\n find_dfs_spanning (G)\n\nFind edges of the DFS spanning tree for graph G\nThis is used to visualize the spanning tree (spanning forest, tree cover) in example small graphs. This function is also used to compute DFS intervals, among others in find_dfs_intervals()\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nG\nNetworkX graph\nGraph to find spanning-tree in.\n\n\nReturns\nlist\nA list of edges in the depth-first-search.\n\n\n\nSimple test that it works correctly for a very simple linear graph\n\nnx.draw(nx.path_graph(5),with_labels=True,node_size=500,width=2.0)\n\n\n\n\n\nprint(find_dfs_spanning(nx.path_graph(5)))\nassert find_dfs_spanning(nx.path_graph(5)) == [(0, 1), (1, 2), (2, 3), (3, 4)]\n\n[(0, 1), (1, 2), (2, 3), (3, 4)]\n\n\nVisualize a spanning tree in undirected graph\n\nG5=nx.path_graph(5)\nG5.pos={v:(v,0) for v in range(5)}\n\nnx.draw_networkx(G5,\n                 pos=G5.pos,\n                 node_size=500,width=8.0,\n                 edge_color='#aaaaaa')\nnx.draw_networkx_edges(G5,\n                       pos=G5.pos,\n                       edgelist=find_dfs_spanning(G5),\n                       node_size=500,width=2.0,\n                       arrowstyle='-&gt;')\nplt.draw()\n\n\n\n\nDraw a small DAG from the FELINE paper, and visualize automatically generated spanning tree (or rather one of the spanning trees)\n\n#@title Draw a small DAG and its find_dfs_spanning\n\n# get graph\nsd=graphs.small_DAG_FELINE()\n# TODO: move setting original positions to `small_DAG_FELINE()` to avoid repetitions\nsd.tree={\n  'a':(2,3),'b':(4.5,3),\n  'c':(1,2),'d':(2,2),'e':(3,2),'f':(4,2),'g':(5,2),\n  'h':(3,1)\n}\n\n# configure plot\nplt.axis('off')\nplt.title('Small DAG with the spanning tree overlay '+\n          '(find_dfs_spanning)')\n\n# draw graph and its spanning tree\nnx.draw_networkx(sd,\n                 pos=sd.tree,\n                 node_size=500,width=8.0,\n                 edge_color='#aaaaaa',node_color='y')\nnx.draw_networkx_edges(sd,\n                       pos=sd.tree,\n                       edgelist=find_dfs_spanning(sd),\n                       node_size=500,width=2.0,\n                       arrowstyle='-&gt;')\nplt.draw()\n\n\n\n\nDraw a crown DAG, known as \\(S_0^3\\) graph, used in FELINE paper as simple example of irreductible false-positive for FELINE labels\n\n# get graph\ncd=graphs.crown_DAG()\ncd.pos={1:(1,4),2:(3,6),'u':(2,2),3:(4,1),4:(6,3),'v':(5,5)}\n\n# configure plot\nplt.axis('off')\nplt.title('Crown DAG $S_0^3$ with the spanning tree overlay '+\n          '(find_dfs_spanning)')\n\n# draw graph and its spanning tree\nnx.draw_networkx(cd,\n                 pos=cd.pos,\n                 node_size=500,width=8.0,\n                 edge_color='#aaaaaa')\nnx.draw_networkx_edges(cd,\n                       pos=cd.pos,\n                       edgelist=find_dfs_spanning(cd),\n                       node_size=500,width=2.0,\n                       arrowstyle='-&gt;')\nplt.draw()\n\n\n\n\nNote that the last tree in the above spanning forest for \\(S_0^3\\) crown DAG is not visible because if consist of a single vertex (node) \\(u\\).\nTODO: in the Reachability labels for version control graphs notebook from Google Colaboratory there are a few more examples to be copied, or replaced with interactive visualization (where you can choose the graph):\n\ndraw a didactic example of DAG from FELINE\ndraw DAG from level-filter example from FELINE\n\n\n\nComputing min-post tree intervals (positive-cut filter)\nInterval labeling (min-post strategy) labels each node \\(u\\) with a range \\(I_u = [s_u, e_u]\\), where \\(e_u\\) denotes the rank of the node \\(u\\) in some post-order traversal of the tree, where the ranks are assumed to begin at 1, and all the children of a node are assumed to be ordered and fixed for that traversal. Further, \\(s_u\\) denotes the lowest rank for any node \\(x\\) in the subtree rooted at \\(u\\) (i.e., including \\(u\\)).\nFor more see the section with the definitions.\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Parameters:\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Returns:\n  else: warn(msg)\n\nsource\n\n\nfind_dfs_intervals\n\n find_dfs_intervals (DG, attr=None)\n\nFind min-post labeling of all vertices in graph G, using DFS spanning-tree\nMin-post interval labeling strategy labels each node ‘u’ in the directed spanning tree (DFS tree here) of the acyclic graph DG with a range I_u = [s_u, e_u], where e_u denotes the rank of the node ‘u’ in post-order traversal of the tree (ranks assumed to begin at 1), and s_u denotes the lowest rank of any node ‘w’ in the subtree rooted at ‘u’ (including ‘u’).\nCheck that there are no runtime errors when running find_dfs_intervals(graph)\n\n# DEBUG\nsd=graphs.small_DAG_FELINE()\nsd.mpi={\n    'a':[1,5],'b':[7,9],\n    'c':[1,1],'d':[2,2],'e':[3,4],'f':[7,7],'g':[8,8],\n    'h':[3,3],\n}\n\nprint('Exact results depend on the spanning tree (DFS traversal order)')\nprint('found intervals: %r' % find_dfs_intervals(sd))\nprint('provided ones:   %r' % sd.mpi)\n\nExact results depend on the spanning tree (DFS traversal order)\nfound intervals: {'h': [1, 1], 'c': [1, 2], 'd': [3, 3], 'e': [4, 4], 'a': [1, 5], 'f': [6, 6], 'g': [7, 7], 'b': [6, 8]}\nprovided ones:   {'a': [1, 5], 'b': [7, 9], 'c': [1, 1], 'd': [2, 2], 'e': [3, 4], 'f': [7, 7], 'g': [8, 8], 'h': [3, 3]}\n\n\nCheck visually that min-post tree intervals make sense for an example of small DAG from the FELINE paper\n\n# get graph\nsd=graphs.small_DAG_FELINE()\n# TODO: move setting original positions to `small_DAG_FELINE()` to avoid repetitions\nsd.tree={\n  'a':(2,3),'b':(4.5,3),\n  'c':(1,2),'d':(2,2),'e':(3,2),'f':(4,2),'g':(5,2),\n  'h':(3,1)\n}\n\n# find min-post intervals, and make labels\nintervals=find_dfs_intervals(sd)\nintervals_repr={\n  v:('[%d,%d]'%(i[0],i[1]))\n  for (v,i) in intervals.items()\n}\n\n# configure plot\nplt.axis('off')\nplt.title('Small DAG with the spanning tree overlay and interval labels\\n'+\n          '(find_dfs_spanning, find_dfs_intervals)')\n\n# draw graph, its spanning tree and min-post tree interval labels\nnx.draw_networkx(sd,\n                 pos=sd.tree,\n                 node_size=500,width=8.0,\n                 edge_color='#aaaaaa',node_color='y')\nnx.draw_networkx_edges(sd,\n                       pos=sd.tree,\n                       edgelist=find_dfs_spanning(sd),\n                       node_size=500,width=2.0,\n                       arrowstyle='-&gt;')\nnx.draw_networkx_labels(sd,\n                        pos={u:(x+0.35,y-0.08) for (u,(x,y)) in sd.tree.items()},\n                        labels=intervals_repr)\nplt.draw()\n\n\n\n\n\nTODO: - interactive visualization of spanning tree and min-post tree intervals shown as labels - interactive visualization of post(v) vs level(v) drawn graph, with min-post intervals as labels\n\n\n\nComputing min-post graph intervals (negative-cut filter) (TODO)\n\n\nComputing PReaCH extensions to min-post intervals\nCode taken from section “DFS spanning-tree extra data” in Google Colab version of the Reachability labels for version control graphs notebook.\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section References:\n  else: warn(msg)\n\nsource\n\n\nfind_dfs_intervals_extra\n\n find_dfs_intervals_extra (DG, extra=False)\n\nFind DFS-derived data of all vertices in graph G\nMin-post interval labeling strategy labels each node ‘u’ in the directed spanning tree (DFS tree here) of the acyclic graph DG with a range I_u = [s_u, e_u], where e_u denotes the rank of the node ‘u’ in post-order traversal of the tree (ranks assumed to begin at 1), and s_u denotes the lowest rank of any node ‘w’ in the subtree rooted at ‘u’ (including ‘u’). This interval is stored under ‘min’ and ‘post’ keys in returned dict.\nFor depth first-search (DFS) all nodes reachable from node ‘u’ have their topological ordering in DFS tree, i.e. ‘post’, smaller than the ‘post’ value for ‘u’. Additionally we can define ‘f_min’ as minimum ‘post’ number for each node reachable from ‘u’, and ‘f_gap’ as maximum ‘post’ number of nodes reachable from ‘u’ that are not in DFS subtree rooted at ‘u’ or None if ‘f_gap’ would be equal to ‘u’ node’s ‘min’ number.\nWe can also define ‘p_tree’ to be node reachable from ‘u’, but not in subtree rooted at ‘u’, that (‘p_tree’) has largest subtree. If such node does not exist it is set to None. For DFS spanning tree we can calculate this value effectively.\nThis is based on modified algorithms from PReaCH paper [1]; note that the original version in the paper [1] uses preorder not postorder numbers (pre-max not min-post).\n\n# DEBUG\nsd=graphs.small_DAG_FELINE()\nfind_dfs_intervals_extra(sd, extra=True)\n\n{'h': {'post': 1, 'min': 1, 'f_min': 1, 'f_gap': None, 'p_tree': None},\n 'c': {'post': 2, 'min': 1, 'f_min': 1, 'f_gap': None, 'p_tree': None},\n 'd': {'post': 3, 'min': 3, 'f_min': 3, 'f_gap': None, 'p_tree': None},\n 'e': {'post': 4, 'min': 4, 'f_min': 1, 'f_gap': 1, 'p_tree': 'h'},\n 'a': {'post': 5, 'min': 1, 'f_min': 1, 'f_gap': 1, 'p_tree': None},\n 'f': {'post': 6, 'min': 6, 'f_min': 1, 'f_gap': 1, 'p_tree': 'h'},\n 'g': {'post': 7, 'min': 7, 'f_min': 7, 'f_gap': None, 'p_tree': None},\n 'b': {'post': 8, 'min': 6, 'f_min': 1, 'f_gap': 1, 'p_tree': None}}\n\n\n\n# get graph\nsd=graphs.small_DAG_FELINE()\n# TODO: move setting original positions to `small_DAG_FELINE()` to avoid repetitions\nsd.tree={\n  'a':(2,3),'b':(4.5,3),\n  'c':(1,2),'d':(2,2),'e':(3,2),'f':(4,2),'g':(5,2),\n  'h':(3,1)\n}\n\n# find min-post intervals, and make labels\nintervals=find_dfs_intervals_extra(sd)\nintervals_repr={\n  v:('[%d:%d,%d]'%(i['f_min'],i['min'],i['post']))\n  for (v,i) in intervals.items()\n}\n\n# configure plot\nplt.axis('off')\nplt.title('Small DAG with the spanning tree overlay and extended interval labels\\n'+\n          '(find_dfs_spanning, find_dfs_intervals_extra)')\n\n# draw graph, its spanning tree and min-post tree interval labels\nnx.draw_networkx(sd,\n                 pos=sd.tree,\n                 node_size=500,width=8.0,\n                 edge_color='#aaaaaa',node_color='y')\nnx.draw_networkx_edges(sd,\n                       pos=sd.tree,\n                       edgelist=find_dfs_spanning(sd),\n                       node_size=500,width=2.0,\n                       arrowstyle='-&gt;')\nnx.draw_networkx_labels(sd,\n                        pos={u:(x+0.25,y-0.12) for (u,(x,y)) in sd.tree.items()},\n                        labels=intervals_repr)\nplt.draw()"
  },
  {
    "objectID": "interval_labels.html#references",
    "href": "interval_labels.html#references",
    "title": "DFS Intervals Labelling",
    "section": "References",
    "text": "References\nTODO\n\n[PReaCH] Florian Merz, Peter Sanders: “PReaCH: A Fast Lightweight Reachability Index using Pruning and Contraction Hierarchies” (2014)\nIn: Schulz A.S., Wagner D. (eds) Algorithms - ESA 2014. ESA 2014. Lecture Notes in Computer Science, vol 8737.\nSpringer, Berlin, Heidelberg (Conference Paper: European Symposium on Algorithms)\nhttps://doi.org/10.1007/978-3-662-44777-2_58\narXiv:1404.4465v1 [cs.DS], 17 Apr 2014,\nhttp://arxiv.org/abs/1404.4465"
  },
  {
    "objectID": "reachability_index.html",
    "href": "reachability_index.html",
    "title": "Reachability index",
    "section": "",
    "text": "Reachability query is one of the fundamental graph operations to answer whether a vertex (also called a node) can reach another vertex over a large directed graph. Usually directed acyclic graphs (DAGs) are considered, as reachability query in original graph can be answered by taking graph of strongly connected components (which is acyclic) and answering the translated query there.\nLet \\(G = (V,E)\\) be a directed graph, with \\(V\\) being its set of vertices and \\(E \\subseteq V^2\\) its set of edges. A reachability query \\(r(u, v)\\) asks whether a vertex \\(v \\in V\\) is reachable from a vertex \\(u \\in V\\), i. e., whether there is a path from \\(u\\) to \\(v\\) in \\(G\\).\nThe main idea behind the various considered approaches in the literature is to compute a label for every vertex in a graph \\(G\\), by precomputing them offline. This is known as the index construction (there is constructed an index to maintain the mapping from vertices to labels). The index construction takes time, and storing the labels takes space.\nThe various methods to speed up reachability queries can be divided into two main categories (approaches), namely Label-Only and Label+G (or Label+Graph).\n\nLabel-Only approaches (as the name indicates) answer reachability queries using only the labels. Those have index size which is nonlinear function of the number of vertices (nodes), or have unbound index size.\nLabel+Graph approaches use labels computed where possible, and conduct on-line search (be it depth-first search (DFS), breadth-first search (BFS), or bidirectional BFS) at run-time, if the reachability queries cannot be answered using the labels only. This class of methods is also called Refined Online Search.\n\nAll the approaches take a different way to balance the three main costs, namely, - the index construction time, - the index size, and - the query time.\nNOTE: this is considered exploratory notebook, so the above text lack citations.\n\n\n\nReachability query tradeoffs\n\n\nThe two basic approaches on extremes of a spectrum are shown on the figure above.\nThe first approach (left side) is to pre-compute and store the full transitive closure of edges, which allows constant time queries, but requires a quadratic space complexity, making it infeasible to maintain the index in the case of very large graphs. - index build time: \\(\\mathcal{O}(|V|*|E|)\\) - query answering time: \\(\\mathcal{O}(1)\\) (constant time) - index size: \\(\\mathcal{O}(|V|^2)\\) (quadratic memory size)\nThe second approach (right side) is to employ a DFS or BFS, or bidi-BFS search to verify the reachability, starting from vertex \\(u\\) to vertex \\(v\\). This approach requires \\(O(|V| + |E|)\\) time for each query, which is often unacceptable. - index build time: \\(\\mathcal{O}(1)\\) (no build time) - query answering time: \\(\\mathcal{O}(|V|+|E|)\\) - index size: \\(\\mathcal{O}(1)\\) (no additional memory needed)"
  },
  {
    "objectID": "reachability_index.html#label-only-and-labelgraph-approaches",
    "href": "reachability_index.html#label-only-and-labelgraph-approaches",
    "title": "Reachability index",
    "section": "",
    "text": "Reachability query is one of the fundamental graph operations to answer whether a vertex (also called a node) can reach another vertex over a large directed graph. Usually directed acyclic graphs (DAGs) are considered, as reachability query in original graph can be answered by taking graph of strongly connected components (which is acyclic) and answering the translated query there.\nLet \\(G = (V,E)\\) be a directed graph, with \\(V\\) being its set of vertices and \\(E \\subseteq V^2\\) its set of edges. A reachability query \\(r(u, v)\\) asks whether a vertex \\(v \\in V\\) is reachable from a vertex \\(u \\in V\\), i. e., whether there is a path from \\(u\\) to \\(v\\) in \\(G\\).\nThe main idea behind the various considered approaches in the literature is to compute a label for every vertex in a graph \\(G\\), by precomputing them offline. This is known as the index construction (there is constructed an index to maintain the mapping from vertices to labels). The index construction takes time, and storing the labels takes space.\nThe various methods to speed up reachability queries can be divided into two main categories (approaches), namely Label-Only and Label+G (or Label+Graph).\n\nLabel-Only approaches (as the name indicates) answer reachability queries using only the labels. Those have index size which is nonlinear function of the number of vertices (nodes), or have unbound index size.\nLabel+Graph approaches use labels computed where possible, and conduct on-line search (be it depth-first search (DFS), breadth-first search (BFS), or bidirectional BFS) at run-time, if the reachability queries cannot be answered using the labels only. This class of methods is also called Refined Online Search.\n\nAll the approaches take a different way to balance the three main costs, namely, - the index construction time, - the index size, and - the query time.\nNOTE: this is considered exploratory notebook, so the above text lack citations.\n\n\n\nReachability query tradeoffs\n\n\nThe two basic approaches on extremes of a spectrum are shown on the figure above.\nThe first approach (left side) is to pre-compute and store the full transitive closure of edges, which allows constant time queries, but requires a quadratic space complexity, making it infeasible to maintain the index in the case of very large graphs. - index build time: \\(\\mathcal{O}(|V|*|E|)\\) - query answering time: \\(\\mathcal{O}(1)\\) (constant time) - index size: \\(\\mathcal{O}(|V|^2)\\) (quadratic memory size)\nThe second approach (right side) is to employ a DFS or BFS, or bidi-BFS search to verify the reachability, starting from vertex \\(u\\) to vertex \\(v\\). This approach requires \\(O(|V| + |E|)\\) time for each query, which is often unacceptable. - index build time: \\(\\mathcal{O}(1)\\) (no build time) - query answering time: \\(\\mathcal{O}(|V|+|E|)\\) - index size: \\(\\mathcal{O}(1)\\) (no additional memory needed)"
  },
  {
    "objectID": "reachability_index.html#types-of-labels-in-augmented-online-search-algorithms-labelg",
    "href": "reachability_index.html#types-of-labels-in-augmented-online-search-algorithms-labelg",
    "title": "Reachability index",
    "section": "Types of labels in augmented online search algorithms (Label+G)",
    "text": "Types of labels in augmented online search algorithms (Label+G)\nIn Label+Graph approach we conduct graph search if the reachability query cannot be answered using the labels only. In most methods those labels are then used to limit the search space (to augment the search to make if faster).\nTwo main types of labels are those that - exclude unreachable nodes, thus working as negative-cut filter, - find reachable nodes, thus working as positive-cut filter\nIn this case, usually one can define partial order between given labels, that is for vertices \\(u\\) and \\(v\\) we can have \\(l(u) \\preceq l(v)\\), or \\(l(u) \\succeq l(v)\\), or labels are incomparable (where \\(l(u)\\) is the label for vertex \\(u\\)).\n\nNegative-cut filter\nThis type of filter (or reachability label) can be used to exclude unreachable nodes.\n\nfor every \\(u\\) and \\(v\\), if \\(u \\neq v\\) and \\(u\\) can reach \\(v\\), that is \\(r(u,v)\\) holds, then we have \\(l(u) \\preceq l(v)\\)\ntherefore if the condition \\(l(u) \\preceq l(v)\\) is not met, then \\(u\\) cannot reach \\(v\\) (there is no path from \\(u\\) to \\(v\\))\nthe reverse is not always true; there can be false positives\n\n\n\nPositive-cut filter\nThis type of filter (or reachability label) can be used to find when nodes are reachable one from the other\n\nif for \\(u \\neq v\\) we have \\(l(u) \\preceq l(v)\\), then \\(r(u,v)\\) is true, that is \\(u\\) can reach \\(v\\)\n\\(v\\) can be reachable from \\(u\\) even if the condition is not met (\\(l(u) \\not\\preceq l(v)\\)): false negative"
  },
  {
    "objectID": "reachability_index.html#git-specific-considerations",
    "href": "reachability_index.html#git-specific-considerations",
    "title": "Reachability index",
    "section": "Git-specific considerations",
    "text": "Git-specific considerations\nFor reachability label to be considered for being added to Git (or more accurately to the git commit-graph file format it must work for large graphs.\n\nLinux kernel: 826 000 commits (2019)\nMS Windows: 3 100 000 commits (2019)\nAndroid (AOSP): 874 000 commits (2019)\nChromium: 772 000 commits (2019)\n\nThe commit graph (also known as revision graph) in version control systems is not static; it grows, but in very specific way. It grows by adding vertices (nodes), while existing vertices are immutable - with the sole exception that commits that are not reachable from one of the entry points (branches and tags) are removed during the garbage collection (gc) step.\nThere is no adding of edges (only as a byproduct of adding nodes), and no deletion of nodes.\nBecause the commit graph grows in size with time, we would like for reachability label to be able to be computed incrementally.\nThis could mean either that the reachability label is immutable itself, that is it would not change with the node-addition only growth of the graph. It could also mean that existing labels can be cheaply updated with the growth of the commit graph.\n\nTODO: the problem of layers of commit graph, see slides"
  },
  {
    "objectID": "related.html",
    "href": "related.html",
    "title": "Related works: various reachability labelings",
    "section": "",
    "text": "FELINE\nFELINE index (weak dominance drawing using two topological orderings)\n“Reachability Queries in Very Large Graphs: A Fast Refined Online Search Approach” (2014)\nhttp://openprocedings.org/EDBT/2014/paper_166.pdf\nGRAIL\nmin-post spanning-tree interval labeling (description)\n“GRAIL: Scalable Reachability Index for Large Graphs” (2010)\nhttps://www.researchgate.net/publication/220538786_GRAIL_Scalable_reachability_index_for_large_graphs\nPReaCH\nother pruning mechanisms (contraction hierarchies) with enhanced min-post / pre-max DFS spanning-tree interval labeling\n“PReaCH: A Fast Lightweight Reachability Index using Pruning and Contraction Hierarchies” (2014)\nhttps://arxiv.org/abs/1404.4465\nFERRARI\nmin-post spanning-tree intervals (exact and inexact)\n“FERRARI: Flexible and efficient reachability range assignment for graph indexing” (2013)\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.365.2894&rep=rep1&type=pdf\nIP+\nindependent permutations (IP) labelling, a variant of probabilistic reachable vertices set containment labeling\nH. Wei, J. X. Yu, C. Lu, and R. Jin. “Reachability querying: An independent permutation labeling approach.” PVLDB, 7(12):1191-1202 (2014)\nhttp://www1.se.cuhk.edu.hk/~lucan/paper/vldb14-reachability.pdf\nhttp://www1.se.cuhk.edu.hk/~lucan/paper/vldbj17-reachability.pdf\nBFL\nBloom filter labeling (BFL), a variant of probabilistic reachable vertices set containment labeling\n“Reachability Querying: Can It Be Even Faster?” (2016)\nhttps://ieeexplore.ieee.org/document/7750623 (no free PDF) https://www.researchgate.net/publication/310665301_Reachability_Querying_Can_It_Be_Even_Faster (no full text)"
  },
  {
    "objectID": "related.html#labelgraph-algorithms-online-search",
    "href": "related.html#labelgraph-algorithms-online-search",
    "title": "Related works: various reachability labelings",
    "section": "",
    "text": "FELINE\nFELINE index (weak dominance drawing using two topological orderings)\n“Reachability Queries in Very Large Graphs: A Fast Refined Online Search Approach” (2014)\nhttp://openprocedings.org/EDBT/2014/paper_166.pdf\nGRAIL\nmin-post spanning-tree interval labeling (description)\n“GRAIL: Scalable Reachability Index for Large Graphs” (2010)\nhttps://www.researchgate.net/publication/220538786_GRAIL_Scalable_reachability_index_for_large_graphs\nPReaCH\nother pruning mechanisms (contraction hierarchies) with enhanced min-post / pre-max DFS spanning-tree interval labeling\n“PReaCH: A Fast Lightweight Reachability Index using Pruning and Contraction Hierarchies” (2014)\nhttps://arxiv.org/abs/1404.4465\nFERRARI\nmin-post spanning-tree intervals (exact and inexact)\n“FERRARI: Flexible and efficient reachability range assignment for graph indexing” (2013)\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.365.2894&rep=rep1&type=pdf\nIP+\nindependent permutations (IP) labelling, a variant of probabilistic reachable vertices set containment labeling\nH. Wei, J. X. Yu, C. Lu, and R. Jin. “Reachability querying: An independent permutation labeling approach.” PVLDB, 7(12):1191-1202 (2014)\nhttp://www1.se.cuhk.edu.hk/~lucan/paper/vldb14-reachability.pdf\nhttp://www1.se.cuhk.edu.hk/~lucan/paper/vldbj17-reachability.pdf\nBFL\nBloom filter labeling (BFL), a variant of probabilistic reachable vertices set containment labeling\n“Reachability Querying: Can It Be Even Faster?” (2016)\nhttps://ieeexplore.ieee.org/document/7750623 (no free PDF) https://www.researchgate.net/publication/310665301_Reachability_Querying_Can_It_Be_Even_Faster (no full text)"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "How to contribute",
    "section": "",
    "text": "Before anything else, please install the git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts). After cloning the repository, run the following command inside it:\nnbdev_install_git_hooks\n\n\n\n\nEnsure the bug was not already reported by searching on GitHub under Issues.\nIf you’re unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\nBe sure to add the complete error messages.\n\n\n\n\nOpen a new GitHub pull request with the patch.\nEnsure that your PR includes a test that fails without your patch, and pass with it.\nEnsure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable.\n\n\n\n\n\n\nKeep each PR focused. While it’s more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\nDo not mix style changes/fixes with “functional” changes. It’s very difficult to review such PRs and it most likely get rejected.\nDo not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won’t need to review the whole PR again. In the exception case where you realize it’ll take many many commits to complete the requests, then it’s probably best to close the PR, do the work and then submit it again. Use common sense where you’d choose one way over another.\n\n\n\n\n\nDocs are automatically created from the notebooks in the nbs folder."
  },
  {
    "objectID": "CONTRIBUTING.html#how-to-get-started",
    "href": "CONTRIBUTING.html#how-to-get-started",
    "title": "How to contribute",
    "section": "",
    "text": "Before anything else, please install the git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts). After cloning the repository, run the following command inside it:\nnbdev_install_git_hooks"
  },
  {
    "objectID": "CONTRIBUTING.html#did-you-find-a-bug",
    "href": "CONTRIBUTING.html#did-you-find-a-bug",
    "title": "How to contribute",
    "section": "",
    "text": "Ensure the bug was not already reported by searching on GitHub under Issues.\nIf you’re unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\nBe sure to add the complete error messages.\n\n\n\n\nOpen a new GitHub pull request with the patch.\nEnsure that your PR includes a test that fails without your patch, and pass with it.\nEnsure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable."
  },
  {
    "objectID": "CONTRIBUTING.html#pr-submission-guidelines",
    "href": "CONTRIBUTING.html#pr-submission-guidelines",
    "title": "How to contribute",
    "section": "",
    "text": "Keep each PR focused. While it’s more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\nDo not mix style changes/fixes with “functional” changes. It’s very difficult to review such PRs and it most likely get rejected.\nDo not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won’t need to review the whole PR again. In the exception case where you realize it’ll take many many commits to complete the requests, then it’s probably best to close the PR, do the work and then submit it again. Use common sense where you’d choose one way over another."
  },
  {
    "objectID": "CONTRIBUTING.html#do-you-want-to-contribute-to-the-documentation",
    "href": "CONTRIBUTING.html#do-you-want-to-contribute-to-the-documentation",
    "title": "How to contribute",
    "section": "",
    "text": "Docs are automatically created from the notebooks in the nbs folder."
  },
  {
    "objectID": "example_graphs.html",
    "href": "example_graphs.html",
    "title": "Example directed graphs",
    "section": "",
    "text": "# this is needed only for demonstration purposes\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "example_graphs.html#create-crown-dag-known-as-s_30-graph",
    "href": "example_graphs.html#create-crown-dag-known-as-s_30-graph",
    "title": "Example directed graphs",
    "section": "Create crown DAG known as \\(S_3^0\\) graph",
    "text": "Create crown DAG known as \\(S_3^0\\) graph\n\n\n\nCrown DAG also known as \\(S_0^3\\) graph\n\n\nGenerate crown DAG (Directed Acyclic Graph), also known as \\(S_3^0\\) graph.\nThis graph is taken from FELINE paper (2014), where it is used to show falsely-implied path or false-positive of the FELINE index.\nThis graph is shown in Figure 4 of the FELINE paper.\n\nsource\n\ncrown_DAG\n\n crown_DAG ()\n\nReturns crown DAG known as S_3^0 graph\n\n\nDraw the crown DAG (crown_DAG())\nDraw the 6-nodes crown DAG, known as \\(S_3^0\\) graph, using [automatic] shell layout:\n\n#@title Draw the $S_3^0$ graph using [automatic] shell layout\nDG=crown_DAG()\nnx.draw_shell(DG,with_labels=True,node_size=500,width=2.0)\n\n\n\n\nDrawing the crown DAG with circular layout results in the same plot as with shell layout, shown above.\nDraw the 6-nodes crown DAG, known as \\(S_3^0\\) graph, using the default spring layout positioning:\n\nDG=crown_DAG()\nnx.draw(DG,with_labels=True,node_size=500,width=2.0)\n\n\n\n\n\n#DG=crown_DAG()\n#nx.draw_kamada_kawai(DG,with_labels=True,node_size=500,width=2.0)\n\nDraw the 6-nodes crown DAG, known as the \\(S_3^0\\) graph, using automatic bipartite layout.\nThis is a similar layout to the one used in left side of Figure 4 in the FELINE paper, just with changed order of nodes in each of sets of vertices (each partition).\n\nDG=crown_DAG()\nnx.draw(DG,pos=nx.bipartite_layout(DG,[1,'u',3],align='horizontal'),\n        with_labels=True,node_size=500,width=2.0)\n\n\n\n\nDraw the crown DAG known as \\(S_3^0\\) graph using weak dominance drawing.\nThis is the same layout as the one used on right size of Figure 4 in the FELINE paper. It shows example of exception between nodes \\(u\\) and \\(v\\). The orange arrow is a falsely implied path or a false-positive for the FELINE index.\nIt is important to notice that some graphs, such as \\(S_3^0\\), do not admit a 2D index which is free of false-positives. The weak dominance drawing shown below has the smallest possible number of false-positives for this graph.\n\n#@title Draw the $S_3^0$ graph using weak dominance drawing\nfrom matplotlib.patches import Rectangle, FancyArrowPatch\n\nax=plt.gca()\nax.add_patch(Rectangle((2,2), 5, 5, facecolor=\"lightgrey\",alpha=0.5))\nax.add_patch(\n    FancyArrowPatch((2,2),(5,5),\n                    shrinkA=15,shrinkB=15,\n                    arrowstyle='fancy',mutation_scale=18,\n                    linestyle='dashed',hatch='\\\\',color='orange')\n)\n\nplt.grid(True)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('$S_3^0$ graph')\nplt.xlim(xmin=0,xmax=7)\nplt.ylim(ymin=0,ymax=7)\n\nDG=crown_DAG()\nDG.pos={1:(1,4),2:(3,6),'u':(2,2),3:(4,1),4:(6,3),'v':(5,5)}\nnx.draw_networkx(DG,pos=DG.pos,\n                 node_size=500,width=2.0,label='$S_3^0$ graph')"
  },
  {
    "objectID": "example_graphs.html#create-the-small-dag-example-in-figure-2-in-feline-paper",
    "href": "example_graphs.html#create-the-small-dag-example-in-figure-2-in-feline-paper",
    "title": "Example directed graphs",
    "section": "Create the small DAG (example in Figure 2 in FELINE paper)",
    "text": "Create the small DAG (example in Figure 2 in FELINE paper)\n\n\n\nAn example of a small DAG\n\n\nAn example of a small DAG from FELINE paper, used in various places\n\nsource\n\nsmall_DAG_FELINE\n\n small_DAG_FELINE ()\n\nCreate small DAG (example in Figure 2 in FELINE paper)\n\n\nDraw a small DAG example (small_DAG_FELINE())\nDraw using default spring layout (somewhat random)\n\nsd=small_DAG_FELINE()\nnx.draw(sd,with_labels=True,node_size=500,width=2.0)\n\n\n\n\nDraw using node positions created with the help of pydot module and Graphviz.\nWe use ‘dot’ layout algorithm, which creates “hierarchical” or layered drawings of directed graphs. This is the default tool in Graphviz to use if edges have directionality (like in this case).\n\ntry:\n    import pydot\n    \n    sd=small_DAG_FELINE()\n    pos=nx.drawing.nx_pydot.pydot_layout(sd, prog='dot')\n    nx.draw(sd, pos=pos, with_labels=True,\n            node_size=500,width=2.0,node_color='#FF7F00')\nexcept ModuleNotFoundError:\n    print(\"'pydot' module not installed\")\n\n'pydot' module not installed\n\n\nDraw a small DAG as shown in Figure 2 in the FELINE paper (levels-based, if we use levels starting from sources)\n\n#@title Draw a small DAG, as in Figure 2 (source levels-based)\nlimits = plt.axis('off')\nsd=small_DAG_FELINE()\nnx.draw_networkx(sd,\n                 pos={'a':(2,3),'b':(4.5,3),'c':(1,2),'d':(2,2),'e':(3,2),'f':(4,2),'g':(5,2),'h':(3,1)},\n                 node_size=500,width=2.0,node_color='y')\n\n\n\n\nDraw a small DAG from FELINE paper using weak dominance drawing\n\n#@title Draw small DAG using weak dominance drawing\nfrom matplotlib.patches import Rectangle\n\nsd=small_DAG_FELINE()\nsd.xy=dict(zip([chr(c) for c in range(ord('a'),ord('h')+1)],\n               zip([1,5,2,3,4,6,8,7],[4,1,7,6,5,3,2,8])))\n\nax=plt.gca()\nax.add_patch(Rectangle(sd.xy['a'], 8, 5, facecolor=\"lightgrey\",alpha=0.4))\n\nfor node in sd.nodes:\n  if sd.out_degree(node) == 0:\n    continue\n  (vx,vy)=sd.xy[node]\n  #print(node,vx,vy)\n  plt.axhline(xmin=vx/9.0,y=vy,color='0.5', ls=':')\n  plt.axvline(x=vx,ymin=vy/9.0,color='0.7', ls=':')\n\nplt.grid(True)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Small DAG (Fig. 2 of FELINE paper): example of dominance regions')\nplt.xlim(xmin=0,xmax=9)\nplt.ylim(ymin=0,ymax=9)\n\nnx.draw_networkx(sd,pos=sd.xy,\n                 node_size=500,width=2.0,node_color='y')\n\n\n\n\nThe above figure shows example of dominance regions for the drawing of a small DAG (Figure 3 of FELINE paper)."
  },
  {
    "objectID": "example_graphs.html#create-a-didactic-example-of-a-dag-a-tree",
    "href": "example_graphs.html#create-a-didactic-example-of-a-dag-a-tree",
    "title": "Example directed graphs",
    "section": "Create a didactic example of a DAG (a tree)",
    "text": "Create a didactic example of a DAG (a tree)\n\n\n\nA didactic example of a DAG\n\n\nCreates a tree DAG, used as a didactic example in the FELINE paper, where it is shown in Figure 6\n\nsource\n\ntree_DAG\n\n tree_DAG ()\n\nCreate tree DAG (‘didactic example’ in Figure 6 in FELINE paper)\nThe created tree graph has a single source node with out-degree of 3, its 3 out-neighbours have out-degrees, respectively, of 2, 1 and 2. Out-neighbours of those nodes are sink nodes with out-degree of 0. The tree has maximum level of 3.\nBelow there is ASCII-art rendering of this graph; edges point downward\n                        'a'\n                      __/|\\__\n                     /   |                              'b'  'c'  'd'\n                   / \\   |   /                           /  |   |   |                          'e' 'f' 'g' 'h' 'i'\n\n\nDraw a tree DAG (didactic example from Figure 6 in FELINE paper) - tree_DAG()\nDraw using node positions created with the help of pydot module and Graphviz, if it is present (installed).\nWe use ‘dot’ layout algorithm, which creates “hierarchical” or layered drawings of directed graphs. This is the default tool in Graphviz to use if edges have directionality (like in this case).\n\ntry:\n    import pydot\n    \n    de=tree_DAG()\n    pos=nx.drawing.nx_pydot.pydot_layout(de, prog='dot')\n    nx.draw(de, pos=pos, with_labels=True,\n            node_size=500,width=2.0,node_color='#7DF9FF')\nexcept ModuleNotFoundError:\n    print(\"'pydot' module not installed\")\n\n'pydot' module not installed\n\n\nDraw a didactic example of a DAG, as in Figure 6 (levels-based) of FELINE paper\n\n#@title Draw a didactic example of a DAG, as in Figure 6 (levels-based)\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.lines import Line2D\n\nde=tree_DAG()\n\nplt.axis('off')\nde.tree={\n  'a':(3,3),\n  'b':(1.5,2),'c':(3,2),'d':(4.5,2),\n  'e':(1,1),'f':(2,1),'g':(3,1),'h':(4,1),'i':(5,1)\n}\n\nax=plt.gca()\nax.add_patch(Rectangle((2.5,0.5), 1, 3, facecolor=\"grey\",alpha=0.3))\nax.add_line(Line2D([2.5,2.5], [0.5,3.5],lw=5., alpha=0.3))\nax.add_line(Line2D([3.5,3.5], [0.5,3.5],lw=5., alpha=0.3))\n\nnx.draw_networkx(de,\n                 pos=de.tree,\n                 node_size=500,width=2.0,node_color='c')\n\n\n\n\nThis figure shows a didactic example of a DAG and its two topological orderings obtained by Algorithm 1 in FELINE-index paper.\nGiven a query \\(r(a,g)\\), all vertices after \\(g\\) in first and second table are discarded (on the left and on the right of \\(g\\)). This strategy reduces the search space to only vertices \\(a\\), \\(c\\) and \\(g\\).\nDraw a didactic example of a DAG (tree DAG) using weak dominance drawing\n\n#@title Draw a didactic example of a DAG using weak dominance drawing\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.lines import Line2D\n\nde=tree_DAG()\no1=list('abefcgdhi')\no2=list('adihcgbfe')\nde.pos={v: (x,0) for (x,v) in enumerate(o1)}\nfor (y,v) in enumerate(o2):\n  de.pos[v]=(de.pos[v][0],y)\n\nax=plt.gca()\nax.add_patch(Rectangle(de.pos['a'],\n                       de.pos['g'][0] - de.pos['a'][0],\n                       de.pos['g'][1] - de.pos['a'][1],\n                       facecolor=\"grey\",alpha=0.35))\nax.add_line(Line2D([de.pos['g'][0],de.pos['g'][0]],\n                   [de.pos['g'][1],de.pos['a'][1]],\n                   lw=5., alpha=0.3))\nax.add_line(Line2D([de.pos['g'][0],de.pos['a'][0]],\n                   [de.pos['g'][1],de.pos['g'][1]],\n                   lw=5., alpha=0.3))\n\n  \nplt.axvline(x=de.pos['a'][1],color='0.5', ls=':')\nplt.axhline(y=de.pos['a'][0],color='0.5', ls=':')\n\nplt.grid(True)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Didactic example of a DAG: search space of FELINE')\nplt.xlim(xmin=-1,xmax=9)\nplt.ylim(ymin=-1,ymax=9)\n\nnx.draw_networkx(de,pos=de.pos,\n                 node_size=500,width=2.0,node_color='c')\n\n\n\n\nNOTE: we are skipping figure titled ‘Didactic example of a DAG: search space of FERRARI(topological ordering x)’ in Reachability labels for version control graphs.ipynb. If it is to be added, it would be added in the notebook describing various reachability indices."
  },
  {
    "objectID": "example_graphs.html#create-the-levels-example-dag-from-figure-9-in-feline-paper",
    "href": "example_graphs.html#create-the-levels-example-dag-from-figure-9-in-feline-paper",
    "title": "Example directed graphs",
    "section": "Create the levels example DAG (from Figure 9 in FELINE paper)",
    "text": "Create the levels example DAG (from Figure 9 in FELINE paper)\n\n\n\nExample DAG to show level filter\n\n\nThis is the graph created to help understand the applicability of level filter. The example shows that a false-positive query \\(r(h, g)\\) could be pruned in constant time just by verifying that \\(h\\) and \\(g\\) are at the same level.\nThe numbers represents the coordinates attributed by FELINE algorithm; the vertical position of vertex corresponds to its forward topological level.\n\nsource\n\nlevels_DAG_FELINE\n\n levels_DAG_FELINE ()\n\nCreate graph from Figure 9 in FELINE paper, used to show levels-filter\n\n\nDraw levels-filter example (from Figure 9 in FELINE paper) - levels_DAG_FELINE()\nDraw the graph as it was shown in Figure 9 of the FELINE paper\n\nlf=levels_DAG_FELINE()\n# forwards level\nlf.lvl={\n    'g':0,'h':0,'i':0,\n    'c':1,'d':1,'e':1,'f':1,\n    'a':2,'b':2\n}\n# coordinates attributed by FELINE\nlf.fel={\n    'a':(1,4),'b':(6,1),'c':(2,9),'d':(3,7),'e':(4,5),\n    'f':(7,2),'g':(8,8),'h':(5,6),'i':(9,3),\n}\n#@title Draw an example DAG for levels-filter, as in Figure 9 of FELINE paper (levels-based)\n# positions in Figure 9 of FELINE paper\nlf.tree={\n  'a':(2,2),'b':(4,2),\n  'c':(1,1),'d':(2,1),'e':(3,1),'f':(4,1),\n  'g':(2,0),'h':(3,0),'i':(4,0)\n}\n\n#plt.axis('off')\nplt.ylabel('forward topological level')\n#plt.subplots_adjust(left=0,right=1.2)\nplt.xlim(left=0.75,right=4.5)\nnx.draw_networkx(lf,\n                 pos=lf.tree,\n                 nodelist=list(lf.lvl.keys()),\n                 node_color=list(lf.lvl.values()),\n                 cmap=plt.cm.Greens,\n                 vmin=-2,vmax=5,\n                 node_size=500,width=2.0)\nlabels=nx.draw_networkx_labels(lf,\n                               pos={k: (v[0]+0.31,v[1]-0.06) for (k,v) in lf.tree.items()},\n                               labels=lf.fel)\n\n\n\n\n\n\n\nExample DAG to show level filter\n\n\n\nlf=levels_DAG_FELINE()\n# forwards level\nlf.lvl={\n    'g':0,'h':0,'i':0,\n    'c':1,'d':1,'e':1,'f':1,\n    'a':2,'b':2\n}\n# coordinates attributed by FELINE\nlf.fel={\n    'a':(1,4),'b':(6,1),'c':(2,9),'d':(3,7),'e':(4,5),\n    'f':(7,2),'g':(8,8),'h':(5,6),'i':(9,3),\n}\n#@title Draw an example DAG for levels-filter, as in Figure 9 of FELINE paper (levels-based)\n# positions in Figure 9 of FELINE paper\nlf.tree={\n  'a':(2,2),'b':(4,2),\n  'c':(1,1),'d':(2,1),'e':(3,1),'f':(4,1),\n  'g':(2,0),'h':(3,0),'i':(4,0)\n}\n\n#@title Draw an example DAG for levels-filter, backwards levels based\n# correction from forward to backward topological levels\nlf.lvl['c']=0\nlf.tree['c']=(1,0)\n\n# configure plot\n#plt.axis('off')\nplt.ylabel('backward topological level')\n#plt.subplots_adjust(left=0,right=1.2)\nplt.xlim(left=0.75,right=4.5)\nnx.draw_networkx(lf,\n                 pos=lf.tree,\n                 nodelist=list(lf.lvl.keys()),\n                 node_color=list(lf.lvl.values()),\n                 cmap=plt.cm.Greens,\n                 vmin=-2,vmax=5,\n                 node_size=500,width=2.0)\nlabels=nx.draw_networkx_labels(lf,\n                               pos={k: (v[0]+0.21,v[1]-0.04) \n                                    for (k,v) in lf.tree.items()},\n                               labels=lf.fel)\n\n\n\n\nDraw using node positions created with the help of pydot module and Graphviz, if it is present (installed).\n\ntry:\n    import pydot\n    \n    lf=levels_DAG_FELINE()\n    pos=nx.drawing.nx_pydot.pydot_layout(lf, prog='dot')\n    nx.draw(lf, pos=pos, with_labels=True,\n            node_size=400,width=2.0,node_color='#66ff66')\nexcept ModuleNotFoundError:\n    print(\"'pydot' module not installed\")\n\n'pydot' module not installed\n\n\nDraw using weak dominance drawing (coordinates attributed by FELINE)\n\n#@title Draw an example DAG for levels-filter, using weak dominance drawing\nfrom matplotlib.patches import Rectangle, FancyArrowPatch\nfrom matplotlib.lines import Line2D\n\nlf=levels_DAG_FELINE()\n# backward level\nlf.lvl={\n    'c':0,'g':0,'h':0,'i':0,\n    'd':1,'e':1,'f':1,\n    'a':2,'b':2\n}\n# coordinates attributed by FELINE\nlf.fel={\n    'a':(1,4),'b':(6,1),'c':(2,9),'d':(3,7),'e':(4,5),\n    'f':(7,2),'g':(8,8),'h':(5,6),'i':(9,3),\n}\n\nax=plt.gca()\nax.add_patch(Rectangle(lf.fel['h'],\n                       #lf.fel['g'][0] - lf.fel['h'][0],\n                       #lf.fel['g'][1] - lf.fel['h'][1],\n                       10 - lf.fel['h'][0],\n                       10 - lf.fel['h'][1],\n                       facecolor=\"grey\",alpha=0.25))\n\nax.add_patch(\n    FancyArrowPatch(lf.fel['h'],lf.fel['g'],\n                    shrinkA=15,shrinkB=15,\n                    arrowstyle='fancy',mutation_scale=18,\n                    linestyle='dashed',hatch='\\\\',color='orange')\n)\n  \n#plt.axvline(x=lf.fel['h'][0],color='0.4', ls=':')\n#plt.axhline(y=lf.fel['h'][1],color='0.4', ls=':')\n\nplt.grid(True)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Example of false-implied path pruned by level filter')\nplt.xlim(xmin=-0.5,xmax=10)\nplt.ylim(ymin= 0.0,ymax=10)\n\nnx.draw_networkx(lf,pos=lf.fel,\n                 node_size=500,width=2.0,\n                 nodelist=list(lf.lvl.keys()),\n                 edgecolors='k',\n                 node_color=list(lf.lvl.values()),\n                 cmap=plt.cm.Greens,\n                 vmin=-2,vmax=5)\nlabels=nx.draw_networkx_labels(lf,\n                               pos={k: (v[0]-0.68,v[1]+0.36) for (k,v) in lf.fel.items()},\n                               labels={k:'$l_'+str(k)+'='+str(l)+'$'\n                                       for (k,l) in lf.lvl.items()})\n\n\n\n\nExample where the vertex \\(h\\) does not reach \\(g\\), but there is false-implied path (orange) between them (\\(g\\) is in domination area of \\(h\\), shown as the gray area). However, \\(g\\) and \\(h\\) are in the same level: \\(l_g = l_h = 0\\), and the level filter prunes the search. The labels and the color represents level of vertex."
  },
  {
    "objectID": "example_graphs.html#create-example-rch-graph-from-preach-paper",
    "href": "example_graphs.html#create-example-rch-graph-from-preach-paper",
    "title": "Example directed graphs",
    "section": "Create example RCH graph (from PReaCH paper)",
    "text": "Create example RCH graph (from PReaCH paper)\nCreate an example DAG used among others for showing how Reachability Contraction Hierarchies (RCH) labeling works. This graph is shown on Figures 1 and 2 in the PReaCH paper.\nThe graph as shown in Figure 1 in the PReaCH paper (preprint) can be seen below:\n\n\n\nExample RCH graph\n\n\nCaption: Example RCH. Edges in the forward search space are light green, and those in the backward search space are dark blue. The search spaces for a query from \\(s\\) to \\(t\\) are circled. Node labels specify the RCH node ordering.\n\nsource\n\nRCH_graph\n\n RCH_graph ()\n\nCreate RCH example graph (example in Figure 1 and 2 in PReaCH paper)\nPositions of nodes in the drawing from Figure 1 and 2 in PReaCH paper is stored in pos attribute of returned graph object.\nBackward topological levels (i.e. with level equal to 0 for nodes with no outgoing edges, that is with out-degree of 0) are stored in lvl attribute of returned graph object.\n\n\nDraw RCH graph (from Figures 1 and 2 in the PReaCH paper) - RCH_graph()\nDraw using node positions created with the help of pydot module and Graphviz, if it is present (installed).\nWe use ‘dot’ layout algorithm, which creates “hierarchical” or layered drawings of directed graphs. This is the default tool in Graphviz to use if edges have directionality (like in this case).\n\ntry:\n    import pydot\n    \n    ch=RCH_graph()\n    pos=nx.drawing.nx_pydot.pydot_layout(ch, prog='dot')\n    nx.draw(ch, pos=pos, with_labels=True,\n            node_size=500,width=2.0,node_color='#ee88ff')\nexcept ModuleNotFoundError:\n    print(\"'pydot' module not installed\")\n\n'pydot' module not installed\n\n\n\nch=RCH_graph()\n\nplt.axis(\"off\")\nplt.title('Example graph from PReaCH paper (Figures 1 and 2)\\ndraw reversed - with arrows pointing down')\nnx.draw_networkx(ch,pos={key:(value[0],-value[1]) for (key,value) in ch.pos.items()},\n                 node_size=500,width=2.0,node_color='#dd44ff')\nplt.show()\n\n\n\n\n\nch=RCH_graph()\n\nplt.axis(\"off\")\nplt.title('Example graph from PReaCH paper (Figures 1 and 2)')\nnx.draw_networkx(ch,pos=ch.pos,\n                 node_size=500,width=2.0,node_color='m')\nplt.show()"
  },
  {
    "objectID": "example_graphs.html#create-example-commit-graph-from-stolee-blog-posts",
    "href": "example_graphs.html#create-example-commit-graph-from-stolee-blog-posts",
    "title": "Example directed graphs",
    "section": "Create example commit graph (from Stolee blog posts)",
    "text": "Create example commit graph (from Stolee blog posts)\nAn example commit graph from the series of blog posts “Supercharging the Git Commit Graph” by Derrick Stolee.\nIn the drawing of this graph below arrows (directed edges) point from right to left.\n\n\n\nAn example Git commit graph\n\n\n\nsource\n\ncommit_graph_Stolee\n\n commit_graph_Stolee ()\n\nCreate an example of Git commit graph\nThis graph is taken from Derrick Stolee series of blog posts “Supercharging the Git Commit Graph”.\nPositions of nodes in the drawing from commit-graph-example.png is stored in pos attribute of returned graph object.\n\n\nDraw an example commit graph (commit_graph_Stolee())\nDraw using node positions created with the help of pydot module and Graphviz, using ‘dot’ layout algorithm, if the module is present (installed).\nThe resulting positioning is transformed from vertical to horizontal, and flipped along horizontal axis to better match the original drawing in the Stolee blog post(s).\n\ntry:\n    import pydot\n    \n    cg=commit_graph_Stolee()\n    pos=nx.drawing.nx_pydot.pydot_layout(cg, prog='dot')\n    nx.draw(cg, pos={n:(y,-x) for (n,(x,y)) in pos.items()}, with_labels=True,\n           node_size=500,width=2.0,node_color='#ff9988')\nexcept ModuleNotFoundError:\n    print(\"'pydot' module not installed\")\n\n'pydot' module not installed\n\n\nPlot just like it was done in Derrick Stolee blog post (but with arrows on directed edges).\n\ncg=commit_graph_Stolee()\n\nplt.axis(\"off\")\nplt.gca().set_aspect(1.2)\nplt.title('An example Git commit graph (Stolee blog post)')\nnx.draw_networkx(cg,pos={n:(x,-y) for (n,(x,y)) in cg.pos.items()},\n                 node_size=500,width=2.0,node_color='#ff9955')\nplt.show()"
  },
  {
    "objectID": "example_graphs.html#references",
    "href": "example_graphs.html#references",
    "title": "Example directed graphs",
    "section": "References",
    "text": "References\nThere is no single way of handling references and citations (bibliography) in Jupyter Notebook.\nSome of the various solutions are presented below: - nbconvert examples :: Managing citations in the IPython Notebook - Writing academic papers in plain text with Markdown and Jupyter notebook by Sylvain Deville (2015) - use the Document Tools of the Calico suite: Calico Document Tools and BibTeX - cite2c extension for live citations in IPython notebooks, which uses Zotero as citations database, and rendering both bibliographies and inline citations in Markdown cells with the help of citeproc-js - use jupyter_latex_envs Jupyter extension to generate Reference section; note, that nbdev now only supports converting of BibTeX citations by the way of \\cite{} to &lt;a class=\"latex_cit\" id=\"call-\" href=\"#cit-\"&gt;&lt;/a&gt; via cite2link()\nTherefore, for the time being, live / hyperlinked citations and automatically generated bibliography won’t be used.\nThe bibliography is currently generated by hand, and work are referred to by their nicknames, e.g. “the FELINE paper”.\n\n[FELINE] Renê R. Veloso, Loïc Cerf, Wagner Meira Jr, Mohammed J. Zaki: “Reachability Queries in Very Large Graphs: A Fast Refined Online Search Approach”, Proc. 17th International Conference on Extending Database Technology (EDBT), March 24-28, 2014, Athens, Greece: ISBN 978-3-89318065-3,\nhttp://openprocedings.org/EDBT/2014/paper_166.pdf\nhttps://dx.doi.org/10.5441/002/edbt.2014.46\n[PReaCH] Florian Merz, Peter Sanders: “PReaCH: A Fast Lightweight Reachability Index using Pruning and Contraction Hierarchies”, arXiv:1404.4465v1 [cs.DS], 17 Apr 2014,\nhttps://arxiv.org/abs/1404.4465\n[Stolee] Derrick Stolee: “Supercharging the Git Commit Graph II: File Format”, July 2nd, 2018,\nhttps://devblogs.microsoft.com/devops/supercharging-the-git-commit-graph-ii-file-format/"
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "Graphs in Python",
    "section": "",
    "text": "This notebooks describes various tools that can be used to work with graphs in Python.\nThe Network Chart - The Python Graph Gallery uses mainly NetworkX library for showing examples of network diagrams / charts (or graphs), but also mentions Graph Tool library for consideration, especially concerning high dimension networks.\nList of considered libraries"
  },
  {
    "objectID": "tools.html#python-graph-libraries",
    "href": "tools.html#python-graph-libraries",
    "title": "Graphs in Python",
    "section": "Python graph libraries",
    "text": "Python graph libraries\n\n\n\nnetworkx-logo\n\n\n\nNetworkX library – data structures for graphs (or networks) along with graph algorithms, generators, and drawing tools\n\nData structures for graphs, digraphs, and multigraphs\nMany standard graph algorithms\nNetwork structure and analysis measures\nNodes can be “anything” (e.g., text, images, XML records),\nEdges can hold arbitrary data (e.g., weights, time-series)\n\ncuGraph - GPU Graph Analytics\n\nRAPIDS Graph Analytics Library\nProcess data found in GPU DataFrames (cuDF), with the Pandas-like API\nNetworkX-like API to allow existing code to be ported with minimal effort into RAPIDS\ncurrent limitations: Vertex IDs are expected to be contiguous integers starting from 0 (renumbering can be done automatically)\nthe renumbered vertex IDs need to be representable in 32-bit integers\n\n\ngraph-tool\n\nGraph Tool library – the core data structures and algorithms are implemented in C++\n\nBased heavily on the Boost Graph C++ Library\nMany algorithms are implemented in parallel using OpenMP\nSupport for arbitrary vertex, edge or graph properties\nAn extensive array of features is included, such as graph statistics, centrality measures, standard topological algorithms\nGraph-tool has its own layout algorithms and versatile, interactive drawing routines based on cairo and GTK+, but it can also work as a very comfortable interface to the excellent graphviz package\n\n\nDrawing in Jupyter Notebooks described e.g. in https://gist.github.com/jg-you/2ce69ffccf6f3ed17c09e5d7b2695f1c\n\n\n\nsnap-logo\n\n\n\nSnap.py is a Python interface for SNAP (Stanford Network Analysis Platform)\n\nSNAP is a general purpose, high performance system in C++ for analysis and manipulation of large networks\nMost of the SNAP functionality is available via Snap.py in Python\nIt efficiently manipulates large graphs, calculates structural properties, generates regular and random graphs, and supports attributes on nodes and edges\nNodes, edges and attributes in a graph or a network can be changed dynamically during the computation\n\n\n\n\nNetworKit is a growing open-source toolkit for large-scale network analysis.\n\nHigh-performance algorithms are written in C++ and exposed to Python via the Cython toolchain\nDocumented in NetworKit UserGuide notebook\nSeamless integration with Python libraries for scientific computing and data analysis, e.g.\n\npandas for data framework processing and analytics,\nmatplotlib for plotting,\nnetworkx for additional network analysis tasks, or\nnumpy and scipy for numerical and scientific computing\n\n\n\n\n\n\niGraph is a collection of network analysis tools with the emphasis on efficiency, portability and ease of use.\n\nopen source and free\nthe source code of igraph packages is written in C\ncan be programmed in Python, R, Mathematica, and C/C++\npython-igraph documentation: User’s Manual with Tutorial, API reference\n\nsupports inline plots within a Jupyter notebook via both the Cairo and matplotlib backend\nyou can generate graph from edges stored in a pandas.DataFrame\n\ncapable of handling large networks efficiently\ninteractive and non-interactive usage are both supported\n\npython-graph – A library for working with graphs in Python\n\nno documentation (?)"
  },
  {
    "objectID": "tools.html#examples",
    "href": "tools.html#examples",
    "title": "Graphs in Python",
    "section": "Examples",
    "text": "Examples\nFirst, we need to import appropriate libraries\n\n#@title Imports for graphs (NetworkX and matplotlib)\nimport networkx as nx\n#import matplotlib.pyplot as plt\n\nLet’s create crown DAG (Directed Acyclic Graph), know as \\(S_3^0\\) graph:\n\n#@title Create a crown DAG known as $S_3^0$ graph\nDG=nx.DiGraph()\nDG.add_nodes_from([1,2,3,4,'u','v'])\nDG.add_edges_from([(1,2),(3,4),(1,'v'),(3,'v'),('u',2),('u',4)])\n\nDraw the \\(S_3^0\\) graph using [automatic] shell layout,\nusing draw_shell from networkx\n\n#@title Draw the $S_3^0$ graph using [automatic] shell layout\nnx.draw_shell(DG,with_labels=True,node_size=500,width=2.0)\n\n\n\n\nSee https://networkx.github.io/documentation/stable/auto_examples/index.html for more examples"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Commit graph labeling for speeding up Git commands",
    "section": "",
    "text": "Git uses various clever methods for making operations on very large repositories faster, from bitmap indices for git fetch[1], to generation numbers (also known as topological levels) in the commit-graph file for commit graph traversal operations like git log --graph[2].\nThe goal of this project is to examine various possible improvements that can make Git even faster, other than just using generation numbers. For example there are many methods to make reachability queries in very large graphs faster; it remains to be seen if they would work on large commit graphs (the graph of project history) as well as they work on other real-life graphs.\nUltimately, this project is about examining extension to Git’s commit-graph feature, including mainly adding reachability indexes / labels for the DAG (Directed Acyclic Graph) of revisions.\nThis project, while mainly exploratory in nature, is using nbdev library for literate programming in Python using Jupyter Notebooks – not to create a Python module (to publish in PyPi and/or Conda), but to allow for splitting it up.\nThe original notebook at Google Colaboratory: “Reachability labels for version control graphs.ipynb” got quite unwieldy; it takes too much time to run it. By splitting it into smaller notebooks, and turning the code into helper modules, the hope is that it should be possible to quickly go to the interesting parts of exploration."
  },
  {
    "objectID": "index.html#graph-operations-in-git",
    "href": "index.html#graph-operations-in-git",
    "title": "Commit graph labeling for speeding up Git commands",
    "section": "Graph operations in Git",
    "text": "Graph operations in Git\nThis project focuses on the Git operations that involve examining and walking the commit graph, i.e. the project history. Such operations include:\n\ngit merge-base --is-ancestor\ngit branch --contains\ngit tag --contains\ngit branch --merged\ngit tag --merged\ngit merge-base --all\ngit log --topo-sort\n\nOnly the first command performs straight reachability query."
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "Commit graph labeling for speeding up Git commands",
    "section": "Table of contents",
    "text": "Table of contents\n\n\n\n\n\n\nWarning\n\n\n\nthis is not generated automatically\n\n\n\nGraphs in Python\nRelated works: various reachability labelings\nExample directed graphs\nDrawing graphs\nReachability index\nTopological levels\nDFS intervals labelling\nReachability queries\nExtracting commit graphs from Git repositories\n\nExploring extraction of commit graphs from Git repositories, and examining their shape and stats\n\nCheckpointing\nGraph datasets\nLarge Git repositories\nGraph stats\nReachability evaluation"
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "Commit graph labeling for speeding up Git commands",
    "section": "Slides",
    "text": "Slides\nThis topic is covered in much more details in slides for the presentation “Graph operations in Git version control system” by Jakub Narebski (2019).\nThose slides are also available to read on SlideShare and on Speaker Deck:\n\nhttps://www.slideshare.net/JakubNarbski/graph-operations-in-git-version-control-system-how-the-performance-was-improved-for-large-repositories-how-can-it-be-further-improved\nhttps://speakerdeck.com/jnareb/graph-operations-in-git-and-how-to-make-them-faster"
  },
  {
    "objectID": "index.html#todo-notebooks-to-split",
    "href": "index.html#todo-notebooks-to-split",
    "title": "Commit graph labeling for speeding up Git commands",
    "section": "TODO: Notebooks to split",
    "text": "TODO: Notebooks to split\n\nReachability labels for version control graphs\n\ngraphs in Python\nrelated work / references\nexample graphs\nlevel filter\nmin-post intervals filter\npre-max version of intervals filter (maybe)\nFELINE filter (maybe)\ncontraction hierarchy figure (maybe)\nhelper functions (find_heads, etc.)\ncomputing spanning tree\ncomputing reach of a node\ncomputing levels\ncomputing min-post intervals\ncomputing FELINE (maybe)\ntesting quality [of FELINE]\nreachability queries with levels and min-post\nreachability queries with FELINE and PReaCH min-post (maybe)\nreachability query stats\npost vs level graphs visualization\ncode for drawing graphs\ninteractive visualizations\nversion control graphs (LARGE)\n\ngit in Python (somewhat: shell vs GitPython)\ncloning repository, generating graph of commits\nsaving data to file, restoring data from file\nrepositories to use: git, linux kernel\ngraph statistics\ndrawing a fragment of graph and whole graph (FELINE, min-post vs levels)\nlevels: histogram and histogram of distribution\nlevels as color in graph drawing\nchecking reachability between two commits\nrev_to_node\ninteractive visualization\ncolorized walk drawing (positive-cut, negative-cut)\nN^2 connectivity on [random sample of] commit graph nodes\nconnectivity on random pairs of nodes, statistics for reachability indexes\nmin-post range size and normalized size histogram\n\ngraph datasets vs commit graphs\n\nReachability queries in large graphs\n\ndatasets from FELINE\ndatasets from FERRARI\ndatasets from PReaCH"
  },
  {
    "objectID": "a.09_git_explore.html",
    "href": "a.09_git_explore.html",
    "title": "Exploring extraction of commit graphs from Git repositories, and examining their shape and stats",
    "section": "",
    "text": "Necessary imports\n# creating graphs in Python\nimport networkx as nx\n# calling git commands\nimport subprocess\n# checking for existence of paths, and manipulating paths\nfrom pathlib import Path"
  },
  {
    "objectID": "a.09_git_explore.html#cloning-git.git-repository-and-extracting-and-saving-the-commit-graph",
    "href": "a.09_git_explore.html#cloning-git.git-repository-and-extracting-and-saving-the-commit-graph",
    "title": "Exploring extraction of commit graphs from Git repositories, and examining their shape and stats",
    "section": "Cloning git.git repository, and extracting and saving the commit graph",
    "text": "Cloning git.git repository, and extracting and saving the commit graph\nWhy it is important to use sparse clone - it is much faster, and takes up less space. For example for git.git repository we have the following (all times are wall time, and subject to interference from other CPU load): - full clone: 54s - mirror: 1min 19s - sparse: 14.9s - 19.4s\nPrepare the directory to store repositories (note: mkdir -d &lt;directory&gt; should show no error if &lt;directory&gt; exists, but it is not portable).\nClone example repository (for exploration). Might give errors if repository already exists.\n\n# create the directory to store cloned repositories, if it does not exist\nPath(\"repos\").mkdir(exist_ok=True)\n\n\n!git -C repos clone --mirror --filter=tree:0 --quiet https://github.com/git/git.git\n\nWall time: 1.05 s\n\n\nfatal: destination path 'git.git' already exists and is not an empty directory.\n\n\nExtract the history of the project\n\n# create the directory to store datasets\nPath(\"datasets\").mkdir(exist_ok=True)\n\n\n!git -C repos/git.git log --format=\"%h %p\" --topo-order --branches &gt;datasets/git-commit_graph.adjlist.txt\n\nWall time: 6.81 s\n\n\nIt takes a few seconds on a laptop with HDD to extract the commit graph (for all branches), storing results in a git-commit_graph.adjlist.txt text file in the adjacent list format in datasets/ subdirectory\n\ngraph_git = nx.read_adjlist(\"datasets/git-commit_graph.adjlist.txt\", create_using=nx.DiGraph)\n\nWall time: 663 ms\n\n\nIt takes less than a second to create NetworkX graph for a commit graph\n\nprint('commit-graph of git repository graph has {} nodes and {} edges'.format(\n      graph_git.number_of_nodes(),\n      graph_git.number_of_edges()))\nprint('is directed:   {}'.format(graph_git.is_directed()))\nprint('is DAG:        {}'.format(nx.is_directed_acyclic_graph(graph_git)))\nprint('example nodes: {}'.format(list(graph_git.nodes)[:5]))\n\ncommit-graph of git repository graph has 63829 nodes and 79664 edges\nis directed:   True\nis DAG:        True\nexample nodes: ['836aadd78', 'a93475d10', '55fce44a3', '6d9d59c31', '084bd2a9a']\n\n\n\n# compress\n\n403 ms ± 32 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nIt takes around 0.6-0.8 seconds to write it back gzip-compressed in the adjacency list format as git-commit_graph.adjlist.txt.gz\n\nTesting different graph output formats\n\n#%%time\n#nx.write_graph6(graph_git, 'datasets/git-commit_graph.g6')\n\nIt turns out that the graph6 / sparse6 format is not a good choice for storing the commit graph for the following reasons: - NetworkX does not support digraph6 format; neither write_graph6 not write_sparse6 are implemented for directed graphs - the digraph6 format is dense graph format, storing the encoded adjacency matrix; there is no disparse6 or digraph6sparse format - SparseGraph6 formats do not store node/vertex names, which might be needed for future computations or visualizations\n\nimport pickle\n\n\nprint('edgelist.txt:            ', end=' ', flush=True)\n\nprint('multiline_adjlist.txt:   ', end=' ', flush=True)\n\nprint('edgelist.txt.gz:         ', end=' ', flush=True)\n\nprint('multiline_adjlist.txt.gz:', end=' ', flush=True)\n\nprint('gexf.gz:                 ', end=' ', flush=True)\n\nprint('graphml.gz:              ', end=' ', flush=True)\n\nprint('pajek.gz:                ', end=' ', flush=True)\n\nedgelist.txt:             Wall time: 244 ms\nmultiline_adjlist.txt:    Wall time: 186 ms\nedgelist.txt.gz:          Wall time: 819 ms\nmultiline_adjlist.txt.gz: Wall time: 1.06 s\ngexf.gz:                  Wall time: 5.34 s\ngraphml.gz:               Wall time: 5.01 s\npajek.gz:                 Wall time: 3.14 s\n\n\n\n#print('gpickle.gz:              ', end=' ', flush=True)\n#%time nx.write_gpickle(graph_git, 'datasets/git-commit_graph.gpickle.gz')  # removed in NetworkX 3.x\n\ngpickle.gz:               \n\n\n\nprint('gpickle.gz:              ', end=' ', flush=True)\nwith open('datasets/git-commit_graph.gpickle.gz', 'wb') as f:\n    pickle.dump(graph_git, f, pickle.HIGHEST_PROTOCOL)\n\ngpickle.gz:               Wall time: 273 ms\n\n\n\nimport pandas as pd\n\n\ngraph_git_df = nx.to_pandas_edgelist(graph_git)\ngraph_git_df\n\n\n\n\n\n\n\n\nsource\ntarget\n\n\n\n\n0\n836aadd78\na93475d10\n\n\n1\n836aadd78\n55fce44a3\n\n\n2\na93475d10\ndf525e622\n\n\n3\n55fce44a3\n6d9d59c31\n\n\n4\n6d9d59c31\n084bd2a9a\n\n\n...\n...\n...\n\n\n79659\n24778e335\n19b2860cb\n\n\n79660\n19b2860cb\nbf0c6e839\n\n\n79661\nbf0c6e839\ne497ea2a9\n\n\n79662\ne497ea2a9\n8bc9a0c76\n\n\n79663\n8bc9a0c76\ne83c51633\n\n\n\n\n79664 rows × 2 columns\n\n\n\n\ngraph_git_df.to_pickle('datasets/git-commit_graph.df_edgelist.pickle.gz')\n\nWall time: 1.07 s\n\n\n\ngraph_git_df.to_csv('datasets/git-commit_graph.df_edgelist.csv.gz')\n\nWall time: 1.04 s\n\n\nBoth to_feather() and to_parquet() need pyarrow module installed (the latter might use fastparquet instead)\n\ntry:\n    graph_git_df.to_feather('datasets/git-commit_graph.df_edgelist.feather')\nexcept ImportError:\n    print(\"Missing optional dependency 'pyarrow' required to save dataframe to the feather format\")\n\nWall time: 587 ms\n\n\n\ntry:\n    graph_git_df.to_parquet('datasets/git-commit_graph.df_edgelist.parquet')\nexcept ImportError:\n    print(\"Missing optional dependency 'pyarrow' or 'fastparquet' required to save dataframe to the parquet format\")\n\nWall time: 1.18 s\n\n\nto_hdf() needs tables module installed (PyTables).\n\ntry:\n    graph_git_df.to_hdf('datasets/git-commit_graph.df_edgelist.hdf5', 'df_edgelist', mode='w', complevel=6)\nexcept ImportError:\n    print(\"Missing optional dependency 'table' required to save dataframe to the HDF5 format\")\n\nWall time: 2.6 s\n\n\n\n\nComparing the filesize of different graph output formats\nPreviously !dir datasets was used, but - it is not portable (on Linux it only lists files, instead of showing among others file size information) - on MS Windows where it works for this purpose, it includes unnecessary information\n\n[\"{name:&lt;50} {size:&gt;7}\".format(name=p.name,size=p.stat().st_size) for p in Path(\"datasets\").glob(\"git*.gz\")]\n\n['git-commit_graph-df_reachability_sample.csv.gz      338527',\n 'git-commit_graph.adjlist.txt.gz                     581624',\n 'git-commit_graph.df_edgelist.csv.gz                 808989',\n 'git-commit_graph.df_edgelist.pickle.gz              668474',\n 'git-commit_graph.df_nodedata.csv.gz                 773957',\n 'git-commit_graph.edgelist.txt.gz                    606109',\n 'git-commit_graph.gexf.gz                           1524987',\n 'git-commit_graph.gpickle.gz                        3851388',\n 'git-commit_graph.graphml.gz                        1109122',\n 'git-commit_graph.info.csv.gz                       1785780',\n 'git-commit_graph.info.pickle.gz                    1467752',\n 'git-commit_graph.multiline_adjlist.txt.gz           638575',\n 'git-commit_graph.pajek.gz                           876408']\n\n\n\n[\"{name:&lt;50} {size:&gt;7}\".format(name=p.name,size=p.stat().st_size) for p in Path(\"datasets\").glob(\"git*\") if not p.match(\"*.gz\")]\n\n['git-commit_graph.adjlist.txt                       1434939',\n 'git-commit_graph.df_edgelist.csv                   4383858',\n 'git-commit_graph.df_edgelist.feather               1918282',\n 'git-commit_graph.df_edgelist.hdf5                  2421869',\n 'git-commit_graph.df_edgelist.parquet               1617488',\n 'git-commit_graph.edgelist.txt                      1832272',\n 'git-commit_graph.hdf5                              3803473',\n 'git-commit_graph.info.hdf5                         1382956',\n 'git-commit_graph.info.parquet                      3164039',\n 'git-commit_graph.multiline_adjlist.txt             1801780']\n\n\n\n\n\n\n\n\nNote\n\n\n\nthe information below was gathered for git log --all, not git log --branches as it is done now, so the exact sizes will be different\n\n\n\nimport io\n\ncsv = u\"\"\"filename;size [bytes];size uncompressed [bytes];time [s]\ngit-commit_graph.adjlist.txt;2875179;2875179\ngit-commit_graph.adjlist.txt.gz;1173712;2875369;0.910\ngit-commit_graph.multiline_adjlist.txt;3610938;3610938;0.456\ngit-commit_graph.edgelist.txt;3692282;3692282;0.624\ngit-commit_graph.gpickle.gz;3254690;7712162;4.92\ngit-commit_graph.gexf.gz;3063783;16419236;9.49\ngit-commit_graph.graphml.gz;2231397;10305482;4.74\ngit-commit_graph.pajek.gz;1754236;6682509;6.74\ngit-commit_graph.df_edgelist.pickle.gz;1332691;2736191;1.9\ngit-commit_graph.df_edgelist.feather;3857818;3857818;0.487\ngit-commit_graph.df_edgelist.parquet;3116020;3116020;0.333\ngit-commit_graph.df_edgelist.csv;4383858;4383858\ngit-commit_graph.df_edgelist.csv.gz;1629263;4383858;1.2\ngit-commit_graph.df_edgelist.hdf5;3808521;5077784;0.244\n\"\"\"\n\ncsv_stream = io.StringIO(csv)\ntable_sizes=pd.read_csv(csv_stream,sep=';') # ,index_col=0\ntable_sizes\n\n\n\n\n\n\n\n\nfilename\nsize [bytes]\nsize uncompressed [bytes]\ntime [s]\n\n\n\n\n0\ngit-commit_graph.adjlist.txt\n2875179\n2875179\nNaN\n\n\n1\ngit-commit_graph.adjlist.txt.gz\n1173712\n2875369\n0.910\n\n\n2\ngit-commit_graph.multiline_adjlist.txt\n3610938\n3610938\n0.456\n\n\n3\ngit-commit_graph.edgelist.txt\n3692282\n3692282\n0.624\n\n\n4\ngit-commit_graph.gpickle.gz\n3254690\n7712162\n4.920\n\n\n5\ngit-commit_graph.gexf.gz\n3063783\n16419236\n9.490\n\n\n6\ngit-commit_graph.graphml.gz\n2231397\n10305482\n4.740\n\n\n7\ngit-commit_graph.pajek.gz\n1754236\n6682509\n6.740\n\n\n8\ngit-commit_graph.df_edgelist.pickle.gz\n1332691\n2736191\n1.900\n\n\n9\ngit-commit_graph.df_edgelist.feather\n3857818\n3857818\n0.487\n\n\n10\ngit-commit_graph.df_edgelist.parquet\n3116020\n3116020\n0.333\n\n\n11\ngit-commit_graph.df_edgelist.csv\n4383858\n4383858\nNaN\n\n\n12\ngit-commit_graph.df_edgelist.csv.gz\n1629263\n4383858\n1.200\n\n\n13\ngit-commit_graph.df_edgelist.hdf5\n3808521\n5077784\n0.244\n\n\n\n\n\n\n\n\ntable_sizes.dtypes\n\nfilename                      object\nsize [bytes]                   int64\nsize uncompressed [bytes]      int64\ntime [s]                     float64\ndtype: object\n\n\n\ntable_sizes.sort_values(by='size [bytes]')\n\n\n\n\n\n\n\n\nfilename\nsize [bytes]\nsize uncompressed [bytes]\ntime [s]\n\n\n\n\n1\ngit-commit_graph.adjlist.txt.gz\n1173712\n2875369\n0.910\n\n\n8\ngit-commit_graph.df_edgelist.pickle.gz\n1332691\n2736191\n1.900\n\n\n12\ngit-commit_graph.df_edgelist.csv.gz\n1629263\n4383858\n1.200\n\n\n7\ngit-commit_graph.pajek.gz\n1754236\n6682509\n6.740\n\n\n6\ngit-commit_graph.graphml.gz\n2231397\n10305482\n4.740\n\n\n0\ngit-commit_graph.adjlist.txt\n2875179\n2875179\nNaN\n\n\n5\ngit-commit_graph.gexf.gz\n3063783\n16419236\n9.490\n\n\n10\ngit-commit_graph.df_edgelist.parquet\n3116020\n3116020\n0.333\n\n\n4\ngit-commit_graph.gpickle.gz\n3254690\n7712162\n4.920\n\n\n2\ngit-commit_graph.multiline_adjlist.txt\n3610938\n3610938\n0.456\n\n\n3\ngit-commit_graph.edgelist.txt\n3692282\n3692282\n0.624\n\n\n13\ngit-commit_graph.df_edgelist.hdf5\n3808521\n5077784\n0.244\n\n\n9\ngit-commit_graph.df_edgelist.feather\n3857818\n3857818\n0.487\n\n\n11\ngit-commit_graph.df_edgelist.csv\n4383858\n4383858\nNaN\n\n\n\n\n\n\n\n\ntable_sizes.sort_values(by='size uncompressed [bytes]')\n\n\n\n\n\n\n\n\nfilename\nsize [bytes]\nsize uncompressed [bytes]\ntime [s]\n\n\n\n\n8\ngit-commit_graph.df_edgelist.pickle.gz\n1332691\n2736191\n1.900\n\n\n0\ngit-commit_graph.adjlist.txt\n2875179\n2875179\nNaN\n\n\n1\ngit-commit_graph.adjlist.txt.gz\n1173712\n2875369\n0.910\n\n\n10\ngit-commit_graph.df_edgelist.parquet\n3116020\n3116020\n0.333\n\n\n2\ngit-commit_graph.multiline_adjlist.txt\n3610938\n3610938\n0.456\n\n\n3\ngit-commit_graph.edgelist.txt\n3692282\n3692282\n0.624\n\n\n9\ngit-commit_graph.df_edgelist.feather\n3857818\n3857818\n0.487\n\n\n11\ngit-commit_graph.df_edgelist.csv\n4383858\n4383858\nNaN\n\n\n12\ngit-commit_graph.df_edgelist.csv.gz\n1629263\n4383858\n1.200\n\n\n13\ngit-commit_graph.df_edgelist.hdf5\n3808521\n5077784\n0.244\n\n\n7\ngit-commit_graph.pajek.gz\n1754236\n6682509\n6.740\n\n\n4\ngit-commit_graph.gpickle.gz\n3254690\n7712162\n4.920\n\n\n6\ngit-commit_graph.graphml.gz\n2231397\n10305482\n4.740\n\n\n5\ngit-commit_graph.gexf.gz\n3063783\n16419236\n9.490\n\n\n\n\n\n\n\n\n\nSummary of findings\nStoring DataFrame of edgelist data\nThe DataFrame is created using nx.to_pandas_edgelist(graph)\nThe smallest file is the result of storing DataFrame of edgelist data as gzipped pickle (written using Pandas’ to_pickle()) - the *.df_edgelist.pickle.gz file.\n\nAdvantages:\n\n2nd smallest size\nno extra modules to install\npreserves types\n\nDisadvantages:\n\nPython-specific\nunsafe\nslow (?)\n\n\nNext smallest is, surprisingly, gzipped CSV representing DataFrame of edgelist data (written using Pandas’ to_csv()) - the *.df_edgelist.csv.gz file.\n\nAdvantages:\n\n3rd smallest size\nno extra modules to install\nuniversal format\n\nDisadvantages:\n\nslow (?)\n\n\nThe fast interchange formats, Feather and Parquet turned out to produce quite large files; they are however 4th smallest and 2nd smallest, respectively, among uncompressed formats.\nFeather provides a binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy.\nFeather is designed to faithfully serialize and de-serialize DataFrames, supporting all of the pandas dtypes, including extension dtypes such as categorical and datetime with tz.\nApache Parquet provides a partitioned binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy. Parquet can use a variety of compression techniques (default is to use ‘snappy’) to shrink the file size as much as possible while still maintaining good read performance.\nParquet is designed to faithfully serialize and de-serialize DataFrames, supporting all of the pandas dtypes, including extension dtypes such as datetime with tz.\n\nAdvantages:\n\nuniversal, cross language\npreserves types\nfast\nsupport for CUDA\n\nDisadvantages:\n\nrequires pyarrow package to be installed\nquite large file size\n\n\nThe HDF5 format has the advantage of being able to store multiple DataFrames in a single file, for example one DataFrame to hold edgelist data to define the graph connections, and one DataFrame holding various per vertex (per node) rechablity index data. Unfortunately, even internally compressed it has one of larger file sizes (and largest compressed).\nHierarchical Data Format (HDF) is self-describing, allowing an application to interpret the structure and contents of a file with no outside information. One HDF file can hold a mix of related objects which can be accessed as a group or as individual objects.\nThe to_hdf() method in Pandas uses the HDFStore in the background, which in turn utilizes the PyTables library.\n\nAdvantages:\n\nuniversal, cross language\npreserves types\ncan store multiple DataFrames\n\nDisadvantages:\n\nlarge file size\nrequires tables package to be installed (PyTables)\nslow (?)\nunsafe (because of serializing object-dtype data with pickle)\n\n\n\nStoring NetworkX DiGraph directly\nExcept for the adjacency lists format (*.adjlist.txt), which cannot store any additional data (not even per-edge), the multi-line adjacency lists (*.multiline_adjlist.txt) and edge lists (*.edgelist.txt) formats have file sizes larger than the Parquet storing DataFrame of edge lists. Note however that they are uncompressed.\nThe adjacency list format can be created directly by appropriate Git command.\nBecause those formats cannot store per-node data, they are not considered for use.\nAmong specialized graph file formats, which I assume can store both per-node and per-edge data, Pajek format is smallest (compressed), and 4th smallest among all considered storage formats. It is also smallest uncompressed among similar file formats.\nWriting NetworkX graphs as Python pickles using write_gpickle() (*.gpickle.gz) results in quite large file compressed, one of the largest files uncompressed.\nOn the other hand NetworkX graphs can contain any hashable Python object as node (not just integers and strings). For arbitrary data types it may be difficult to represent the data as text. In that case using Python pickles to store the graph data can be used."
  },
  {
    "objectID": "a.09_git_explore.html#examining-computing-and-storing-reachability-labels-and-other-per-node-data",
    "href": "a.09_git_explore.html#examining-computing-and-storing-reachability-labels-and-other-per-node-data",
    "title": "Exploring extraction of commit graphs from Git repositories, and examining their shape and stats",
    "section": "Examining computing and storing reachability labels and other per-node data",
    "text": "Examining computing and storing reachability labels and other per-node data\n\nfrom git_commit_graph_ext.labelling.levels import *\nfrom git_commit_graph_ext.labelling.dfs_intervals import *\n\n\nimport numpy as np\n\n\nComputing reachability labels for git.git commit graph\n\nprint('git.git commit graph')\ngraph = graph_git\n\nprint('graph has {} nodes and {} edges'.format(graph.number_of_nodes(), graph.number_of_edges()))\n\nprint('\\nfind levels:')\n\nprint('\\nfind min-post intervals:')\n\nprint('\\nfind min-post intervals (graph+tree):')\n\ngit.git commit graph\ngraph has 63829 nodes and 79664 edges\n\nfind levels:\nWall time: 669 ms\n\nfind min-post intervals:\nWall time: 1.93 s\n\nfind min-post intervals (graph+tree):\nWall time: 1.68 s\n\n\nAll times to calculate different graph indices for git.git commit graph take around few second.\n\n# calculating parameters of the git.git commit graphs: distribution of in-degree, out-degree and level\nprint('git.git commit graph')\ngraph = graph_git\n\ngraph_info_df=pd.DataFrame.from_dict(graph.lvl,orient='index').rename(columns={0: 'level'})\ngraph_info_df.index.name='node'\ngraph_info_df['in degree']=pd.Series(dict(graph.in_degree()))\ngraph_info_df['out degree']=pd.Series(dict(graph.out_degree()))\ngraph_info_df['degree']=graph_info_df['in degree']+graph_info_df['out degree']\n\ngraph_info_df = pd.concat([graph_info_df,\n                           pd.DataFrame.from_dict(graph.mpi_ext,orient='index',columns=['f_min','min','post'])],\n                          axis=1,join='inner')\n\ngraph_info_df\n\ngit.git commit graph\n\n\n\n\n\n\n\n\n\nlevel\nin degree\nout degree\ndegree\nf_min\nmin\npost\n\n\n\n\ne83c51633\n0\n1\n0\n1\n1\n1\n1\n\n\n8bc9a0c76\n1\n1\n1\n2\n1\n1\n2\n\n\ne497ea2a9\n2\n1\n1\n2\n1\n1\n3\n\n\nbf0c6e839\n3\n1\n1\n2\n1\n1\n4\n\n\n19b2860cb\n4\n1\n1\n2\n1\n1\n5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\ne6d102643\n21094\n1\n2\n3\n1\n63662\n63825\n\n\n4c6c5054a\n21095\n1\n2\n3\n1\n63662\n63826\n\n\nf29945cb3\n21096\n1\n2\n3\n1\n63662\n63827\n\n\nb16af9417\n21097\n1\n2\n3\n1\n63662\n63828\n\n\n45daf8777\n21098\n0\n2\n2\n1\n63662\n63829\n\n\n\n\n63829 rows × 7 columns\n\n\n\n\ngraph_info_df.dtypes\n\nlevel         int64\nin degree     int64\nout degree    int64\ndegree        int64\nf_min         int64\nmin           int64\npost          int64\ndtype: object\n\n\n\ngraph_info_df.columns\n\nIndex(['level', 'in degree', 'out degree', 'degree', 'f_min', 'min', 'post'], dtype='object')\n\n\n\ngraph_info_df.describe()\n\n\n\n\n\n\n\n\nlevel\nin degree\nout degree\ndegree\nf_min\nmin\npost\n\n\n\n\ncount\n63829.000000\n63829.000000\n63829.000000\n63829.000000\n63829.000000\n63829.000000\n63829.000000\n\n\nmean\n11774.655439\n1.248085\n1.248085\n2.496169\n2122.979116\n23913.162168\n31915.000000\n\n\nstd\n6198.793361\n1.556817\n0.436128\n1.602244\n10631.734068\n21750.776061\n18425.989503\n\n\nmin\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n25%\n7003.000000\n1.000000\n1.000000\n2.000000\n1.000000\n1.000000\n15958.000000\n\n\n50%\n12695.000000\n1.000000\n1.000000\n2.000000\n1.000000\n21878.000000\n31915.000000\n\n\n75%\n17120.000000\n1.000000\n1.000000\n3.000000\n1.000000\n43554.000000\n47872.000000\n\n\nmax\n21136.000000\n110.000000\n10.000000\n111.000000\n62308.000000\n63662.000000\n63829.000000\n\n\n\n\n\n\n\nThe git.git repository (state for December 2020) has, in all its branches - around 64k commits - maximum level of around 21k (around 1/3 of number of commits) - mean out-degree of 1.25\n\n\nVisualizing topological level stats for git.git commit graph\nImports for plots\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nsns.set()\n\nHistogram for backward topological levels\n\nplt.title('Histogram of levels')\nplt.xlabel('Level')\ngraph_info_df['level'].plot.hist(bins=50)\nplt.draw()\n\n\n\n\nPlot of counts for backward topological levels\n\nlevel_counts=graph_info_df['level'].value_counts()\n\nlevel_counts_df=pd.DataFrame({'level counts': level_counts})\nlevel_counts_df.index.name='level'\nlevel_counts_df.sort_index(inplace=True)\nlevel_counts_df.plot()\n\nplt.title('%s commit graph - number of nodes with given level' %\n          'git.git')\nplt.xlabel('level of node / vertex')\nplt.ylabel('number of nodes / vertices')\nplt.show()\n\n\n\n\nHistogram of counts for backward topological levels\n\nlevel_counts=graph_info_df['level'].value_counts()\n\nplt.title('Histogram of level counts\\n'+'Number of levels with given count of nodes')\nplt.xlabel('Level value count\\n'+'number of commits with given level')\nlevel_counts.plot.hist(bins=50)\n\nlevel_counts.describe()\n\ncount    21137.000000\nmean         3.019776\nstd          3.477064\nmin          1.000000\n25%          1.000000\n50%          2.000000\n75%          4.000000\nmax        112.000000\nName: level, dtype: float64\n\n\n\n\n\nSanity checking the computed level counts; nodes with backward topological level of zero are sink nodes, with out-degree of 0.\nThus the number of nodes with level of 0 should be equal to number of nodes with out-degree of 0.\n\n# sanity checking level counts\n\nprint('git.git commit graph')\ngraph = graph_git\n\nsinks  =[n for n in graph if graph.out_degree(n) == 0]\nsources=[n for n in graph if graph.in_degree(n)  == 0]\nprint('there are {:3d} nodes with  in-degree of 0'.format(len(sources)))\nprint('there are {:3d} nodes with out-degree of 0: {}'.format(len(sinks),sinks))\nprint('there are {:3d} nodes with level of 0'.format(level_counts.at[0]))\n\ngraph_info_df[graph_info_df['level']==0]\n\ngit.git commit graph\nthere are   3 nodes with  in-degree of 0\nthere are   9 nodes with out-degree of 0: ['7d77f2e9c', '1bd90415d', '0ca71b373', '16d6b8ab6', 'cb07fc2a2', '161332a52', '2744b2344', '1db95b00a', 'e83c51633']\nthere are   9 nodes with level of 0\n\n\n\n\n\n\n\n\n\nlevel\nin degree\nout degree\ndegree\nf_min\nmin\npost\n\n\n\n\ne83c51633\n0\n1\n0\n1\n1\n1\n1\n\n\n1db95b00a\n0\n1\n0\n1\n799\n799\n799\n\n\n2744b2344\n0\n1\n0\n1\n1149\n1149\n1149\n\n\n161332a52\n0\n1\n0\n1\n5044\n5044\n5044\n\n\ncb07fc2a2\n0\n1\n0\n1\n8137\n8137\n8137\n\n\n16d6b8ab6\n0\n1\n0\n1\n10300\n10300\n10300\n\n\n0ca71b373\n0\n1\n0\n1\n28384\n28384\n28384\n\n\n1bd90415d\n0\n1\n0\n1\n61749\n61749\n61749\n\n\n7d77f2e9c\n0\n1\n0\n1\n62308\n62308\n62308\n\n\n\n\n\n\n\n\n\nVisualizing in-degree and out-degree in git.git commit graph\n\ngraph_info_df[['in degree','out degree']].hist(bins=10)\nplt.show()\n\n\n\n\n\ngraph_info_df.hist(column=['in degree','out degree'],bins=10,range=(0,10))\nplt.show()\n\n\n\n\n\ngraph_info_df[['in degree','out degree']].plot.hist(bins=51,alpha=0.4,range=(0,50))\nplt.show()\n\n\n\n\nFor a graph that follows the power law (for example small-world or scale-free graphs), the distribution of degrees should be a straight line on log-log scale.\nFor example the plot below (taken from the KONECT handbook, Figure 7a) shows the degree distribution and cumulative degree distribution for the Wikipedia election network (EL).\n\n\n\nThe degree distribution and cumulative degree distribution for the Wikipedia election network (EL)\n\n\nBelow there is the same graph for the commit graph of the git.git repository\n\ndegree_count_df=graph_info_df['degree'].value_counts(sort=False)\\\n                    .to_frame(name='Frequency').rename_axis('degree')\\\n                    .sort_index()\ndegree_count_df['Degree (d)'] = degree_count_df.index\n\ndegree_count_df.plot(kind='scatter',x='Degree (d)',y='Frequency',color='#0000ff',\n                     figsize=(8,7),logx=True,logy=True,\n                     title='git.git commit graph')\nsns.set_style('ticks', {\"xtick.major.size\": 20, \"ytick.major.size\": 20})\nplt.xlim([.8, 1e4])\nplt.ylim([1., 7e4])\nplt.show()\n#degree_count_df\n\n\n\n\nNote that degrees span the range up to around \\(10^2\\) in the case of git.git commit graph, not \\(10^4\\) of the Wikipedia elections network. Note also that degree of 1 is an outlier; it does not lie on the line through other points.\nBelow there is in/outdegree comparison plot for the git.git commit graph\n\ngraph_info_df\n\n\n\n\n\n\n\n\nlevel\nin degree\nout degree\ndegree\nf_min\nmin\npost\n\n\n\n\ne83c51633\n0\n1\n0\n1\n1\n1\n1\n\n\n8bc9a0c76\n1\n1\n1\n2\n1\n1\n2\n\n\ne497ea2a9\n2\n1\n1\n2\n1\n1\n3\n\n\nbf0c6e839\n3\n1\n1\n2\n1\n1\n4\n\n\n19b2860cb\n4\n1\n1\n2\n1\n1\n5\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\ne6d102643\n21094\n1\n2\n3\n1\n63662\n63825\n\n\n4c6c5054a\n21095\n1\n2\n3\n1\n63662\n63826\n\n\nf29945cb3\n21096\n1\n2\n3\n1\n63662\n63827\n\n\nb16af9417\n21097\n1\n2\n3\n1\n63662\n63828\n\n\n45daf8777\n21098\n0\n2\n2\n1\n63662\n63829\n\n\n\n\n63829 rows × 7 columns\n\n\n\n\ngrid=sns.relplot(data=graph_info_df,x='out degree',y='in degree',\n                 kind='scatter',color='#0000ff')\nax=grid.axes[0][0]\nax.set_title('git.git commit graph')\n#ax.set_xscale('log')\n#ax.set_yscale('log')\nplt.show()\n\n\n\n\n\n\nVisualizing min-post intervals in git.git commit graphs\n\ngraph_info_df.hist(column=['f_min','min'],bins=20)\ngraph_info_df[['f_min','min']].plot.hist(bins=50,alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nNormalized intervals (interval divide by post-visit order number)\n\ngraph_info_df['|interval|']=graph_info_df['post'] - graph_info_df['min']\ngraph_info_df['|interval|/post']=graph_info_df['|interval|']/graph_info_df['post']\ngraph_info_df['|f_interval|']=graph_info_df['post'] - graph_info_df['f_min']\ngraph_info_df['|f_interval|/post']=graph_info_df['|f_interval|']/graph_info_df['post']\ngraph_info_df\n\n\n\n\n\n\n\n\nlevel\nin degree\nout degree\ndegree\nf_min\nmin\npost\n|interval|\n|interval|/post\n|f_interval|\n|f_interval|/post\n\n\n\n\ne83c51633\n0\n1\n0\n1\n1\n1\n1\n0\n0.000000\n0\n0.000000\n\n\n8bc9a0c76\n1\n1\n1\n2\n1\n1\n2\n1\n0.500000\n1\n0.500000\n\n\ne497ea2a9\n2\n1\n1\n2\n1\n1\n3\n2\n0.666667\n2\n0.666667\n\n\nbf0c6e839\n3\n1\n1\n2\n1\n1\n4\n3\n0.750000\n3\n0.750000\n\n\n19b2860cb\n4\n1\n1\n2\n1\n1\n5\n4\n0.800000\n4\n0.800000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\ne6d102643\n21094\n1\n2\n3\n1\n63662\n63825\n163\n0.002554\n63824\n0.999984\n\n\n4c6c5054a\n21095\n1\n2\n3\n1\n63662\n63826\n164\n0.002569\n63825\n0.999984\n\n\nf29945cb3\n21096\n1\n2\n3\n1\n63662\n63827\n165\n0.002585\n63826\n0.999984\n\n\nb16af9417\n21097\n1\n2\n3\n1\n63662\n63828\n166\n0.002601\n63827\n0.999984\n\n\n45daf8777\n21098\n0\n2\n2\n1\n63662\n63829\n167\n0.002616\n63828\n0.999984\n\n\n\n\n63829 rows × 11 columns\n\n\n\n\ngraph_info_df.describe()\n\n\n\n\n\n\n\n\nlevel\nin degree\nout degree\ndegree\nf_min\nmin\npost\n|interval|\n|interval|/post\n|f_interval|\n|f_interval|/post\n\n\n\n\ncount\n63829.000000\n63829.000000\n63829.000000\n63829.000000\n63829.000000\n63829.000000\n63829.000000\n63829.000000\n63829.000000\n63829.000000\n63829.000000\n\n\nmean\n11774.655439\n1.248085\n1.248085\n2.496169\n2122.979116\n23913.162168\n31915.000000\n8001.837832\n0.303827\n29792.020884\n0.947877\n\n\nstd\n6198.793361\n1.556817\n0.436128\n1.602244\n10631.734068\n21750.776061\n18425.989503\n15664.880755\n0.458586\n18603.735072\n0.212082\n\n\nmin\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n7003.000000\n1.000000\n1.000000\n2.000000\n1.000000\n1.000000\n15958.000000\n1.000000\n0.000037\n13660.000000\n0.999923\n\n\n50%\n12695.000000\n1.000000\n1.000000\n2.000000\n1.000000\n21878.000000\n31915.000000\n7.000000\n0.000282\n29954.000000\n0.999966\n\n\n75%\n17120.000000\n1.000000\n1.000000\n3.000000\n1.000000\n43554.000000\n47872.000000\n7365.000000\n0.999864\n45942.000000\n0.999978\n\n\nmax\n21136.000000\n110.000000\n10.000000\n111.000000\n62308.000000\n63662.000000\n63829.000000\n61747.000000\n0.999984\n63828.000000\n0.999984\n\n\n\n\n\n\n\n\ngraph_info_df.hist(column=['min'],bins=50,color='m')\ngraph_info_df.hist(column=['min'],bins=50,log=True,color='m')\ngraph_info_df.hist(column=['f_min'],bins=50,color='#800080')\ngraph_info_df.hist(column=['f_min'],bins=50,log=True,color='#800080')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngraph_info_df.hist(column=['|interval|'],bins=50,log=True)\ngraph_info_df.hist(column=['|interval|/post'],bins=50,log=True)\nplt.show()\n\n\n\n\n\n\n\n\ngraph_info_df['|interval|/post'].plot.kde(bw_method=0.05,ind=np.linspace(0,1,200))\nplt.title('kernel density estimator, bw=0.05, logscale')\nplt.xlabel('|interval|/post')\nplt.gca().set_yscale(\"log\", nonpositive='clip')\nplt.show()\n\n\n\n\n\ngraph_info_df.hist(column=['|interval|/post'],bins=40,log=True)\ngraph_info_df['|interval|/post'].plot.kde(ind=np.linspace(0,1,500),color='k')\nplt.title('histogram of relative size of the min-post tree interval\\n'+'kernel density estimator, logscale')\nplt.xlabel('|interval|/post')\nplt.gca().set_yscale(\"log\", nonpositive='clip')\nplt.show()\n\n\n\n\n\ngraph_info_df.hist(column=['|f_interval|'],bins=50,color='g')\ngraph_info_df.hist(column=['|f_interval|/post'],bins=50,color='c')\ngraph_info_df.hist(column=['|f_interval|/post'],bins=50,log=True,color='c')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nStoring computed reachability labels and related stats\n\nfrom shutil import copyfile\ncopyfile('datasets/git-commit_graph.df_edgelist.hdf5', 'datasets/git-commit_graph.hdf5')\n\n'datasets/git-commit_graph.hdf5'\n\n\n\nprint('pickle.gz: ', end=' ', flush=True)\n\nprint('csv.gz:    ', end=' ', flush=True)\n\nprint('hdf (full):', end=' ', flush=True)\n\nprint('hdf (appd):', end=' ', flush=True)\n\npickle.gz:  Wall time: 3.8 s\ncsv.gz:     Wall time: 2.11 s\nhdf (full): Wall time: 316 ms\nhdf (appd): Wall time: 321 ms\n\n\nSaving to pickle.gz and csv.gz file format takes few seconds, saving to HDF file format takes less than 0.5s\n\n# feather does not support non-numerical index, IIRC\n#%time graph_info_df.to_feather('datasets/git-commit_graph.info.feather')\n\n\ntry:\n    graph_info_df.to_parquet('datasets/git-commit_graph.info.parquet')\nexcept ImportError:\n    print(\"Missing optional dependency 'pyarrow' required\")\n\nWall time: 468 ms\n\n\n\n[\"{name:&lt;50} {size:&gt;7}\".format(name=p.name,size=p.stat().st_size) for p in Path(\"datasets\").glob(\"*.info.*\")]\n\n['git-commit_graph.info.csv.gz                       1785780',\n 'git-commit_graph.info.hdf5                         1382956',\n 'git-commit_graph.info.parquet                      3164039',\n 'git-commit_graph.info.pickle.gz                    1465333']\n\n\n\n[\"{name:&lt;50} {size:&gt;7}\".format(name=p.name,size=p.stat().st_size) for p in Path(\"datasets\").glob(\"git-commit_graph.hdf5\")]\n\n['git-commit_graph.hdf5                              3803473']\n\n\nNote that the sizes below are for git log --all, not current git log --branches, but the relative sizes should be similar\n\ncsv = u\"\"\"filename;size [bytes];time [s]\ndatasets/git-commit_graph.info.pickle.gz;2833650;6.8\ndatasets/git-commit_graph.info.csv.gz;3681511;5.26\ndatasets/git-commit_graph.info.parquet;6079159;0.345 \ndatasets/git-commit_graph.info.hdf5;2735986;0.674\ndatasets/git-commit_graph.hdf5;6548777;0.672\n\"\"\"\n\ncsv_stream = io.StringIO(csv)\ntable_sizes_2=pd.read_csv(csv_stream,sep=';') # ,index_col=0\ntable_sizes_2\n\n\n\n\n\n\n\n\nfilename\nsize [bytes]\ntime [s]\n\n\n\n\n0\ndatasets/git-commit_graph.info.pickle.gz\n2833650\n6.800\n\n\n1\ndatasets/git-commit_graph.info.csv.gz\n3681511\n5.260\n\n\n2\ndatasets/git-commit_graph.info.parquet\n6079159\n0.345\n\n\n3\ndatasets/git-commit_graph.info.hdf5\n2735986\n0.674\n\n\n4\ndatasets/git-commit_graph.hdf5\n6548777\n0.672\n\n\n\n\n\n\n\n\ntable_sizes_2.sort_values(by='size [bytes]')\n\n\n\n\n\n\n\n\nfilename\nsize [bytes]\ntime [s]\n\n\n\n\n3\ndatasets/git-commit_graph.info.hdf5\n2735986\n0.674\n\n\n0\ndatasets/git-commit_graph.info.pickle.gz\n2833650\n6.800\n\n\n1\ndatasets/git-commit_graph.info.csv.gz\n3681511\n5.260\n\n\n2\ndatasets/git-commit_graph.info.parquet\n6079159\n0.345\n\n\n4\ndatasets/git-commit_graph.hdf5\n6548777\n0.672\n\n\n\n\n\n\n\n\ntable_sizes_2.sort_values(by='time [s]')\n\n\n\n\n\n\n\n\nfilename\nsize [bytes]\ntime [s]\n\n\n\n\n2\ndatasets/git-commit_graph.info.parquet\n6079159\n0.345\n\n\n4\ndatasets/git-commit_graph.hdf5\n6548777\n0.672\n\n\n3\ndatasets/git-commit_graph.info.hdf5\n2735986\n0.674\n\n\n1\ndatasets/git-commit_graph.info.csv.gz\n3681511\n5.260\n\n\n0\ndatasets/git-commit_graph.info.pickle.gz\n2833650\n6.800\n\n\n\n\n\n\n\n\n\nSummary of findings\nStrangely,smallest files size this time is given when storing using the HDF5 (with compresslevel=6) format, which is also 2nd fastest.\nSaving DataFrame using the Parquet format is fastest, but this time it gives the largest size of the file (except for git-commit_graph.hdf5, which stores both commit graph in edge list format, and per-node information)\nSaving as *.pickle.gz or *.csv.gz is slowest and second slowest, while both being in the middlle with respect to file sizes (the pickle-based format gives smaller file size)."
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Graph datasets",
    "section": "",
    "text": "Imports SciPy modules and matplotlib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nOther imports\nimport io"
  },
  {
    "objectID": "datasets.html#datasets-from-feline-table_fel",
    "href": "datasets.html#datasets-from-feline-table_fel",
    "title": "Graph datasets",
    "section": "Datasets from FELINE (table_fel)",
    "text": "Datasets from FELINE (table_fel)\n“Reachability Queries in Very Large Graphs: A Fast Refined Online Search Approach” (2014)\nhttp://openprocedings.org/EDBT/2014/paper_166.pdf\n\n\n\nTable 1: Datasets\n\n\n\nvertices - \\(|V|\\), number of vertices / nodes in the graph\nedges - \\(|E|\\), number of edges in the graph\nCluster-coeff - clustering coefficient\nEff-diameter - effective diameter (or effective eccentricity) is an estimated size of the path in which 90% of all pairs of vertices connected are reachable from each other\nroots - number of roots, nodes with no incoming edges, vertices with no predecessors (sources)\nleafs - number of leafs, nodes with no outgoing edges (sinks)\n\nThe FELINE paper authors used the SNAP software &lt;snap.stanford.edu/snap/&gt; to compute these values\n\n#@title Table 1: Datasets from FELINE (table_fel) { form-width: \"25%\" }\n\ncsv=u\"\"\"Graph;vertices;edges;Cluster coeff.;Eff. diameter;roots;leafs\narXiv;6000;66707;0.35;5.48;961;624\nYago;6642;42392;0.24;6.57;5176;263\nGo;6793;13361;0.07;10.92;64;3087\nPubmed;9000;40028;0.10;6.32;2609;4702\nciteseer;10720;44258;0.28;8.36;4572;1868\nUniprot22m;1595444;1595442;0.00;3.3;1556157;1\nCit-patents;3774768;16518047;0.09;10.5;515785;1685423\nciteseerx;6540401;15011260;0.06;8.4;567149;5740710\nGo-uniprot;6967956;34770235;0.00;4.8;6945721;4\nUniprot100m;16087295;16087293;0.00;4.1;14598959;1\nUniprot150m;25037600;25037598;0.00;4.4;21650056;1\n\"\"\"\ncsv_stream = io.StringIO(csv)\n\ntable_fel=pd.read_csv(csv_stream,sep=';',index_col=0)\ntable_fel.insert(loc=2,column='edges[% V^2]',\n               value=100.0*(table_fel['edges']/(table_fel['vertices']**2)))\ntable_fel.insert(loc=3,column='edges/vertices',\n               value=table_fel['edges']/table_fel['vertices'])\ntable_fel\n\n\n\n\n\n\n\n\nvertices\nedges\nedges[% V^2]\nedges/vertices\nCluster coeff.\nEff. diameter\nroots\nleafs\n\n\nGraph\n\n\n\n\n\n\n\n\n\n\n\n\narXiv\n6000\n66707\n0.185297\n11.117833\n0.35\n5.48\n961\n624\n\n\nYago\n6642\n42392\n0.096092\n6.382415\n0.24\n6.57\n5176\n263\n\n\nGo\n6793\n13361\n0.028954\n1.966878\n0.07\n10.92\n64\n3087\n\n\nPubmed\n9000\n40028\n0.049417\n4.447556\n0.10\n6.32\n2609\n4702\n\n\nciteseer\n10720\n44258\n0.038513\n4.128545\n0.28\n8.36\n4572\n1868\n\n\nUniprot22m\n1595444\n1595442\n0.000063\n0.999999\n0.00\n3.30\n1556157\n1\n\n\nCit-patents\n3774768\n16518047\n0.000116\n4.375911\n0.09\n10.50\n515785\n1685423\n\n\nciteseerx\n6540401\n15011260\n0.000035\n2.295159\n0.06\n8.40\n567149\n5740710\n\n\nGo-uniprot\n6967956\n34770235\n0.000072\n4.990019\n0.00\n4.80\n6945721\n4\n\n\nUniprot100m\n16087295\n16087293\n0.000006\n1.000000\n0.00\n4.10\n14598959\n1\n\n\nUniprot150m\n25037600\n25037598\n0.000004\n1.000000\n0.00\n4.40\n21650056\n1\n\n\n\n\n\n\n\n\nee=np.geomspace(5000,25000000,num=10)\nax=table_fel[table_fel['vertices'] &lt; 100000].plot.scatter(x='vertices',y='edges',\n                          logx=True,logy=True,\n                          grid=True,label='small',\n                          title='Graphs from FELINE paper, Table 1')\nax=table_fel[table_fel['vertices'] &gt;= 100000].plot.scatter(ax=ax,x='vertices',y='edges',\n                          logx=True,logy=True,\n                          grid=True,c='red',label='large',\n                          title='Graphs from FELINE paper, Table 1')\nax.plot(ee,2*ee,'-',label='dense vs sparse',c='gray')\nfor row in table_fel.itertuples():\n    ax.annotate(row[0], xy=(row.vertices,row.edges),\n                xytext=(3,-3), textcoords='offset points',horizontalalignment='left',\n                family='sans-serif', fontsize=9, color='darkslategrey')\nax.set_xlim(left  =2e3)\nax.set_ylim(bottom=5e3)\nplt.legend(loc='upper left')\n#plt.savefig('feline_graphs_vertices_vs_edges.pdf')\nplt.show()\n\n\n\n\n\nax=table_fel[table_fel['vertices'] &lt; 100000].plot.scatter(x='roots',y='leafs',\n                          logx=True,logy=True,\n                          grid=True,label='small',\n                          title='Graphs from FELINE paper, Table 1')\nax=table_fel[table_fel['vertices'] &gt;= 100000].plot.scatter(ax=ax,x='roots',y='leafs',\n                          logx=True,logy=True,\n                          grid=True,c='red',label='large',\n                          title='Graphs from FELINE paper, Table 1')\nfor row in table_fel.itertuples():\n    ax.annotate(row[0], xy=(row.roots,row.leafs),\n                xytext=(-3,-3), textcoords='offset points',horizontalalignment='right',\n                family='sans-serif', fontsize=9, color='darkslategrey')\nplt.show()"
  },
  {
    "objectID": "datasets.html#datasets-from-ferrari-table_fer",
    "href": "datasets.html#datasets-from-ferrari-table_fer",
    "title": "Graph datasets",
    "section": "Datasets from FERRARI (table_fer)",
    "text": "Datasets from FERRARI (table_fer)\n“FERRARI: Flexible and Efficient Reachability Range Assignment for Graph Indexing” (2013)\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.365.2894&rep=rep1&type=pdf\n\n\n\n\n\nTable 1. Datasets used\n\n\n\n\\(|V|\\) - number of vertices (nodes)\n\\(|E|\\) - number of edges\n\\(|V_C|\\), \\(|E_C|\\) - number of, respectively, vertices and edges in the condensed graph, where maximial strongly connected components were collapsed into “supernodes”, i.e. directed acyclic graph (DAG) created from given graph\n\n\n#@title Table 1: Datasets Used from FERRARI (table_fer) { form-width: \"25%\" }\n\ncsv=u\"\"\"Graph;type;vertices;edges\narXiv;small, dense;6000;66707\nGo;small, dense;6793;13361\nPubmed;small, dense;9000;40028\nHuman;small, sparse;38811;39816\nciteseer;large;693947;312383\nCit-patents;large;3774768;16518047\nciteseerx;large;6540401;15011260\nGo-uniprot;large;6967956;34770235\nGovWild;RDF;80222880;23652610\nYago2;RDF;16375503;25908132\nTwitter;social network;18121168;18359487\nWeb-UK;web graph;22753644;38184039\n\"\"\"\ncsv_stream = io.StringIO(csv)\n\ntable_fer=pd.read_csv(csv_stream,sep=';',index_col=0)\ntable_fer.insert(loc=3,column='edges/vertices',\n               value=table_fer['edges']/table_fer['vertices'])\ntable_fer\n\n\n\n\n\n\n\n\ntype\nvertices\nedges\nedges/vertices\n\n\nGraph\n\n\n\n\n\n\n\n\narXiv\nsmall, dense\n6000\n66707\n11.117833\n\n\nGo\nsmall, dense\n6793\n13361\n1.966878\n\n\nPubmed\nsmall, dense\n9000\n40028\n4.447556\n\n\nHuman\nsmall, sparse\n38811\n39816\n1.025895\n\n\nciteseer\nlarge\n693947\n312383\n0.450154\n\n\nCit-patents\nlarge\n3774768\n16518047\n4.375911\n\n\nciteseerx\nlarge\n6540401\n15011260\n2.295159\n\n\nGo-uniprot\nlarge\n6967956\n34770235\n4.990019\n\n\nGovWild\nRDF\n80222880\n23652610\n0.294836\n\n\nYago2\nRDF\n16375503\n25908132\n1.582127\n\n\nTwitter\nsocial network\n18121168\n18359487\n1.013151\n\n\nWeb-UK\nweb graph\n22753644\n38184039\n1.678150\n\n\n\n\n\n\n\n\nee=np.geomspace(5000,80000000,num=10)\nax=table_fer[table_fer['vertices'] &lt; 100000].plot.scatter(x='vertices',y='edges',\n                          logx=True,logy=True,\n                          grid=True,label='small',\n                          title='Graphs from FERRARI paper, Table 1(a)(b)')\nax=table_fer[table_fer['vertices'] &gt;= 100000].plot.scatter(ax=ax,x='vertices',y='edges',\n                          logx=True,logy=True,\n                          grid=True,c='red',label='large',\n                          title='Graphs from FERRARI paper, Table 1(a)(b)')\nax.plot(ee,ee,'--',ee,2*ee,'-',label='dense vs sparse',c='gray')\nfor row in table_fer[table_fer['vertices'] &lt; 100000].itertuples():\n    ax.annotate(row[0], xy=(row.vertices,row.edges),\n                xytext=(3,-3), textcoords='offset points',horizontalalignment='left',\n                family='sans-serif', fontsize=9, color='darkslategrey')\n    #print(row)\nfor row in table_fer[table_fer['vertices'] &gt;= 100000].itertuples():\n    ax.annotate(row[0], xy=(row.vertices,row.edges),\n                xytext=(-3,-8), textcoords='offset points',horizontalalignment='right',\n                family='sans-serif', fontsize=9, color='darkslategrey')\nplt.show()"
  },
  {
    "objectID": "datasets.html#datasets-from-preach-table_rch",
    "href": "datasets.html#datasets-from-preach-table_rch",
    "title": "Graph datasets",
    "section": "Datasets from PReaCH (table_rch)",
    "text": "Datasets from PReaCH (table_rch)\n“PReaCH: A Fast Lightweight Reachability Index using Pruning and Contraction Hierarchies” (2014)\nhttps://arxiv.org/abs/1404.4465\n\n\n\n\\(m/n\\) - edge density, number of edges divided by number of nodes (vertices)\n\\(d\\) - length of the longest path, or maximal path length\n% pos - the fraction of positive queries in a random sample of 100000 queries\n\n\n#@title Table 1: Instances used for experimets from PReaCH (table_rch) { form-width: \"25%\" }\n\ncsv=u\"\"\"Graph;type;vertices;edges;edges/vertices;maxlevel;r-score\narXiv;small, dense;6000;66707;11.12;167;0.15\nciteseer-sub;small, dense;11000;44000;4.13;36;0.004\nGo;small, dense;6793;13361;1.97;16;0.002\nPubmed;small, dense;9000;40028;4.45;19;0.007\nYago;small, dense;6642;42392;6.38;13;0.002\nagrocyc;small, sparse;13000;14000;1.07;16;0.001\namaze;small, sparse;3710;3947;1.06;16;0.17\nanthra;small, sparse;12000;13000;1.07;16;0.001\necoo;small, sparse;13000;14000;1.08;22;0.001\nHuman;small, sparse;38811;39816;1.01;18;0.000\nkegg;small, sparse;3617;4395;1.22;26;0.20\nmtbrv;small, sparse;9602;10000;1.09;22;0.002\nnasa;small, sparse;5605;6538;1.17;35;0.006\nvchocyc;small, sparse;9491;10000;1.09;21;0.001\nxmark;small, sparse;6080;7051;1.16;38;0.014\nciteseer;large;693947;312383;0.45;13;0.000\nciteseerx;large;6540401;15011260;2.30;59;0.002\nCit-patents;large;3774768;16518047;4.38;32;0.001\nGo-uniprot;large;6967956;34770235;4.99;21;0.000\nUniprot22m;large;1595444;1595442;1.00;4;0.000\nUniprot100m;large;16087295;16087293;1.00;9;0.000\nUniprot150m;large;25037600;25037598;1.00;10;0.000\nemail-EuAll;stanford;231000;223000;0.97;7;0.05\np2p-Gnutella31;stanford;48000;55000;1.15;14;0.008\nsoc-LiveJournal1;stanford;971000;1024000;1.05;24;0.21\nweb-Google;stanford;372000;518000;1.39;34;0.15\nwiki-Talk;stanford;2282000;2312000;1.01;8;0.008\n\"\"\"\ncsv_stream = io.StringIO(csv)\n\ntable_rch=pd.read_csv(csv_stream,sep=';',index_col=0)\nprint(table_rch['type'].unique())\ntable_rch\n\n['small, dense' 'small, sparse' 'large' 'stanford']\n\n\n\n\n\n\n\n\n\ntype\nvertices\nedges\nedges/vertices\nmaxlevel\nr-score\n\n\nGraph\n\n\n\n\n\n\n\n\n\n\narXiv\nsmall, dense\n6000\n66707\n11.12\n167\n0.150\n\n\nciteseer-sub\nsmall, dense\n11000\n44000\n4.13\n36\n0.004\n\n\nGo\nsmall, dense\n6793\n13361\n1.97\n16\n0.002\n\n\nPubmed\nsmall, dense\n9000\n40028\n4.45\n19\n0.007\n\n\nYago\nsmall, dense\n6642\n42392\n6.38\n13\n0.002\n\n\nagrocyc\nsmall, sparse\n13000\n14000\n1.07\n16\n0.001\n\n\namaze\nsmall, sparse\n3710\n3947\n1.06\n16\n0.170\n\n\nanthra\nsmall, sparse\n12000\n13000\n1.07\n16\n0.001\n\n\necoo\nsmall, sparse\n13000\n14000\n1.08\n22\n0.001\n\n\nHuman\nsmall, sparse\n38811\n39816\n1.01\n18\n0.000\n\n\nkegg\nsmall, sparse\n3617\n4395\n1.22\n26\n0.200\n\n\nmtbrv\nsmall, sparse\n9602\n10000\n1.09\n22\n0.002\n\n\nnasa\nsmall, sparse\n5605\n6538\n1.17\n35\n0.006\n\n\nvchocyc\nsmall, sparse\n9491\n10000\n1.09\n21\n0.001\n\n\nxmark\nsmall, sparse\n6080\n7051\n1.16\n38\n0.014\n\n\nciteseer\nlarge\n693947\n312383\n0.45\n13\n0.000\n\n\nciteseerx\nlarge\n6540401\n15011260\n2.30\n59\n0.002\n\n\nCit-patents\nlarge\n3774768\n16518047\n4.38\n32\n0.001\n\n\nGo-uniprot\nlarge\n6967956\n34770235\n4.99\n21\n0.000\n\n\nUniprot22m\nlarge\n1595444\n1595442\n1.00\n4\n0.000\n\n\nUniprot100m\nlarge\n16087295\n16087293\n1.00\n9\n0.000\n\n\nUniprot150m\nlarge\n25037600\n25037598\n1.00\n10\n0.000\n\n\nemail-EuAll\nstanford\n231000\n223000\n0.97\n7\n0.050\n\n\np2p-Gnutella31\nstanford\n48000\n55000\n1.15\n14\n0.008\n\n\nsoc-LiveJournal1\nstanford\n971000\n1024000\n1.05\n24\n0.210\n\n\nweb-Google\nstanford\n372000\n518000\n1.39\n34\n0.150\n\n\nwiki-Talk\nstanford\n2282000\n2312000\n1.01\n8\n0.008\n\n\n\n\n\n\n\n\nee=np.geomspace(4000,25000000,num=10)\nax=plt.gca()\ntypes =table_rch['type'].unique()\ncolors=('blue', 'cyan', 'red', 'orange')\nfor i, t in enumerate(types):\n  ax=table_rch[table_rch['type'] == t].plot.scatter(x='vertices',y='edges',ax=ax,\n                                                    logx=True,logy=True,grid=True,\n                                                    label=t,c=colors[i],\n                                                    title='Graphs from PReaCH paper, Table 1')\nax.plot(ee,ee,'--',ee,2*ee,'-',label='dense vs sparse',c='gray')\nann=('Uniprot150m','soc-LiveJournal1','wiki-Talk','Go','amaze','arXiv','Human','citeseer','citeseerx')\nfor g in ann:\n  row = table_rch.loc[g]\n  ax.annotate(g, xy=(row['vertices'],row['edges']),\n              xytext=(5,-3), textcoords='offset points',horizontalalignment='left',\n              family='sans-serif', fontsize=9, color='darkslategrey')\nplt.show()"
  },
  {
    "objectID": "datasets.html#datasets-from-oreach-table_or",
    "href": "datasets.html#datasets-from-oreach-table_or",
    "title": "Graph datasets",
    "section": "Datasets from O’Reach (table_or)",
    "text": "Datasets from O’Reach (table_or)\n“Faster Reachability in Static Graphs” (2020)\nhttps://arxiv.org/abs/2008.10932\n\nTo facilitate comparability, we adopt the instances used in the papers introducing PReaCH [22], GRAIL [35], and TF [4], which are available either from the GRAIL code repository or the Stanford Network Analysis Platform SNAP [21]\n\n\n\n\n\\(n/10^3\\) - number of nodes (vertices) in 1000s\n\\(m/10^3\\) - number of edges in 1000s\n\\(m/n\\) - edge density, number of edges divided by number of nodes (vertices)\n\\(\\mathcal{S}\\%\\) - ratio of number of non-isolated sources to the number of nodes\n\\(\\mathcal{T}\\%\\) - ratio of number of non-isolated sinks to the number of nodes\n\\(\\mathcal{I}\\%\\) - ratio of number of isolated nodes to the total number of nodes\n#WCC - number of weakly connected components\n#WCC(large) - number of weakly connected components with more than \\(n/10\\) nodes\n\\(L_{\\text{max}}\\) - maximum topological level, which is also length of the longest path, or maximal path length (equals diameter)\n\\(\\rho\\%\\) - the fraction of positive queries in a random sample of 100000 queries\n\n\n#@title Table 2: Instances used for experimets from O'Reach (table_or) { form-width: \"25%\" }\n\ncsv=u\"\"\"Graph;type;source;vertices/1000;edges/1000;edges/vertices;sources [%];sinks [%];isolated [%];WCCs;WCCs (large);maxlevel;r-score [%]\nciteseer.scc;large;citation network;693.9;312.3;0.45;37.5;4.1;50.9;28663;1;13;0.0002\nciteseerx;large;citation network;6540.4;15011.3;2.30;8.7;87.8;0.0;47076;1;59;0.1367\ncit-Patents;large;citation network;3774.8;16518.9;4.38;13.7;44.6;0.0;3627;1;32;0.0409\ngo_uniprot;large;taxonomy graph;6968.0;34769.3;4.99;99.7;0.0;0.0;1;1;20;0.0004\nuniprotenc_22m;large;RDF graph of protein database;1595.4;1595.4;1.00;97.5;0.0;0.0;1;1;4;0.0001\nuniprotenc_100m;large;RDF graph of protein database;16087.3;16087.3;1.00;90.7;0.0;0.0;1;1;9;0.0000\nuniprotenc_150m;large;RDF graph of protein database;25037.6;25037.6;1.00;86.5;0.0;0.0;1;1;10;0.0000\n\ngo_sub;small, dense;taxonomy graph;6.8;13.4;1.97;0.9;45.4;0.0;1;1;16;0.2258\npubmed_sub;small, dense;citation network;9.0;40.0;4.45;29.0;52.2;0.0;1;1;19;0.6458\nyago_sub;small, dense;semantic knowledge database;6.6;42.4;6.38;77.9;4.0;0.0;1;1;13;0.1506\nciteseer_sub;small, dense;citation network;10.7;44.3;4.13;42.6;17.4;0.0;1;1;36;0.3672\narXiv;small, dense;citation network;6.0;66.7;11.12;16.0;10.4;0.0;1;1;167;15.4643\n\namaze;small, sparse;metabolic network;3.7;3.6;0.97;32.1;41.8;9.9;22;1;16;17.2337\nkegg;small, sparse;metabolic network;3.6;4.4;1.22;32.6;45.2;0.1;22;1;26;20.1636\nnasa;small, sparse;XML documents;5.6;6.5;1.17;0.0;55.6;0.0;1;1;35;0.5284\nxmark;small, sparse;XML documents;6.1;7.1;1.16;0.0;58.3;0.0;1;1;38;1.4513\nvchocyc;small, sparse;pathway and genome database;9.5;10.3;1.09;0.0;92.8;0.0;1;1;21;0.1517\nmtbrv;small, sparse;pathway and genome database;9.6;10.4;1.09;0.0;93.0;0.0;1;1;22;0.1511\nanthra;small, sparse;pathway and genome database;12.5;13.1;1.05;0.0;94.7;0.0;2;1;16;0.0951\necoo;small, sparse;pathway and genome database;12.6;13.4;1.06;0.0;94.1;0.0;1;1;22;0.1088\nagrocyc;small, sparse;pathway and genome database;12.7;13.4;1.06;0.0;94.1;0.0;1;1;16;0.1060\nhuman;small, sparse;pathway and genome database;38.8;39.6;1.02;0.0;98.1;0.0;1;1;18;0.0231\n\np2p-Gnutella31;SNAP;peer-to-peer network;48.4;55.3;1.14;0.6;95.4;0.0;12;1;14;0.7725\nemail-EuAll;SNAP;e-mail network graph;230.8;223.0;0.97;82.6;17.3;0.0;15631;1;7;5.0732\nweb-Google;SNAP;web graph;371.8;517.8;1.39;43.7;37.9;0.0;2585;1;34;14.8090\nsoc-LiveJournal1;SNAP;social network;970.3;1024.1;1.06;39.9;57.7;0.0;521;1;24;5.3781\nwiki-Talk;SNAP;communication network;2281.9;2311.6;1.01;1.1;98.5;0.0;2487;1;8;0.8117\n\"\"\"\ncsv_stream = io.StringIO(csv)\n\ntable_or=pd.read_csv(csv_stream,sep=';',index_col=0)\nprint(table_or['type'].unique())\ntable_or.drop('source', axis=1)\n\n['large' 'small, dense' 'small, sparse' 'SNAP']\n\n\n\n\n\n\n\n\n\ntype\nvertices/1000\nedges/1000\nedges/vertices\nsources [%]\nsinks [%]\nisolated [%]\nWCCs\nWCCs (large)\nmaxlevel\nr-score [%]\n\n\nGraph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nciteseer.scc\nlarge\n693.9\n312.3\n0.45\n37.5\n4.1\n50.9\n28663\n1\n13\n0.0002\n\n\nciteseerx\nlarge\n6540.4\n15011.3\n2.30\n8.7\n87.8\n0.0\n47076\n1\n59\n0.1367\n\n\ncit-Patents\nlarge\n3774.8\n16518.9\n4.38\n13.7\n44.6\n0.0\n3627\n1\n32\n0.0409\n\n\ngo_uniprot\nlarge\n6968.0\n34769.3\n4.99\n99.7\n0.0\n0.0\n1\n1\n20\n0.0004\n\n\nuniprotenc_22m\nlarge\n1595.4\n1595.4\n1.00\n97.5\n0.0\n0.0\n1\n1\n4\n0.0001\n\n\nuniprotenc_100m\nlarge\n16087.3\n16087.3\n1.00\n90.7\n0.0\n0.0\n1\n1\n9\n0.0000\n\n\nuniprotenc_150m\nlarge\n25037.6\n25037.6\n1.00\n86.5\n0.0\n0.0\n1\n1\n10\n0.0000\n\n\ngo_sub\nsmall, dense\n6.8\n13.4\n1.97\n0.9\n45.4\n0.0\n1\n1\n16\n0.2258\n\n\npubmed_sub\nsmall, dense\n9.0\n40.0\n4.45\n29.0\n52.2\n0.0\n1\n1\n19\n0.6458\n\n\nyago_sub\nsmall, dense\n6.6\n42.4\n6.38\n77.9\n4.0\n0.0\n1\n1\n13\n0.1506\n\n\nciteseer_sub\nsmall, dense\n10.7\n44.3\n4.13\n42.6\n17.4\n0.0\n1\n1\n36\n0.3672\n\n\narXiv\nsmall, dense\n6.0\n66.7\n11.12\n16.0\n10.4\n0.0\n1\n1\n167\n15.4643\n\n\namaze\nsmall, sparse\n3.7\n3.6\n0.97\n32.1\n41.8\n9.9\n22\n1\n16\n17.2337\n\n\nkegg\nsmall, sparse\n3.6\n4.4\n1.22\n32.6\n45.2\n0.1\n22\n1\n26\n20.1636\n\n\nnasa\nsmall, sparse\n5.6\n6.5\n1.17\n0.0\n55.6\n0.0\n1\n1\n35\n0.5284\n\n\nxmark\nsmall, sparse\n6.1\n7.1\n1.16\n0.0\n58.3\n0.0\n1\n1\n38\n1.4513\n\n\nvchocyc\nsmall, sparse\n9.5\n10.3\n1.09\n0.0\n92.8\n0.0\n1\n1\n21\n0.1517\n\n\nmtbrv\nsmall, sparse\n9.6\n10.4\n1.09\n0.0\n93.0\n0.0\n1\n1\n22\n0.1511\n\n\nanthra\nsmall, sparse\n12.5\n13.1\n1.05\n0.0\n94.7\n0.0\n2\n1\n16\n0.0951\n\n\necoo\nsmall, sparse\n12.6\n13.4\n1.06\n0.0\n94.1\n0.0\n1\n1\n22\n0.1088\n\n\nagrocyc\nsmall, sparse\n12.7\n13.4\n1.06\n0.0\n94.1\n0.0\n1\n1\n16\n0.1060\n\n\nhuman\nsmall, sparse\n38.8\n39.6\n1.02\n0.0\n98.1\n0.0\n1\n1\n18\n0.0231\n\n\np2p-Gnutella31\nSNAP\n48.4\n55.3\n1.14\n0.6\n95.4\n0.0\n12\n1\n14\n0.7725\n\n\nemail-EuAll\nSNAP\n230.8\n223.0\n0.97\n82.6\n17.3\n0.0\n15631\n1\n7\n5.0732\n\n\nweb-Google\nSNAP\n371.8\n517.8\n1.39\n43.7\n37.9\n0.0\n2585\n1\n34\n14.8090\n\n\nsoc-LiveJournal1\nSNAP\n970.3\n1024.1\n1.06\n39.9\n57.7\n0.0\n521\n1\n24\n5.3781\n\n\nwiki-Talk\nSNAP\n2281.9\n2311.6\n1.01\n1.1\n98.5\n0.0\n2487\n1\n8\n0.8117\n\n\n\n\n\n\n\n\ntable_or.describe()\n\n\n\n\n\n\n\n\nvertices/1000\nedges/1000\nedges/vertices\nsources [%]\nsinks [%]\nisolated [%]\nWCCs\nWCCs (large)\nmaxlevel\nr-score [%]\n\n\n\n\ncount\n27.000000\n27.000000\n27.000000\n27.000000\n27.000000\n27.000000\n27.000000\n27.0\n27.000000\n27.000000\n\n\nmean\n2398.311111\n4214.537037\n2.208519\n30.862963\n49.644444\n2.255556\n3728.296296\n1.0\n26.481481\n3.107011\n\n\nstd\n5701.997573\n9035.370188\n2.355097\n35.359028\n37.259695\n9.906227\n10621.160634\n0.0\n30.505195\n6.077715\n\n\nmin\n3.600000\n3.600000\n0.450000\n0.000000\n0.000000\n0.000000\n1.000000\n1.0\n4.000000\n0.000000\n\n\n25%\n7.900000\n13.250000\n1.015000\n0.000000\n13.850000\n0.000000\n1.000000\n1.0\n13.500000\n0.068000\n\n\n50%\n12.700000\n44.300000\n1.090000\n16.000000\n45.400000\n0.000000\n1.000000\n1.0\n19.000000\n0.151700\n\n\n75%\n1282.850000\n1309.750000\n2.135000\n43.150000\n92.900000\n0.000000\n271.500000\n1.0\n29.000000\n1.131500\n\n\nmax\n25037.600000\n34769.300000\n11.120000\n99.700000\n98.500000\n50.900000\n47076.000000\n1.0\n167.000000\n20.163600\n\n\n\n\n\n\n\n\nee = np.geomspace(3,25000,num=10)\nfig = plt.figure(figsize=(13,8))\nax = fig.gca()\ntypes  = table_or['type'].unique()\ncolors = ('blue', 'cyan', 'red', 'orange')\ncolor_map = {\n    'small, dense': 'blue',\n    'small, sparse': 'cyan',\n    'large': 'red',\n    'SNAP': 'orange',\n}\nfor t in types:\n  ax=table_or[table_or['type'] == t].plot.scatter(x='vertices/1000',y='edges/1000',ax=ax,\n                                                  logx=True,logy=True,grid=True,\n                                                  label=t,c=color_map[t],\n                                                  title='Graphs from O\\'Reach paper, Table 2')\nax.plot(ee,ee,'--',ee,2*ee,'-',label='dense vs sparse',c='gray')\nax.plot(ee,1000*ee*ee,':',label='max edges',c='0.5')\n#ann=('Uniprot150m','soc-LiveJournal1','wiki-Talk','Go','amaze','arXiv','Human','citeseer','citeseerx')\n#for g in ann:\n#  row = table_rch.loc[g]\n#  ax.annotate(g, xy=(row['vertices'],row['edges']),\n#              xytext=(5,-3), textcoords='offset points',horizontalalignment='left',\n#              family='sans-serif', fontsize=9, color='darkslategrey')\nfor row in table_or.itertuples():\n    ax.annotate(row[0], xy=(row[3],row[4]),\n                xytext=(-3,+3), textcoords='offset points',horizontalalignment='right',\n                family='sans-serif', fontsize=9, color='darkslategrey')\n#ax.set_xlim(left  =2e3)\n#ax.set_ylim(bottom=5e3)\nax.set_ylim(top=1e5)\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\n\nee = np.geomspace(4,25000,num=10)\n#fig = plt.figure(figsize=(13,8))\nfig = plt.figure()\nax = fig.gca()\ntypes  = table_or['type'].unique()\ncolors = ('blue', 'cyan', 'red', 'orange')\ncolor_map = {\n    'small, dense': 'blue',\n    'small, sparse': 'cyan',\n    'large': 'red',\n    'SNAP': 'orange',\n}\nfor t in types:\n  ax=table_or[table_or['type'] == t].plot.scatter(x='sources [%]',y='sinks [%]',ax=ax,\n                                                  logx=False,logy=False,grid=True,\n                                                  label=t,c=color_map[t],\n                                                  title='Graphs from O\\'Reach paper, Table 2')\nplt.show()"
  },
  {
    "objectID": "checkpoint.html",
    "href": "checkpoint.html",
    "title": "Checkpointing",
    "section": "",
    "text": "# imports for example graphs\nimport git_commit_graph_ext.example_graphs as graphs\n\n\n# before tests (test fixture)\n\n# example graph\nexample_graph = graphs.commit_graph_Stolee()\nexample_graph_name = 'commit_graph_Stolee'\n\n# check that it worked\nprint(\"the example commit graph '{}' from Stolee blog posts has {:d} nodes and {:d} edges\".\n      format(example_graph_name, example_graph.number_of_nodes(), example_graph.number_of_edges()))\n\nthe example commit graph 'commit_graph_Stolee' from Stolee blog posts has 23 nodes and 31 edges\n\n\n\n\nHelper function to get basename for saving graph and/or graph data to a file (as a dataset).\nTest that examples from the docstring works:\n\nassert _savefile_name('example_graph') == Path('datasets/example_graph.df_edgelist.csv.gz')\nassert _savefile_name('example_graph', kind='adjlist', file_format='txt') == Path('datasets/example_graph.adjlist.txt')\nassert _savefile_name('example_graph', out_dir='data', kind='adjlist', file_format='txt') == Path('data/example_graph.adjlist.txt')\n\nSave graph to a DataFrame (saving containing edgelist information), and restore it.\n\nsource\n\n\n\n\n dataframe_to_graph (df)\n\n\nsource\n\n\n\n\n graph_to_dataframe (graph)\n\nTest that saving to the DataFrame and restoring from it works correctly,… up to nodes that are not connected – those cannot be stored using only edge list data, but with addition of node list it should be possible to restore graph exactly:\n\nexample_graph_df = graph_to_dataframe(example_graph)\nassert len(example_graph_df.index) == example_graph.number_of_edges()\nprint('ok - number of rows in dataframe with edgelist {} matches number of edges {}'.\n      format(len(example_graph_df.index), example_graph.number_of_edges()))\nrestored_graph = dataframe_to_graph(example_graph_df)\nassert restored_graph.number_of_edges() == example_graph.number_of_edges()\nprint('ok - number of edges {} in restored graph matches number of edges {} in the original'.\n      format(restored_graph.number_of_edges(), example_graph.number_of_edges()))\nassert restored_graph.edges == example_graph.edges\nprint('ok - all edges from the original graph got restored')\nassert set(restored_graph).issubset(example_graph)\nprint('ok - all restored nodes are in the original graph')\n\nok - number of rows in dataframe with edgelist 31 matches number of edges 31\nok - number of edges 31 in restored graph matches number of edges 31 in the original\nok - all edges from the original graph got restored\nok - all restored nodes are in the original graph\n\n\nSave graph to a file (via DataFrame containing edgelist information), and restore it\n\nsource\n\n\n\n\n load_graph_df (graph_name, datasets_dir='datasets',\n                input_format='csv.gz')\n\n\nsource\n\n\n\n\n load_df_from_file (filename, input_format='csv.gz')\n\n\nsource\n\n\n\n\n save_graph (graph, graph_name=None, datasets_dir='datasets',\n             output_format='csv.gz', overwrite=False)\n\n\nsource\n\n\n\n\n save_graph_df (df, graph_name, datasets_dir='datasets',\n                output_format='csv.gz', overwrite=False)\n\n\nsource\n\n\n\n\n save_df_to_file (df, filename, output_format='csv.gz')\n\n\nsource\n\n\n\n\n guess_format (filename)\n\nTest guessing file format from file name\n\nassert guess_format('datasets/hellogitworld-commit_graph.df_edgelist.csv.gz') == 'csv.gz'\nassert guess_format('datasets/hellogitworld-commit_graph.df_edgelist.csv') == 'csv'\nassert guess_format('datasets/hellogitworld-commit_graph.adjlist.txt') == 'adjlist.txt'\n\nTest saving graph structure (via DataFrame) to a file, and restoring / reading such DataFrame.\n\nprint('graph.name = {}'.format(example_graph.name))\nprint('testing save_graph()')\nsave_graph(example_graph)\nprint('testing save_graph_df()')\nsave_graph_df(example_graph_df, graph_name=example_graph_name)\nprint('there should be appropriately named file in the list below:')\n[\"{name:&lt;50} {size:&gt;7}\".format(name=p.name, size=p.stat().st_size)\n for p in Path(\"datasets\").glob(example_graph_name+\"*\")]\n\ngraph.name = \ntesting save_graph()\n-&gt; graph_name: \n-&gt; filename: datasets\\.df_edgelist.csv.gz\ntesting save_graph_df()\n-&gt; filename: datasets\\commit_graph_Stolee.df_edgelist.csv.gz\nthere should be appropriately named file in the list below:\n\n\n['commit_graph_Stolee-test.df_edgelist.csv.gz            236',\n 'commit_graph_Stolee-test.df_nodedata.csv.gz            272',\n 'commit_graph_Stolee.df_edgelist.csv.gz                 231',\n 'commit_graph_Stolee.df_nodedata.csv.gz                 267']\n\n\n\nprint('restoring graph named \"{}\"'.format(example_graph_name))\ndf = load_graph_df(example_graph_name)\nassert example_graph_df.equals(df)\nprint('ok - dataframe and restored dataframe are equal')\n\nrestoring graph named \"commit_graph_Stolee\"\n&lt;- filename: datasets\\commit_graph_Stolee.df_edgelist.csv.gz\nok - dataframe and restored dataframe are equal\n\n\n\n\n\nCompute levels and min-post intervals for a graph, and store them as attributes of the graph object\n\nsource\n\n\n\n\n compute_reachability_labels (graph, recompute=False)\n\nTest computing reachability labels and saving them as attributes of the graph object\n\nprint('compute reachability labels for {} graph'.format(example_graph_name))\ncompute_reachability_labels(example_graph)\n\nprint('we should see {} and {} among dict-values public attributes'.format('lvl', 'mpi_ext'))\nfor (attr, val) in example_graph.__dict__.items():\n    if isinstance(val, dict) and not attr.startswith('_'):\n        print('- {:s}'.format(attr))\nassert hasattr(example_graph, 'lvl')\nassert hasattr(example_graph, 'mpi_ext')\nprint('ok - graph has both \"{}\" and \"{}\" attributes'.format('lvl', 'mpi_ext'))\n\nprint('reachability labels should be computed for all nodes')\nassert set(example_graph.lvl.keys()) == set(example_graph.nodes)\nassert set(example_graph.mpi_ext.keys()) == set(example_graph.nodes)\nprint('ok - both lvl and mpi_ext keys are all {} graph nodes'.format(len(example_graph.nodes)))\n\ncompute reachability labels for commit_graph_Stolee graph\nwe should see lvl and mpi_ext among dict-values public attributes\n- graph\n- pos\n- lvl\n- mpi_ext\nok - graph has both \"lvl\" and \"mpi_ext\" attributes\nreachability labels should be computed for all nodes\nok - both lvl and mpi_ext keys are all 23 graph nodes\n\n\nStore reachability labels and per-node information in a DataFrame\n\nsource\n\n\n\n\n graph_data_to_dataframe (graph, append_to=None)\n\nTest computing all per-node data for a graph and storing them in DataFrame\n\ndf = graph_data_to_dataframe(example_graph)\n\nprint('check that the dataframe has all the columns')\nassert set(df.columns) == set(['level', 'f_min', 'min', 'post', 'in degree', 'out degree', 'degree'])\nprint('- columns: {}'.format(df.columns.tolist()))\n\nprint('check that the dataframe has all the rows')\nassert set(df.index) == set(example_graph.nodes)\nprint('- rows:    {}...'.format(list(example_graph.nodes)[0:5]))\n\ndf.head()\n\ncheck that the dataframe has all the columns\n- columns: ['f_min', 'min', 'post', 'level', 'in degree', 'out degree', 'degree']\ncheck that the dataframe has all the rows\n- rows:    ['A', 'a7', 'a5', 'a4', 'a3']...\n\n\n\n\n\n\n\n\n\nf_min\nmin\npost\nlevel\nin degree\nout degree\ndegree\n\n\nnode\n\n\n\n\n\n\n\n\n\n\n\nb0\n1\n1\n1\n0\n3\n0\n3\n\n\na1\n1\n1\n2\n1\n2\n1\n3\n\n\na2\n1\n1\n3\n2\n1\n1\n2\n\n\na3\n1\n1\n4\n3\n2\n1\n3\n\n\na4\n1\n1\n5\n4\n1\n1\n2\n\n\n\n\n\n\n\nCompute and save graph structure, its reachability labels and other per-node info to a file, and restore it.\nDon’t redo calculations that can be retrieved from a file.\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Parameters:\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Returns:\n  else: warn(msg)\n\nsource\n\n\n\n\n compute_cached_df (code, filename, file_format=None, dont_save=False)\n\nCompute DataFrame, or retrieve it from a given file if it exists\nTest that the code runs if the file does not exist.\n\ncompute_cached_df(lambda: print(\"the provided code ran\"), 'this_file_does_not_exist.xxx', dont_save=True)\n\nthe provided code ran\n\n\n\ntest_df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\nassert compute_cached_df(lambda: test_df, 'this_file_does_not_exist.xxx', dont_save=True)\\\n                         .equals(test_df)\n\nTest that the code do not run if the file exists, and that we get what we saved\n\nexpect_df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\ndf_filename = 'datasets/example_dataframe.csv.gz'\nsave_df_to_file(expect_df, df_filename)\nactual_df = compute_cached_df(lambda: print(\"THIS SHOULD NOT RUN\"), df_filename)\nassert actual_df.equals(expect_df)\n\nTest that compute_cached_df saves results to a file\n\n_counter = 0\n\ndef example_dataframe():\n    global _counter\n    _counter = _counter + 1\n    return pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n\nexpect_df = example_dataframe()\n_counter = 0\n\ndf_filename = 'datasets/example_dataframe.csv.gz'\n# Path(df_filename).unlink(missing_ok=True)  # requires Python 3.8 or later\nif Path(df_filename).exists():\n    Path(df_filename).unlink()\n\n# first time it should perform computations and save, second time get from file\nactual_df = compute_cached_df(example_dataframe, df_filename)\nactual_df = compute_cached_df(example_dataframe, df_filename)\n\nprint('dataframe:\\n{}\\n'.format(actual_df))\n\n# tests\nassert actual_df.equals(expect_df)\nprint('ok - extracted dataframe matches the one being saved')\nassert Path(df_filename).exists() and Path(df_filename).is_file()\nprint('ok - there exist file \"{}\"'.format(df_filename))\nassert _counter == 1\nprint('ok - the `example_dataframe()` was called only 1 time (actual: {} time(s))'.format(_counter))\n\ndataframe:\n   col1  col2\n0     1     3\n1     2     4\n\nok - extracted dataframe matches the one being saved\nok - there exist file \"datasets/example_dataframe.csv.gz\"\nok - the `example_dataframe()` was called only 1 time (actual: 1 time(s))\n\n\nCompute DataFrame with graph structure (compute_cached_graph_df()) and reachability labels (compute_cached_reachability_labels_df()) and save it to a file, or retrieve it from a file if it exists.\n\nsource\n\n\n\n\n compute_cached_reachability_labels_df (graph, graph_name=None,\n                                        append_to=None,\n                                        datasets_dir='datasets',\n                                        file_format='csv.gz')\n\n\nsource\n\n\n\n\n compute_cached_graph_df (graph_generator, graph_name,\n                          datasets_dir='datasets', file_format='csv.gz')\n\nTest compute_cached_graph_df() and compute_cached_reachability_labels_df()\n\ngraph_df = compute_cached_graph_df(graphs.commit_graph_Stolee, 'commit_graph_Stolee-test')\nassert graph_df.equals(graph_to_dataframe(graphs.commit_graph_Stolee()))\n\ngraph_data_df = compute_cached_reachability_labels_df(example_graph, example_graph_name)\nassert graph_data_df.equals(graph_data_to_dataframe(example_graph))\n\nTo generate graph out of dataframe, use dataframe_to_graph(), defined earlier.\nTo retrieve reachability labels from the dataframe, and store them in graph attributes, use dataframe_to_reachability_labels().\n\nsource\n\n\n\n\n dataframe_to_reachability_labels (df, graph, recompute=False)\n\n\n# check the assumptions of the code\nassert df[['f_min', 'min', 'post']].to_dict(orient='index') == example_graph.mpi_ext\nassert df['level'].to_dict() == example_graph.lvl\n\nGenerate graph, or retrieve it from saved DataFrame with edge list data.\nGenerate reachability labels for the graph, or retrieve them from saved DataFrame with per-node data, adding missing nodes if necessary\n\nsource\n\n\n\n\n compute_cached_reachability_labels (graph, graph_name=None,\n                                     add_missing_nodes=True,\n                                     datasets_dir='datasets',\n                                     file_format='csv.gz')\n\n\nsource\n\n\n\n\n compute_cached_graph (graph_generator, graph_name,\n                       datasets_dir='datasets', file_format='csv.gz')\n\nTest that compute_cached_graph() works\n\nexpect_graph = graphs.commit_graph_Stolee()\nactual_graph = compute_cached_graph(graphs.commit_graph_Stolee, 'commit_graph_Stolee-test')\nassert expect_graph.nodes == actual_graph.nodes\nprint('ok - retrieved/computed graph has the same nodes')\nassert expect_graph.edges == actual_graph.edges\nprint('ok - retrieved/computed graph has the same edges')\nprint('\\npublic attributes of retrieved/computed graph')\nfor (attr, val) in actual_graph.__dict__.items():\n    if not isinstance(val, type) and not attr.startswith('_'):\n        print('- {:s} ({})'.format(attr, type(val)))\n        \nprint('\\npublic attributes of example graph')\nfor (attr, val) in expect_graph.__dict__.items():\n    if not isinstance(val, type)  and not attr.startswith('_'):\n        print('- {:s} ({})'.format(attr, type(val)))\n\nok - retrieved/computed graph has the same nodes\nok - retrieved/computed graph has the same edges\n\npublic attributes of retrieved/computed graph\n- graph (&lt;class 'dict'&gt;)\n- df_edgelist (&lt;class 'pandas.core.frame.DataFrame'&gt;)\n- nodes (&lt;class 'networkx.classes.reportviews.NodeView'&gt;)\n\npublic attributes of example graph\n- graph (&lt;class 'dict'&gt;)\n- pos (&lt;class 'dict'&gt;)\n- nodes (&lt;class 'networkx.classes.reportviews.NodeView'&gt;)\n\n\nTest that compute_cached_reachability_labels() works\n\nsome_graph = graphs.commit_graph_Stolee()\ncompute_cached_reachability_labels(some_graph, 'commit_graph_Stolee-test')\ncompute_cached_reachability_labels(some_graph, 'commit_graph_Stolee-test')\n\n#print('')\nprint('public attributes of the graph named \"{}\"'.format('commit_graph_Stolee-test'))\nfor (attr, val) in some_graph.__dict__.items():\n    if not isinstance(val, type)  and not attr.startswith('_'):\n        print('- {!s} ({})'.format(attr, type(val)))\n\nprint('')\ndf = some_graph.df_nodedata\nprint('check that the dataframe has all the columns')\nassert set(df.columns) == set(['level', 'f_min', 'min', 'post', 'in degree', 'out degree', 'degree'])\nprint('- columns: {}'.format(df.columns.tolist()))\n\nprint('check that the dataframe has all the rows')\nassert set(df.index) == set(some_graph.nodes)\nprint('- rows:    {}...'.format(list(example_graph.nodes)[0:5]))        \n\nprint('check that graph has \"lvl\" attribute')\nassert hasattr(some_graph, 'lvl')\nprint('- lvls:    {}...'.format({k: some_graph.lvl[k] for k in list(some_graph.lvl)[:5]}))\n\nprint('check that graph has \"mpi_ext\" attribute')\nassert hasattr(some_graph, 'mpi_ext')\nprint('- mpi_ext: {}...'.format({k: some_graph.mpi_ext[k] for k in list(some_graph.mpi_ext)[:2]}))\n\nsome_graph.df_nodedata.head()\n\npublic attributes of the graph named \"commit_graph_Stolee-test\"\n- graph (&lt;class 'dict'&gt;)\n- pos (&lt;class 'dict'&gt;)\n- df_nodedata (&lt;class 'pandas.core.frame.DataFrame'&gt;)\n- lvl (&lt;class 'dict'&gt;)\n- mpi_ext (&lt;class 'dict'&gt;)\n- nodes (&lt;class 'networkx.classes.reportviews.NodeView'&gt;)\n\ncheck that the dataframe has all the columns\n- columns: ['f_min', 'min', 'post', 'level', 'in degree', 'out degree', 'degree']\ncheck that the dataframe has all the rows\n- rows:    ['A', 'a7', 'a5', 'a4', 'a3']...\ncheck that graph has \"lvl\" attribute\n- lvls:    {'b0': 0, 'a1': 1, 'a2': 2, 'a3': 3, 'a4': 4}...\ncheck that graph has \"mpi_ext\" attribute\n- mpi_ext: {'b0': {'f_min': 1, 'min': 1, 'post': 1}, 'a1': {'f_min': 1, 'min': 1, 'post': 2}}...\n\n\n\n\n\n\n\n\n\nf_min\nmin\npost\nlevel\nin degree\nout degree\ndegree\n\n\nnode\n\n\n\n\n\n\n\n\n\n\n\nb0\n1\n1\n1\n0\n3\n0\n3\n\n\na1\n1\n1\n2\n1\n2\n1\n3\n\n\na2\n1\n1\n3\n2\n1\n1\n2\n\n\na3\n1\n1\n4\n3\n2\n1\n3\n\n\na4\n1\n1\n5\n4\n1\n1\n2"
  },
  {
    "objectID": "checkpoint.html#functions-for-computing-reachability-labels-and-saving-them-and-graph-to-a-file-etc.",
    "href": "checkpoint.html#functions-for-computing-reachability-labels-and-saving-them-and-graph-to-a-file-etc.",
    "title": "Checkpointing",
    "section": "",
    "text": "# imports for example graphs\nimport git_commit_graph_ext.example_graphs as graphs\n\n\n# before tests (test fixture)\n\n# example graph\nexample_graph = graphs.commit_graph_Stolee()\nexample_graph_name = 'commit_graph_Stolee'\n\n# check that it worked\nprint(\"the example commit graph '{}' from Stolee blog posts has {:d} nodes and {:d} edges\".\n      format(example_graph_name, example_graph.number_of_nodes(), example_graph.number_of_edges()))\n\nthe example commit graph 'commit_graph_Stolee' from Stolee blog posts has 23 nodes and 31 edges\n\n\n\n\nHelper function to get basename for saving graph and/or graph data to a file (as a dataset).\nTest that examples from the docstring works:\n\nassert _savefile_name('example_graph') == Path('datasets/example_graph.df_edgelist.csv.gz')\nassert _savefile_name('example_graph', kind='adjlist', file_format='txt') == Path('datasets/example_graph.adjlist.txt')\nassert _savefile_name('example_graph', out_dir='data', kind='adjlist', file_format='txt') == Path('data/example_graph.adjlist.txt')\n\nSave graph to a DataFrame (saving containing edgelist information), and restore it.\n\nsource\n\n\n\n\n dataframe_to_graph (df)\n\n\nsource\n\n\n\n\n graph_to_dataframe (graph)\n\nTest that saving to the DataFrame and restoring from it works correctly,… up to nodes that are not connected – those cannot be stored using only edge list data, but with addition of node list it should be possible to restore graph exactly:\n\nexample_graph_df = graph_to_dataframe(example_graph)\nassert len(example_graph_df.index) == example_graph.number_of_edges()\nprint('ok - number of rows in dataframe with edgelist {} matches number of edges {}'.\n      format(len(example_graph_df.index), example_graph.number_of_edges()))\nrestored_graph = dataframe_to_graph(example_graph_df)\nassert restored_graph.number_of_edges() == example_graph.number_of_edges()\nprint('ok - number of edges {} in restored graph matches number of edges {} in the original'.\n      format(restored_graph.number_of_edges(), example_graph.number_of_edges()))\nassert restored_graph.edges == example_graph.edges\nprint('ok - all edges from the original graph got restored')\nassert set(restored_graph).issubset(example_graph)\nprint('ok - all restored nodes are in the original graph')\n\nok - number of rows in dataframe with edgelist 31 matches number of edges 31\nok - number of edges 31 in restored graph matches number of edges 31 in the original\nok - all edges from the original graph got restored\nok - all restored nodes are in the original graph\n\n\nSave graph to a file (via DataFrame containing edgelist information), and restore it\n\nsource\n\n\n\n\n load_graph_df (graph_name, datasets_dir='datasets',\n                input_format='csv.gz')\n\n\nsource\n\n\n\n\n load_df_from_file (filename, input_format='csv.gz')\n\n\nsource\n\n\n\n\n save_graph (graph, graph_name=None, datasets_dir='datasets',\n             output_format='csv.gz', overwrite=False)\n\n\nsource\n\n\n\n\n save_graph_df (df, graph_name, datasets_dir='datasets',\n                output_format='csv.gz', overwrite=False)\n\n\nsource\n\n\n\n\n save_df_to_file (df, filename, output_format='csv.gz')\n\n\nsource\n\n\n\n\n guess_format (filename)\n\nTest guessing file format from file name\n\nassert guess_format('datasets/hellogitworld-commit_graph.df_edgelist.csv.gz') == 'csv.gz'\nassert guess_format('datasets/hellogitworld-commit_graph.df_edgelist.csv') == 'csv'\nassert guess_format('datasets/hellogitworld-commit_graph.adjlist.txt') == 'adjlist.txt'\n\nTest saving graph structure (via DataFrame) to a file, and restoring / reading such DataFrame.\n\nprint('graph.name = {}'.format(example_graph.name))\nprint('testing save_graph()')\nsave_graph(example_graph)\nprint('testing save_graph_df()')\nsave_graph_df(example_graph_df, graph_name=example_graph_name)\nprint('there should be appropriately named file in the list below:')\n[\"{name:&lt;50} {size:&gt;7}\".format(name=p.name, size=p.stat().st_size)\n for p in Path(\"datasets\").glob(example_graph_name+\"*\")]\n\ngraph.name = \ntesting save_graph()\n-&gt; graph_name: \n-&gt; filename: datasets\\.df_edgelist.csv.gz\ntesting save_graph_df()\n-&gt; filename: datasets\\commit_graph_Stolee.df_edgelist.csv.gz\nthere should be appropriately named file in the list below:\n\n\n['commit_graph_Stolee-test.df_edgelist.csv.gz            236',\n 'commit_graph_Stolee-test.df_nodedata.csv.gz            272',\n 'commit_graph_Stolee.df_edgelist.csv.gz                 231',\n 'commit_graph_Stolee.df_nodedata.csv.gz                 267']\n\n\n\nprint('restoring graph named \"{}\"'.format(example_graph_name))\ndf = load_graph_df(example_graph_name)\nassert example_graph_df.equals(df)\nprint('ok - dataframe and restored dataframe are equal')\n\nrestoring graph named \"commit_graph_Stolee\"\n&lt;- filename: datasets\\commit_graph_Stolee.df_edgelist.csv.gz\nok - dataframe and restored dataframe are equal\n\n\n\n\n\nCompute levels and min-post intervals for a graph, and store them as attributes of the graph object\n\nsource\n\n\n\n\n compute_reachability_labels (graph, recompute=False)\n\nTest computing reachability labels and saving them as attributes of the graph object\n\nprint('compute reachability labels for {} graph'.format(example_graph_name))\ncompute_reachability_labels(example_graph)\n\nprint('we should see {} and {} among dict-values public attributes'.format('lvl', 'mpi_ext'))\nfor (attr, val) in example_graph.__dict__.items():\n    if isinstance(val, dict) and not attr.startswith('_'):\n        print('- {:s}'.format(attr))\nassert hasattr(example_graph, 'lvl')\nassert hasattr(example_graph, 'mpi_ext')\nprint('ok - graph has both \"{}\" and \"{}\" attributes'.format('lvl', 'mpi_ext'))\n\nprint('reachability labels should be computed for all nodes')\nassert set(example_graph.lvl.keys()) == set(example_graph.nodes)\nassert set(example_graph.mpi_ext.keys()) == set(example_graph.nodes)\nprint('ok - both lvl and mpi_ext keys are all {} graph nodes'.format(len(example_graph.nodes)))\n\ncompute reachability labels for commit_graph_Stolee graph\nwe should see lvl and mpi_ext among dict-values public attributes\n- graph\n- pos\n- lvl\n- mpi_ext\nok - graph has both \"lvl\" and \"mpi_ext\" attributes\nreachability labels should be computed for all nodes\nok - both lvl and mpi_ext keys are all 23 graph nodes\n\n\nStore reachability labels and per-node information in a DataFrame\n\nsource\n\n\n\n\n graph_data_to_dataframe (graph, append_to=None)\n\nTest computing all per-node data for a graph and storing them in DataFrame\n\ndf = graph_data_to_dataframe(example_graph)\n\nprint('check that the dataframe has all the columns')\nassert set(df.columns) == set(['level', 'f_min', 'min', 'post', 'in degree', 'out degree', 'degree'])\nprint('- columns: {}'.format(df.columns.tolist()))\n\nprint('check that the dataframe has all the rows')\nassert set(df.index) == set(example_graph.nodes)\nprint('- rows:    {}...'.format(list(example_graph.nodes)[0:5]))\n\ndf.head()\n\ncheck that the dataframe has all the columns\n- columns: ['f_min', 'min', 'post', 'level', 'in degree', 'out degree', 'degree']\ncheck that the dataframe has all the rows\n- rows:    ['A', 'a7', 'a5', 'a4', 'a3']...\n\n\n\n\n\n\n\n\n\nf_min\nmin\npost\nlevel\nin degree\nout degree\ndegree\n\n\nnode\n\n\n\n\n\n\n\n\n\n\n\nb0\n1\n1\n1\n0\n3\n0\n3\n\n\na1\n1\n1\n2\n1\n2\n1\n3\n\n\na2\n1\n1\n3\n2\n1\n1\n2\n\n\na3\n1\n1\n4\n3\n2\n1\n3\n\n\na4\n1\n1\n5\n4\n1\n1\n2\n\n\n\n\n\n\n\nCompute and save graph structure, its reachability labels and other per-node info to a file, and restore it.\nDon’t redo calculations that can be retrieved from a file.\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Parameters:\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Returns:\n  else: warn(msg)\n\nsource\n\n\n\n\n compute_cached_df (code, filename, file_format=None, dont_save=False)\n\nCompute DataFrame, or retrieve it from a given file if it exists\nTest that the code runs if the file does not exist.\n\ncompute_cached_df(lambda: print(\"the provided code ran\"), 'this_file_does_not_exist.xxx', dont_save=True)\n\nthe provided code ran\n\n\n\ntest_df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\nassert compute_cached_df(lambda: test_df, 'this_file_does_not_exist.xxx', dont_save=True)\\\n                         .equals(test_df)\n\nTest that the code do not run if the file exists, and that we get what we saved\n\nexpect_df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\ndf_filename = 'datasets/example_dataframe.csv.gz'\nsave_df_to_file(expect_df, df_filename)\nactual_df = compute_cached_df(lambda: print(\"THIS SHOULD NOT RUN\"), df_filename)\nassert actual_df.equals(expect_df)\n\nTest that compute_cached_df saves results to a file\n\n_counter = 0\n\ndef example_dataframe():\n    global _counter\n    _counter = _counter + 1\n    return pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n\nexpect_df = example_dataframe()\n_counter = 0\n\ndf_filename = 'datasets/example_dataframe.csv.gz'\n# Path(df_filename).unlink(missing_ok=True)  # requires Python 3.8 or later\nif Path(df_filename).exists():\n    Path(df_filename).unlink()\n\n# first time it should perform computations and save, second time get from file\nactual_df = compute_cached_df(example_dataframe, df_filename)\nactual_df = compute_cached_df(example_dataframe, df_filename)\n\nprint('dataframe:\\n{}\\n'.format(actual_df))\n\n# tests\nassert actual_df.equals(expect_df)\nprint('ok - extracted dataframe matches the one being saved')\nassert Path(df_filename).exists() and Path(df_filename).is_file()\nprint('ok - there exist file \"{}\"'.format(df_filename))\nassert _counter == 1\nprint('ok - the `example_dataframe()` was called only 1 time (actual: {} time(s))'.format(_counter))\n\ndataframe:\n   col1  col2\n0     1     3\n1     2     4\n\nok - extracted dataframe matches the one being saved\nok - there exist file \"datasets/example_dataframe.csv.gz\"\nok - the `example_dataframe()` was called only 1 time (actual: 1 time(s))\n\n\nCompute DataFrame with graph structure (compute_cached_graph_df()) and reachability labels (compute_cached_reachability_labels_df()) and save it to a file, or retrieve it from a file if it exists.\n\nsource\n\n\n\n\n compute_cached_reachability_labels_df (graph, graph_name=None,\n                                        append_to=None,\n                                        datasets_dir='datasets',\n                                        file_format='csv.gz')\n\n\nsource\n\n\n\n\n compute_cached_graph_df (graph_generator, graph_name,\n                          datasets_dir='datasets', file_format='csv.gz')\n\nTest compute_cached_graph_df() and compute_cached_reachability_labels_df()\n\ngraph_df = compute_cached_graph_df(graphs.commit_graph_Stolee, 'commit_graph_Stolee-test')\nassert graph_df.equals(graph_to_dataframe(graphs.commit_graph_Stolee()))\n\ngraph_data_df = compute_cached_reachability_labels_df(example_graph, example_graph_name)\nassert graph_data_df.equals(graph_data_to_dataframe(example_graph))\n\nTo generate graph out of dataframe, use dataframe_to_graph(), defined earlier.\nTo retrieve reachability labels from the dataframe, and store them in graph attributes, use dataframe_to_reachability_labels().\n\nsource\n\n\n\n\n dataframe_to_reachability_labels (df, graph, recompute=False)\n\n\n# check the assumptions of the code\nassert df[['f_min', 'min', 'post']].to_dict(orient='index') == example_graph.mpi_ext\nassert df['level'].to_dict() == example_graph.lvl\n\nGenerate graph, or retrieve it from saved DataFrame with edge list data.\nGenerate reachability labels for the graph, or retrieve them from saved DataFrame with per-node data, adding missing nodes if necessary\n\nsource\n\n\n\n\n compute_cached_reachability_labels (graph, graph_name=None,\n                                     add_missing_nodes=True,\n                                     datasets_dir='datasets',\n                                     file_format='csv.gz')\n\n\nsource\n\n\n\n\n compute_cached_graph (graph_generator, graph_name,\n                       datasets_dir='datasets', file_format='csv.gz')\n\nTest that compute_cached_graph() works\n\nexpect_graph = graphs.commit_graph_Stolee()\nactual_graph = compute_cached_graph(graphs.commit_graph_Stolee, 'commit_graph_Stolee-test')\nassert expect_graph.nodes == actual_graph.nodes\nprint('ok - retrieved/computed graph has the same nodes')\nassert expect_graph.edges == actual_graph.edges\nprint('ok - retrieved/computed graph has the same edges')\nprint('\\npublic attributes of retrieved/computed graph')\nfor (attr, val) in actual_graph.__dict__.items():\n    if not isinstance(val, type) and not attr.startswith('_'):\n        print('- {:s} ({})'.format(attr, type(val)))\n        \nprint('\\npublic attributes of example graph')\nfor (attr, val) in expect_graph.__dict__.items():\n    if not isinstance(val, type)  and not attr.startswith('_'):\n        print('- {:s} ({})'.format(attr, type(val)))\n\nok - retrieved/computed graph has the same nodes\nok - retrieved/computed graph has the same edges\n\npublic attributes of retrieved/computed graph\n- graph (&lt;class 'dict'&gt;)\n- df_edgelist (&lt;class 'pandas.core.frame.DataFrame'&gt;)\n- nodes (&lt;class 'networkx.classes.reportviews.NodeView'&gt;)\n\npublic attributes of example graph\n- graph (&lt;class 'dict'&gt;)\n- pos (&lt;class 'dict'&gt;)\n- nodes (&lt;class 'networkx.classes.reportviews.NodeView'&gt;)\n\n\nTest that compute_cached_reachability_labels() works\n\nsome_graph = graphs.commit_graph_Stolee()\ncompute_cached_reachability_labels(some_graph, 'commit_graph_Stolee-test')\ncompute_cached_reachability_labels(some_graph, 'commit_graph_Stolee-test')\n\n#print('')\nprint('public attributes of the graph named \"{}\"'.format('commit_graph_Stolee-test'))\nfor (attr, val) in some_graph.__dict__.items():\n    if not isinstance(val, type)  and not attr.startswith('_'):\n        print('- {!s} ({})'.format(attr, type(val)))\n\nprint('')\ndf = some_graph.df_nodedata\nprint('check that the dataframe has all the columns')\nassert set(df.columns) == set(['level', 'f_min', 'min', 'post', 'in degree', 'out degree', 'degree'])\nprint('- columns: {}'.format(df.columns.tolist()))\n\nprint('check that the dataframe has all the rows')\nassert set(df.index) == set(some_graph.nodes)\nprint('- rows:    {}...'.format(list(example_graph.nodes)[0:5]))        \n\nprint('check that graph has \"lvl\" attribute')\nassert hasattr(some_graph, 'lvl')\nprint('- lvls:    {}...'.format({k: some_graph.lvl[k] for k in list(some_graph.lvl)[:5]}))\n\nprint('check that graph has \"mpi_ext\" attribute')\nassert hasattr(some_graph, 'mpi_ext')\nprint('- mpi_ext: {}...'.format({k: some_graph.mpi_ext[k] for k in list(some_graph.mpi_ext)[:2]}))\n\nsome_graph.df_nodedata.head()\n\npublic attributes of the graph named \"commit_graph_Stolee-test\"\n- graph (&lt;class 'dict'&gt;)\n- pos (&lt;class 'dict'&gt;)\n- df_nodedata (&lt;class 'pandas.core.frame.DataFrame'&gt;)\n- lvl (&lt;class 'dict'&gt;)\n- mpi_ext (&lt;class 'dict'&gt;)\n- nodes (&lt;class 'networkx.classes.reportviews.NodeView'&gt;)\n\ncheck that the dataframe has all the columns\n- columns: ['f_min', 'min', 'post', 'level', 'in degree', 'out degree', 'degree']\ncheck that the dataframe has all the rows\n- rows:    ['A', 'a7', 'a5', 'a4', 'a3']...\ncheck that graph has \"lvl\" attribute\n- lvls:    {'b0': 0, 'a1': 1, 'a2': 2, 'a3': 3, 'a4': 4}...\ncheck that graph has \"mpi_ext\" attribute\n- mpi_ext: {'b0': {'f_min': 1, 'min': 1, 'post': 1}, 'a1': {'f_min': 1, 'min': 1, 'post': 2}}...\n\n\n\n\n\n\n\n\n\nf_min\nmin\npost\nlevel\nin degree\nout degree\ndegree\n\n\nnode\n\n\n\n\n\n\n\n\n\n\n\nb0\n1\n1\n1\n0\n3\n0\n3\n\n\na1\n1\n1\n2\n1\n2\n1\n3\n\n\na2\n1\n1\n3\n2\n1\n1\n2\n\n\na3\n1\n1\n4\n3\n2\n1\n3\n\n\na4\n1\n1\n5\n4\n1\n1\n2"
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "Extracting commit graphs from Git repositories",
    "section": "",
    "text": "Imports for the commit_graph module\nImports for plotting\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "git.html#interacting-with-git-repositories-in-python",
    "href": "git.html#interacting-with-git-repositories-in-python",
    "title": "Extracting commit graphs from Git repositories",
    "section": "Interacting with Git repositories in Python",
    "text": "Interacting with Git repositories in Python\nThere are various ways of interacting with Git repositories from Python code:\n\nshelling out and running git directly (for example using import subprocess)\nusing GitPython (import git)\nusing pygit2 (import pygit2) - Python bindings to libgit2 library\n\nGitPython is a python library used to interact with git repositories, high-level like git-porcelain, or low-level like git-plumbing.\nIt provides abstractions of git objects for easy access of repository data, and additionally allows you to access the git repository more directly using either a pure python implementation, or the faster, but more resource intensive git command implementation.\nThe object database implementation is optimized for handling large quantities of objects and large datasets, which is achieved by using low-level structures and data streaming.\nNOTE: Promisory objects (in sparse clone, --filter=tree:0) might be not supported.\nimport git\n\nrepo = git.Repo.clone_from(url, path, ['--mirror', '--filter=tree:0'])\nrepo = git.Repo(path) # just open the repository\nPygit2 is a set of Python bindings to the libgit2 shared library, libgit2 implements the core of Git.\nNOTE: It is quite low level, and not everything is implemented yet (especially new features).\nimport pygit2\n\npygit2.clone_repository(url, path, bare=True) # no support for filters?\nrepo = pygit2.Repository(path)\nThe subprocess module in the Python standard library allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes.\nThe recommended approach to invoking subprocesses is to use the run() function for all use cases it can handle.\nimport subprocess\n\nsubprocess.run(['git', 'clone', '--mirror', '--filter=tree:0', '--quiet', url, path])"
  },
  {
    "objectID": "git.html#cloning-git.git-repository-and-extracting-and-saving-the-commit-graph",
    "href": "git.html#cloning-git.git-repository-and-extracting-and-saving-the-commit-graph",
    "title": "Extracting commit graphs from Git repositories",
    "section": "Cloning git.git repository, and extracting and saving the commit graph",
    "text": "Cloning git.git repository, and extracting and saving the commit graph\n\n\n\n\n\n\nNote\n\n\n\nThe exploration was moved to the Appendix for this notebook: A.09_git_explore.ipynb\n\n\n\nSummary of off-the-record findings about cloning repositories\nWhy it is important to use sparse clone - it is much faster, and takes up less space. For example for git.git repository we have the following (all times are wall time, and subject to interference from other CPU load): - full clone: 54s - mirror: 1min 19s - sparse: 14.9s - 19.4s\n\n\nSummary of findings of testing different graph output formats (saving graph to a file)\nStoring DataFrame of edgelist data\nThe DataFrame is created using nx.to_pandas_edgelist(graph)\nThe smallest file is the result of storing DataFrame of edgelist data as gzipped pickle (written using Pandas’ to_pickle()) - the *.df_edgelist.pickle.gz file.\n\nAdvantages:\n\n2nd smallest size\nno extra modules to install\npreserves types\n\nDisadvantages:\n\nPython-specific\nunsafe\nslow (?)\n\n\nNext smallest is, surprisingly, gzipped CSV representing DataFrame of edgelist data (written using Pandas’ to_csv()) - the *.df_edgelist.csv.gz file.\n\nAdvantages:\n\n3rd smallest size\nno extra modules to install\nuniversal format\n\nDisadvantages:\n\nslow (?)\n\n\nThe fast interchange formats, Feather and Parquet turned out to produce quite large files; they are however 4th smallest and 2nd smallest, respectively, among uncompressed formats.\nFeather provides a binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy.\nFeather is designed to faithfully serialize and de-serialize DataFrames, supporting all of the pandas dtypes, including extension dtypes such as categorical and datetime with tz.\nApache Parquet provides a partitioned binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy. Parquet can use a variety of compression techniques (default is to use ‘snappy’) to shrink the file size as much as possible while still maintaining good read performance.\nParquet is designed to faithfully serialize and de-serialize DataFrames, supporting all of the pandas dtypes, including extension dtypes such as datetime with tz.\n\nAdvantages:\n\nuniversal, cross language\npreserves types\nfast\n\nDisadvantages:\n\nrequires pyarrow package to be installed\nquite large file size\n\n\nThe HDF5 format has the advantage of being able to store multiple DataFrames in a single file, for example one DataFrame to hold edgelist data to define the graph connections, and one DataFrame holding various per vertex (per node) rechablity index data. Unfortunately, even internally compressed it has one of larger file sizes (and largest compressed).\nHierarchical Data Format (HDF) is self-describing, allowing an application to interpret the structure and contents of a file with no outside information. One HDF file can hold a mix of related objects which can be accessed as a group or as individual objects.\nThe to_hdf() method in Pandas uses the HDFStore in the background, which in turn utilizes the PyTables library.\n\nAdvantages:\n\nuniversal, cross language\npreserves types\ncan store multiple DataFrames\n\nDisadvantages:\n\nlarge file size\nrequires tables package to be installed (PyTables)\nslow (?)\nunsafe (because of serializing object-dtype data with pickle)\n\n\n\n\n\nStoring NetworkX DiGraph directly\n\n\nIt turns out that the graph6 / sparse6 format is not a good choice for storing the commit graph for the following reasons: - NetworkX does not support digraph6 format; neither write_graph6 not write_sparse6 are implemented for directed graphs - the digraph6 format is dense graph format, storing the encoded adjacency matrix; there is no disparse6 or digraph6sparse format - SparseGraph6 formats do not store node/vertex names, which might be needed for future computations or visualizations\n\n\nExcept for the adjacency lists format (*.adjlist.txt), which cannot store any additional data (not even per-edge), the multi-line adjacency lists (*.multiline_adjlist.txt) and edge lists (*.edgelist.txt) formats have file sizes larger than the Parquet storing DataFrame of edge lists. Note however that they are uncompressed.\n\n\nThe adjacency list format can be created directly by appropriate Git command.\n\n\nBecause those formats cannot store per-node data, they are not considered for use.\n\n\nAmong specialized graph file formats, which I assume can store both per-node and per-edge data, Pajek format is smallest (compressed), and 4th smallest among all considered storage formats. It is also smallest uncompressed among similar file formats.\n\n\nWriting NetworkX graphs as Python pickles using write_gpickle() (*.gpickle.gz) results in quite large file compressed, one of the largest files uncompressed.\n\n\nOn the other hand NetworkX graphs can contain any hashable Python object as node (not just integers and strings). For arbitrary data types it may be difficult to represent the data as text. In that case using Python pickles to store the graph data can be used.\n\n\n## Examining computing and storing reachability labels and other per-node data\n\n\n:::{.callout-note}\n\n\nThe exploration was moved to the Appendix for this notebook: A.09_git_explore.ipynb\n\n\n:::\n\n\n### Summary of findings of storing computed reachability labels and related stats\n\n\nStrangely,smallest files size this time is given when storing using the HDF5 (with compresslevel=6) format, which is also 2nd fastest.\n\n\nSaving DataFrame using the Parquet format is fastest, but this time it gives the largest size of the file\n\n\nSaving as *.pickle.gz or *.csv.gz is slowest and second slowest, while both being in the middlle with respect to file sizes (the pickle-based format gives smaller file size).\n\n\n## Functions for creating the commit-graph out of repository, computing labelling, etc\n\n\nThose directories would be needed for tests: - repos for storing cloned Git repositories - datasets for storing extracted commit graphs, and other data\n\n\n::: {.cell}\n\n\n### Functions for cloning the repository\n\n\nBecause the repository is needed only to extract the commit graph from it, therefore to save time and disk space sparse clone will be used.\n\n\nThis strongly suggests using the Git command line.\n\n\n\nsource\n\n\nsparse_clone\n\n sparse_clone (url, repo_name=None, repos_dir='repos')\n\nClone Git repository by URL using sparse/partial clone\nThe “Sparse Clone” or the “Partial Clone” feature is a performance optimization for Git that allows Git to function without having a complete copy of the repository. The goal of this feature is to allow Git better handle extremely large repositories, or in this case store the repository using less disk space.\nTo retrieve the commit graph out of the git repository only commit objects are needed, so all tree and blob objects can be omitted from the clone.\nUsing sparse clone is much more efficient: the process is faster, and the resulting repository takes much less disk space than full clone.\nNOTE: Use of partial clone requires that the user be online and the origin remote or other promisor remotes be available for on-demand fetching of missing objects, unless the user can stay within the pre-selected subset of objects.\nsee: https://git-scm.com/docs/git-clone see: https://git-scm.com/docs/git-rev-list#Documentation/git-rev-list.txt—filterltfilter-specgt see: https://github.com/git/git/blob/master/Documentation/technical/partial-clone.txt\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nThe repository to be cloned, in most cases it would be public URLthat can be used to clone the remote repository.\n\n\nrepo_name\nNoneType\nNone\n\n\n\nrepos_dir\nstr\nrepos\nThe directory where to put cloned repository into.By default “repos” is used.\n\n\n\nLet’s test sparse_clone(), for example using quite small and archived https://github.com/githubtraining/hellogitworld “Hello Git World” sample training repository.\n\n# remove the test repository, if exists\nfrom shutil import rmtree\n\nrepo_url=\"https://github.com/githubtraining/hellogitworld\"\nrepo_path=\"repos/hellogitworld.git\"\nrepo_name=\"hellogitworld.git\"\n#rmtree(repo_path, ignore_errors=True)\n\n# clone the repository\nsparse_clone(repo_url, repo_name=repo_path, repos_dir=None)\n\n# the directory should exist, and be a directory\nassert Path(repo_path).is_dir()\nprint('ok - repository exists as directory after cloning')\n# it should look like a git repository\nassert (Path(repo_path) / \"HEAD\").is_file()\nassert (Path(repo_path) / \"objects\").is_dir()\nprint('ok - and it looks like a git repository')\n\n#!git -C repos/hellogitworld.git config --get remote.origin.url\n#!git -C repos/hellogitworld.git remote show -n origin\n\n# NOTE: `capture_output` was added in Python 3.7, GitHub CI runs Python 3.6\nassert subprocess.run(['git', '-C', repo_path, 'config', '--get', 'remote.origin.url'],\n                      stdout=subprocess.PIPE).stdout.decode(\"utf-8\").strip() == repo_url\nprint('ok - cloned repository has correct url')\n\n# attempt cleanup after the test, which may not work on MS Windows\n#rmtree(repo_path, ignore_errors=True)\n\nok - repository exists as directory after cloning\nok - and it looks like a git repository\nok - cloned repository has correct url\n\n\nIf repository exists, there is no need to clone it\n\nsource\n\n\nget_repo\n\n get_repo (url, repo_path, refresh=False)\n\nClone Git repository into given path, if needed\nIf the path given by repo_path does not exist, or refresh is true, clone Git repository at url for later extraction of the commit graph.\nsee: sparse_clone()\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nThe repository to be cloned, in most cases it would be public URLthat can be used to clone the remote repository.\n\n\nrepo_path\n\n\n\n\n\nrefresh\nbool\nFalse\n\n\n\n\n\n\nFunctions for extracting the commit graph out of git repository\nUsing Git commands, we can easily create text file that store commit-graph information in the adjacency list format, from which it is easy to create a NetworkX graph (DiGraph). Shortened object identifiers (shortened SHA-1) will be used to identify commits which are nodes / vertices of the commit graph.\nTest that examples from the docstring works:\n\n#print('%r' % _repo_basename('hellogitworld'))\nassert _repo_basename('hellogitworld') == 'hellogitworld'\n#print('%r' % _repo_basename('hellogitworld.git'))\nassert _repo_basename('hellogitworld.git') == 'hellogitworld'\n#print('%r' % _repo_basename('repos/hellogitworld.git'))\nassert _repo_basename('repos/hellogitworld.git') == 'hellogitworld'\n#print('%r' % _repo_basename('repos/hellogitworld/.git'))\n#assert _repo_basename('repos/hellogitworld/.git') == 'hellogitworld'\n\n\nassert _commit_graph_name('hellogitworld') == 'hellogitworld-commit_graph'\n\n\nassert _repo_graph_name('repos/hellogitworld.git') == 'hellogitworld-commit_graph'\n\nLet’s test the easy-to-test internal function _repo_graph_savefile().\n\nassert _savefile_name('example_graph', kind='adjlist', file_format='txt') == Path('datasets/example_graph.adjlist.txt')\nassert _savefile_name('example_graph', out_dir='data', kind='adjlist', file_format='txt') == Path('data/example_graph.adjlist.txt')\n\n\nassert _repo_graph_savefile('repos/hellogitworld.git') == Path('datasets/hellogitworld-commit_graph.adjlist.txt')\nassert _repo_graph_savefile('repos/hellogitworld.git', out_dir='data') == Path('data/hellogitworld-commit_graph.adjlist.txt')\n\n\nTODO: Fix the problem with generating savefile name if the repository path does not exist\n_repo_graph_savefile('repos/example.git') returns WindowsPath, not Path (???)\n\n\n#assert _repo_graph_savefile('repos/example.git') == Path('datasets/example-commit_graph.adjlist.txt')\n#assert _repo_graph_savefile('repos/example.git', out_dir='data') == Path('data/example-commit_graph.adjlist.txt')\nprint('%r' % _repo_basename('repos/example.git'))\nprint('%r' % _commit_graph_name('example'))\nprint('%r' % _repo_graph_name('repos/example.git'))\nprint('%r' % _repo_graph_savefile('repos/example.git'))\n\n'example'\n'example-commit_graph'\n'example-commit_graph'\nWindowsPath('datasets/example-commit_graph.adjlist.txt')\n\n\n/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Format\n  else: warn(msg)\n\nsource\n\n\nrepo_generate_adjlist\n\n repo_generate_adjlist (repo_path, out_dir='datasets', refresh=False)\n\nGenerate graph of revisions in the adjacency list format\nCan be read with nx.read_adjlist(); don’t forget to ensure that NetworkX generates directed graph.\n\n\n\n\npath = repo_generate_adjlist(“repos/git.git”) graph = nx.read_adjlist(path, create_using=nx.DiGraph)\n\n\n\n\nBy default it does not recreate the file if it exists already, unless refresh=True argument is passed.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nrepo_path\nstr\n\nPath to the Git repository\n\n\nout_dir\nstr\ndatasets\nDirectory where extracted commit graph data would be stored.Defaults to “datasets”.\n\n\nrefresh\nbool\nFalse\nWhether to regenerate the file with the commit graph in adjacencyformat if it exists. Defaults to false.\n\n\nReturns\nPath\n\nPath where the commit graph in the adjacency file formatcan be found.\n\n\n\nTo process the commit graph, we need to create a NetworkX directed graph from it, using adjacency list file in text format, generated using git log with appropriate parameters.\n\nsource\n\n\nrepo_adjlist_to_graph\n\n repo_adjlist_to_graph (repo_path, datasets_dir='datasets')\n\nCreate DiGraph out of adjacency list file created from it\nNOTE: the repo_to_graph() function is safer to use, as it generates the graph of commits in the adjacency list file format of needed.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nrepo_path\nstr\n\nPath to the Git repository\n\n\ndatasets_dir\nstr\ndatasets\nDirectory where extracted commit graph data would be stored.Defaults to “datasets”.\n\n\nReturns\nnx.DiGraph\n\nDirected graph of revisions as NetworkX’s DiGraph\n\n\n\nTying it all together, to get a directed graph of revision history of a given repository into NetworkX DiGraph, we need to create a sparse clone of a repository if it does not exist, write log of revision history in adjacency list format if the file does not exist, and create the commit graph as NetworkX DiGraph.\n\nsource\n\n\ncommit_graph\n\n commit_graph (url, repo_name, repos_dir='repos', datasets_dir='datasets',\n               reclone=False, rescan=False)\n\nCreate a graph of commits for given remote repository, stored locally\nGiven a Git repository url, clone it as repo_name in repo_dir directory, then create a graph of its commits as NetworkX DiGraph.\nAvoids re-cloning of the remote repository unless reclone=True is passed, and avoids rescanning the local copy of the repository if file in the adjacency list format with the commit graph information exists unless rescan=True is passed.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nThe repository to be cloned, in most cases it would be public URLthat can be used to clone the remote repository.\n\n\nrepo_name\n\n\n\n\n\nrepos_dir\nstr\nrepos\nThe directory where to put cloned repository into.By default “repos” is used.\n\n\ndatasets_dir\nstr\ndatasets\nDirectory where extracted commit graph data can be stored, or willbe stored. Defaults to “datasets”.\n\n\nreclone\nbool\nFalse\nWhether to re-clone the repository if the local clone exists.Default is false, to not perform a clone if not needed.\n\n\nrescan\nbool\nFalse\nWhether to regenerate the file with the commit graph in adjacencyformat if it exists. Defaults to false.\n\n\nReturns\nnx.DiGraph\n\nDirected graph of revisions as NetworkX’s DiGraph\n\n\n\n\nsource\n\n\nrepo_to_graph\n\n repo_to_graph (repo_path, datasets_dir='datasets', refresh=False)\n\nCreate a graph of commits for given local repository\nIt uses existing file with the commit graph in the adjacency list file format, unless refresh is requested with refresh=True. Automatically generates such file if it does not exist.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nrepo_path\nstr\n\nPath to the local Git repository\n\n\ndatasets_dir\nstr\ndatasets\nDirectory where extracted commit graph data can be stored, or willbe stored. Defaults to “datasets”.\n\n\nrefresh\nbool\nFalse\nWhether to regenerate the file with the commit graph in adjacencyformat if it exists. Defaults to false.\n\n\nReturns\nnx.DiGraph\n\nDirected graph of revisions as NetworkX’s DiGraph\n\n\n\nInstead of testing all those functions one by one, let’s simply test the final one, namely commit_graph(), which would would as an integration test. It would stress test almost all other functions.\n\n# remove the test repository\nfrom shutil import rmtree\n# check the format of node names\nimport re\n\nrepo_url=\"https://github.com/githubtraining/hellogitworld\"\nrepo_path=\"repos/hellogitworld.git\"\nrepo_name=\"hellogitworld.git\"\n#rmtree(repo_path, ignore_errors=True)\n\n# get commit graph of remote repository\ngraph = commit_graph(repo_url, repo_name, rescan=True)\n\n# find sinks (roots in git terminology) and sources (heads in git terminology)\nsinks  =[n for n in graph if graph.out_degree(n) == 0]\nsources=[n for n in graph if graph.in_degree(n)  == 0]\n\nassert isinstance(graph, nx.DiGraph)\nprint('ok - commit_graph() result is `DiGraph`')\n# this is an archived repository, so it should not change at all\nassert graph.number_of_nodes() == 55\nprint('ok - number of nodes {:d} matches 55 commits on all branches'.format(graph.number_of_nodes()))\nassert graph.number_of_edges() == 53\nprint('ok - number of edges {:d} matches'.format(graph.number_of_edges()))\nassert len(sources) == 6\nprint('ok - number of sources {:d} matches 6 independent branches'.format(len(sources)))\nassert len(sinks) == 4\nprint('ok - number of sinks {:d} matches number of unrelated branches'.format(len(sinks)))\n\nid_regexp = re.compile(r'^[0-9a-f]+$')\nassert all([id_regexp.fullmatch(node) is not None for node in graph.nodes])\nprint('ok - all node names looks like object identifiers (are hexdigits)')\n\n# attempt cleanup\n#rmtree(repo_path, ignore_errors=True)\n\nok - commit_graph() result is `DiGraph`\nok - number of nodes 55 matches 55 commits on all branches\nok - number of edges 53 matches\nok - number of sources 6 matches 6 independent branches\nok - number of sinks 4 matches number of unrelated branches\nok - all node names looks like object identifiers (are hexdigits)\n\n\nThe https://github.com/githubtraining/hellogitworld repository has: - 26 commits on the ‘master’ branch - 55 commits on all branches - 345 commits among all references, including pull requests (5 open, 156 closed)\n\n# this plot is here to check if the number of edges found is plausible\ntry:\n    import pydot\n    \n    fig=plt.figure(figsize=(8,7))\n    plt.suptitle('The commit graph of https://github.com/githubtraining/hellogitworld repository\\n'+\n                 'source nodes are red, sink nodes are yellow, isolated nodes are yellow with red outline')\n    plt.title(\"drawn using 'dot' algorithm from GraphViz, via 'pydot' module\")\n    pos=nx.drawing.nx_pydot.pydot_layout(graph, prog='dot')\n    nx.draw(graph, pos=pos,\n            node_size=80,width=1.0,node_color='#FF7F00')\n    nx.draw_networkx_nodes(graph, pos=pos, nodelist=sources,\n                           node_size=90,node_color='#FF1100')\n    nx.draw_networkx_nodes(graph, pos=pos, nodelist=sinks,\n                           node_size=60,node_color='#FFDD00')\n    plt.draw()\nexcept ModuleNotFoundError:\n    print(\"'pydot' module not installed\")\n\n'pydot' module not installed"
  }
]